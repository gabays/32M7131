{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQAm1X2uzJHB"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gabays/32M7131/blob/main/Cours_03/Cours03.ipynb)\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Licence Creative Commons\" style=\"border-width:0;float:right;\\\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a>\n",
    "\n",
    "Distant Reading 2: linguistique computationnelle\n",
    "\n",
    "# **Lemmatiser un texte**\n",
    "\n",
    "Simon Gabay\n",
    "\n",
    "\n",
    "\n",
    "üö® Pour les entra√Ænements, on va avoir besoin d'un GPU. Si vous utilisez Colab, on va en demander un √† notre meilleur ami Google un en allant sur dans le menu en haut √† gauche et en choisissant `Execution` > `Modifier le type d'ex√©cution` puis en choisissant `GPU`.\n",
    "\n",
    "‚ö†Ô∏è <font color='red'>Attention! l'usage des GPU est limit√© sur Google! Il faut les utiliser avec parcimonie, sinon il faut payer! Ou bien vous pouvez tout faire en ligne de commande sur les clusters HPC de l'uni.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Pr√©parer l'environnement de travail\n",
    "\n",
    "Vous devez d√©finir le dossier de travail, qui n'est pas le m√™me sur Colab ou OpenOnDemand.\n",
    "- si vous √™tes sur Google colab il s'agit de `/content`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY=\"/content\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Si vous √™tes sur OpenOnDemand, cr√©ez un dossier et d√©finissez une variable avec le chemin vers ce fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On cr√©e un dossier o√π on va mettre les documents\n",
    "!mkdir -p Lemmatisation\n",
    "#On d√©finit une variable avec le chemin vers ce dossier\n",
    "DIRECTORY=\"Lemmatisation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va nettoyer notre env des trucs inutiles (install√©s pr√©alablement).\n",
    "Note: c'est inutile sur Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: gensim 4.3.2\n",
      "Uninstalling gensim-4.3.2:\n",
      "  Successfully uninstalled gensim-4.3.2\n",
      "Found existing installation: JSON-minify 0.3.0\n",
      "Uninstalling JSON-minify-0.3.0:\n",
      "  Successfully uninstalled JSON-minify-0.3.0\n",
      "Found existing installation: lxml 5.1.0\n",
      "Uninstalling lxml-5.1.0:\n",
      "  Successfully uninstalled lxml-5.1.0\n",
      "Found existing installation: nlp-pie 0.3.8\n",
      "Uninstalling nlp-pie-0.3.8:\n",
      "  Successfully uninstalled nlp-pie-0.3.8\n",
      "Found existing installation: numpy 1.17.5\n",
      "Uninstalling numpy-1.17.5:\n",
      "  Successfully uninstalled numpy-1.17.5\n",
      "Found existing installation: PyYAML 5.1b3\n",
      "Uninstalling PyYAML-5.1b3:\n",
      "  Successfully uninstalled PyYAML-5.1b3\n",
      "Found existing installation: scikit-learn 0.22.2.post1\n",
      "Uninstalling scikit-learn-0.22.2.post1:\n",
      "  Successfully uninstalled scikit-learn-0.22.2.post1\n",
      "Found existing installation: scipy 1.10.1\n",
      "Uninstalling scipy-1.10.1:\n",
      "  Successfully uninstalled scipy-1.10.1\n",
      "Found existing installation: smart-open 7.0.1\n",
      "Uninstalling smart-open-7.0.1:\n",
      "  Successfully uninstalled smart-open-7.0.1\n",
      "Found existing installation: termcolor 2.4.0\n",
      "Uninstalling termcolor-2.4.0:\n",
      "  Successfully uninstalled termcolor-2.4.0\n",
      "Found existing installation: terminaltables 3.1.0\n",
      "Uninstalling terminaltables-3.1.0:\n",
      "  Successfully uninstalled terminaltables-3.1.0\n",
      "Found existing installation: torch 1.7.1\n",
      "Uninstalling torch-1.7.1:\n",
      "  Successfully uninstalled torch-1.7.1\n",
      "Found existing installation: tqdm 4.66.2\n",
      "Uninstalling tqdm-4.66.2:\n",
      "  Successfully uninstalled tqdm-4.66.2\n",
      "Found existing installation: typing 3.7.4.3\n",
      "Uninstalling typing-3.7.4.3:\n",
      "  Successfully uninstalled typing-3.7.4.3\n",
      "Found existing installation: typing-extensions 4.10.0\n",
      "Uninstalling typing-extensions-4.10.0:\n",
      "  Successfully uninstalled typing-extensions-4.10.0\n",
      "Found existing installation: wrapt 1.16.0\n",
      "Uninstalling wrapt-1.16.0:\n",
      "  Successfully uninstalled wrapt-1.16.0\n",
      "Package                       Version\n",
      "----------------------------- ----------\n",
      "alabaster                     0.7.12\n",
      "appdirs                       1.4.4\n",
      "argon2-cffi                   20.1.0\n",
      "asn1crypto                    1.4.0\n",
      "async-generator               1.10\n",
      "atomicwrites                  1.4.0\n",
      "attrs                         20.2.0\n",
      "Babel                         2.8.0\n",
      "backcall                      0.2.0\n",
      "bcrypt                        3.2.0\n",
      "bitstring                     3.1.7\n",
      "bleach                        3.2.1\n",
      "blist                         1.3.6\n",
      "CacheControl                  0.12.6\n",
      "cachy                         0.3.0\n",
      "certifi                       2020.6.20\n",
      "cffi                          1.14.3\n",
      "chardet                       3.0.4\n",
      "cleo                          0.8.1\n",
      "click                         7.1.2\n",
      "clikit                        0.6.2\n",
      "colorama                      0.4.3\n",
      "crashtest                     0.3.1\n",
      "cryptography                  3.1.1\n",
      "Cython                        0.29.21\n",
      "decorator                     4.4.2\n",
      "defusedxml                    0.6.0\n",
      "distlib                       0.3.1\n",
      "docopt                        0.6.2\n",
      "docutils                      0.16\n",
      "ecdsa                         0.16.0\n",
      "entrypoints                   0.3\n",
      "filelock                      3.0.12\n",
      "flit                          3.0.0\n",
      "flit-core                     3.0.0\n",
      "fsspec                        0.8.4\n",
      "future                        0.18.2\n",
      "html5lib                      1.1\n",
      "idna                          2.10\n",
      "imagesize                     1.2.0\n",
      "importlib-metadata            2.0.0\n",
      "iniconfig                     1.0.1\n",
      "intervaltree                  3.1.0\n",
      "intreehooks                   1.0\n",
      "ipaddress                     1.0.23\n",
      "ipykernel                     5.3.4\n",
      "ipython                       7.18.1\n",
      "ipython-genutils              0.2.0\n",
      "ipywidgets                    7.5.1\n",
      "jedi                          0.17.2\n",
      "jeepney                       0.4.3\n",
      "Jinja2                        2.11.2\n",
      "joblib                        0.17.0\n",
      "json5                         0.9.5\n",
      "jsonschema                    3.2.0\n",
      "jupyter-client                6.1.7\n",
      "jupyter-core                  4.6.3\n",
      "jupyterlab                    2.2.8\n",
      "jupyterlab-pygments           0.1.2\n",
      "jupyterlab-server             1.2.0\n",
      "keyring                       21.4.0\n",
      "keyrings.alt                  4.0.0\n",
      "liac-arff                     2.5.0\n",
      "lockfile                      0.12.2\n",
      "MarkupSafe                    1.1.1\n",
      "mistune                       0.8.4\n",
      "mock                          4.0.2\n",
      "more-itertools                8.5.0\n",
      "msgpack                       1.0.0\n",
      "nbclient                      0.5.0\n",
      "nbconvert                     6.0.7\n",
      "nbformat                      5.0.7\n",
      "nest-asyncio                  1.4.1\n",
      "netaddr                       0.8.0\n",
      "netifaces                     0.10.9\n",
      "nose                          1.3.7\n",
      "notebook                      6.1.4\n",
      "packaging                     20.4\n",
      "pandocfilters                 1.4.2\n",
      "paramiko                      2.7.2\n",
      "parso                         0.7.1\n",
      "pastel                        0.2.1\n",
      "pathlib2                      2.3.5\n",
      "paycheck                      1.0.2\n",
      "pbr                           5.5.0\n",
      "pexpect                       4.8.0\n",
      "pickleshare                   0.7.5\n",
      "pip                           20.2.3\n",
      "pkginfo                       1.5.0.1\n",
      "pluggy                        0.13.1\n",
      "poetry                        1.1.3\n",
      "poetry-core                   1.0.0\n",
      "prometheus-client             0.8.0\n",
      "prompt-toolkit                3.0.7\n",
      "psutil                        5.7.2\n",
      "ptyprocess                    0.6.0\n",
      "py                            1.9.0\n",
      "py-expression-eval            0.3.10\n",
      "pyasn1                        0.4.8\n",
      "pycparser                     2.20\n",
      "pycrypto                      2.6.1\n",
      "Pygments                      2.7.1\n",
      "pylev                         1.3.0\n",
      "PyNaCl                        1.4.0\n",
      "pyparsing                     2.4.7\n",
      "pyrsistent                    0.17.3\n",
      "pytest                        6.1.1\n",
      "python-dateutil               2.8.1\n",
      "pytoml                        0.1.21\n",
      "pytz                          2020.1\n",
      "pyzmq                         19.0.2\n",
      "regex                         2020.10.11\n",
      "requests                      2.24.0\n",
      "requests-toolbelt             0.9.1\n",
      "scandir                       1.10.0\n",
      "SecretStorage                 3.1.2\n",
      "Send2Trash                    1.5.0\n",
      "setuptools                    50.3.0\n",
      "setuptools-scm                4.1.2\n",
      "shellingham                   1.3.2\n",
      "simplegeneric                 0.8.1\n",
      "simplejson                    3.17.2\n",
      "six                           1.15.0\n",
      "snowballstemmer               2.0.0\n",
      "sortedcontainers              2.2.2\n",
      "Sphinx                        3.2.1\n",
      "sphinx-bootstrap-theme        0.7.1\n",
      "sphinxcontrib-applehelp       1.0.2\n",
      "sphinxcontrib-devhelp         1.0.2\n",
      "sphinxcontrib-htmlhelp        1.0.3\n",
      "sphinxcontrib-jsmath          1.0.1\n",
      "sphinxcontrib-qthelp          1.0.3\n",
      "sphinxcontrib-serializinghtml 1.1.4\n",
      "sphinxcontrib-websupport      1.2.4\n",
      "tabulate                      0.8.7\n",
      "terminado                     0.9.1\n",
      "testpath                      0.4.4\n",
      "threadpoolctl                 2.1.0\n",
      "toml                          0.10.1\n",
      "tomlkit                       0.7.0\n",
      "tornado                       6.0.4\n",
      "traitlets                     5.0.4\n",
      "ujson                         4.0.1\n",
      "urllib3                       1.25.10\n",
      "virtualenv                    20.0.34\n",
      "wcwidth                       0.2.5\n",
      "webencodings                  0.5.1\n",
      "wheel                         0.35.1\n",
      "widgetsnbextension            3.5.1\n",
      "xlrd                          1.2.0\n",
      "zipp                          3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze --user --exclude-editable | xargs pip uninstall -y\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGw9TcXHhnil"
   },
   "source": [
    "## 1. Annoter\n",
    "\n",
    "### 1.1 Premier test\n",
    "\n",
    "Il nous faut d'abord installer [_pie-extended_](https://pypi.org/project/pie-extended/) pour utiliser le lemmatiseur [_Pie_](https://github.com/emanjavacas/pie). Il existe bien d'autres outils: l'un des plus populaires est [_spaCy_](https://spacy.io), qui couvre les principales langues (allemand, fran√ßais, anglais, espagnol, anglais‚Ä¶), mais nous nous int√©ressons √† des langues moins bien dot√©es et d'une nature diff√©rente (pr√©-orthographiques) qui n√©cessitent des solutions sp√©cifiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxn4vDYUyzLA",
    "outputId": "34cfc6c0-378a-40d6-a2a0-7cab2d5bf020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "  Using cached pip-24.0-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "Successfully installed pip-24.0\n",
      "Collecting pie-extended\n",
      "  Using cached pie_extended-0.1.2-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting PaPie~=0.4.0 (from pie-extended)\n",
      "  Using cached PaPie-0.4.2-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting autodisambiguator<1.0.0,>=0.0.1 (from pie-extended)\n",
      "  Using cached autodisambiguator-0.0.1-py2.py3-none-any.whl.metadata (817 bytes)\n",
      "Requirement already satisfied: click<8.0,>=7.0 in /opt/ebsofts/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from pie-extended) (7.1.2)\n",
      "Collecting colorama>=0.4.4 (from pie-extended)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting numpy<1.24.0 (from pie-extended)\n",
      "  Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: regex in /opt/ebsofts/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from pie-extended) (2020.10.11)\n",
      "Collecting requests~=2.25.0 (from pie-extended)\n",
      "  Using cached requests-2.25.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting unidecode~=1.1.1 (from pie-extended)\n",
      "  Using cached Unidecode-1.1.2-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting JSON-minify>=0.3.0 (from PaPie~=0.4.0->pie-extended)\n",
      "  Using cached JSON_minify-0.3.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting pytorch-optimizer~=2.7.0 (from PaPie~=0.4.0->pie-extended)\n",
      "  Using cached pytorch_optimizer-2.7.0-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting pyyaml~=6.0 (from PaPie~=0.4.0->pie-extended)\n",
      "  Downloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting termcolor>=1.1.0 (from PaPie~=0.4.0->pie-extended)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting terminaltables~=3.1.0 (from PaPie~=0.4.0->pie-extended)\n",
      "  Using cached terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting torch<2.0.0,>=1.3.1 (from PaPie~=0.4.0->pie-extended)\n",
      "  Downloading torch-1.13.1-cp38-cp38-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Collecting torchmetrics (from PaPie~=0.4.0->pie-extended)\n",
      "  Downloading torchmetrics-1.3.2-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting tqdm>=4.23.3 (from PaPie~=0.4.0->pie-extended)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing (from PaPie~=0.4.0->pie-extended)\n",
      "  Using cached typing-3.7.4.3-py3-none-any.whl\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/ebsofts/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from requests~=2.25.0->pie-extended) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/ebsofts/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from requests~=2.25.0->pie-extended) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/ebsofts/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from requests~=2.25.0->pie-extended) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/ebsofts/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from requests~=2.25.0->pie-extended) (2020.6.20)\n",
      "Collecting typing-extensions (from torch<2.0.0,>=1.3.1->PaPie~=0.4.0->pie-extended)\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<2.0.0,>=1.3.1->PaPie~=0.4.0->pie-extended)\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch<2.0.0,>=1.3.1->PaPie~=0.4.0->pie-extended)\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch<2.0.0,>=1.3.1->PaPie~=0.4.0->pie-extended)\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<2.0.0,>=1.3.1->PaPie~=0.4.0->pie-extended)\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools in /opt/ebsofts/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0.0,>=1.3.1->PaPie~=0.4.0->pie-extended) (49.2.1)\n",
      "Requirement already satisfied: wheel in /opt/ebsofts/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0.0,>=1.3.1->PaPie~=0.4.0->pie-extended) (0.35.1)\n",
      "Requirement already satisfied: packaging>17.1 in /opt/ebsofts/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from torchmetrics->PaPie~=0.4.0->pie-extended) (20.4)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics->PaPie~=0.4.0->pie-extended)\n",
      "  Downloading lightning_utilities-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/ebsofts/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from packaging>17.1->torchmetrics->PaPie~=0.4.0->pie-extended) (2.4.7)\n",
      "Requirement already satisfied: six in /opt/ebsofts/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from packaging>17.1->torchmetrics->PaPie~=0.4.0->pie-extended) (1.15.0)\n",
      "Using cached pie_extended-0.1.2-py2.py3-none-any.whl (69 kB)\n",
      "Using cached autodisambiguator-0.0.1-py2.py3-none-any.whl (11 kB)\n",
      "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading numpy-1.23.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached PaPie-0.4.2-py2.py3-none-any.whl (88 kB)\n",
      "Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Using cached Unidecode-1.1.2-py2.py3-none-any.whl (239 kB)\n",
      "Using cached JSON_minify-0.3.0-py2.py3-none-any.whl (5.2 kB)\n",
      "Using cached pytorch_optimizer-2.7.0-py3-none-any.whl (95 kB)\n",
      "Downloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (736 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m736.6/736.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
      "Downloading torch-1.13.1-cp38-cp38-manylinux1_x86_64.whl (887.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hUsing cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.11.1-py3-none-any.whl (26 kB)\n",
      "Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: JSON-minify, unidecode, typing-extensions, typing, tqdm, terminaltables, termcolor, requests, pyyaml, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, numpy, colorama, autodisambiguator, nvidia-cudnn-cu11, lightning-utilities, torch, torchmetrics, pytorch-optimizer, PaPie, pie-extended\n",
      "Successfully installed JSON-minify-0.3.0 PaPie-0.4.2 autodisambiguator-0.0.1 colorama-0.4.6 lightning-utilities-0.11.1 numpy-1.23.5 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pie-extended-0.1.2 pytorch-optimizer-2.7.0 pyyaml-6.0.1 requests-2.25.1 termcolor-2.4.0 terminaltables-3.1.10 torch-1.13.1 torchmetrics-1.3.2 tqdm-4.66.2 typing-3.7.4.3 typing-extensions-4.10.0 unidecode-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip --user\n",
    "!pip install pie-extended --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q18vBoCG1Hf0"
   },
   "source": [
    "Nous t√©l√©chargeons les mod√®les pour le fran√ßais (dit `fr`) -- une liste des mod√®les disponibles est [en ligne](https://pypi.org/project/pie-extended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pe7aPaOx0PXA",
    "outputId": "80396633-d947-4332-eb34-de4b5f4e9c1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mStarting downloading...\u001b[0m\n",
      "8 files to download\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- lemma.tar downloaded\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- pos.tar downloaded\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- mode.tar downloaded\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- temps.tar downloaded\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- pers.tar downloaded\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- nomb.tar downloaded\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- genre.tar downloaded\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- cas.tar downloaded\n",
      "\u001b[1mFinished !\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pie-extended download fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vb0cpYyzcfOR"
   },
   "source": [
    "Nous cr√©ons un fichier `essai.txt` un avec la phrase _je m'appelle Simon_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "T9FxB_S40awP"
   },
   "outputs": [],
   "source": [
    "!echo \"je m'appelle Simon\" >> $DIRECTORY/essai.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rm9WiW_acsuB"
   },
   "source": [
    "Avec `pie extended` j'annote (`tag`) avec le mod√®le `fr`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tfc5lld71XiN",
    "outputId": "89698fe6-b076-42f3-f195-efe559678e20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mGetting the tagger\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.35it/s]\n"
     ]
    }
   ],
   "source": [
    "!pie-extended tag fr $DIRECTORY/essai.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhxFlfn1dDmI"
   },
   "source": [
    "Il me cr√©e automatiquement un fichier du m√™me nom en ajoutant `-pie` au nom avant l'extension. Je peux regarder le r√©sultat en ouvrant le fichier avec la commande bash `cat`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9kYaUBrj2gk7",
    "outputId": "d26eb476-c4e2-40d7-f767-d1bfe32f2a03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token\tlemma\tPOS\tmorph\ttreated\n",
      "je\tje\tPROper\tPERS.=1|NOMB.=s|CAS=n\tje\n",
      "m'\tje\tPROper\tPERS.=1|NOMB.=s|CAS=r\tm'\n",
      "appelle\tappeler\tVERcjg\tMODE=ind|TEMPS=pst|PERS.=1|NOMB.=s\tappelle\n",
      "Simon\tSimon\tNOMpro\tNOMB.=s\tSimon\n",
      "je\tje\tPROper\tPERS.=1|NOMB.=s|CAS=n\tje\n",
      "m'\tje\tPROper\tPERS.=1|NOMB.=s|CAS=r\tm'\n",
      "appelle\tappeler\tVERcjg\tMODE=ind|TEMPS=pst|PERS.=1|NOMB.=s\tappelle\n",
      "Simon\tSimon\tADVgen\tNOMB.=s\tSimon\n",
      "je\tje\tPROper\tPERS.=1|NOMB.=s|CAS=n\tje\n",
      "m'\tje\tPROper\tPERS.=1|NOMB.=s|CAS=r\tm'\n",
      "appelle\tappeler\tVERcjg\tMODE=ind|TEMPS=pst|PERS.=1|NOMB.=s\tappelle\n",
      "Simon\tSimon\tADVgen\tNOMB.=s\tSimon\n",
      "je\tje\tPROper\tPERS.=1|NOMB.=s|CAS=n\tje\n",
      "m'\tje\tPROper\tPERS.=1|NOMB.=s|CAS=r\tm'\n",
      "appelle\tappeler\tVERcjg\tMODE=ind|TEMPS=pst|PERS.=1|NOMB.=s\tappelle\n",
      "Simon\tSimon\tNOMpro\tNOMB.=s\tSimon\n",
      "je\tje\tPROper\tPERS.=1|NOMB.=s|CAS=n\tje\n",
      "m'\tje\tPROper\tPERS.=1|NOMB.=s|CAS=r\tm'\n",
      "appelle\tappeler\tVERcjg\tMODE=ind|TEMPS=pst|PERS.=1|NOMB.=s\tappelle\n",
      "Simon\tSimon\tNOMpro\tNOMB.=s\tSimon\n"
     ]
    }
   ],
   "source": [
    "!cat $DIRECTORY/essai-pie.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJsOdi6PhsLH"
   },
   "source": [
    "### 1.2 Un test plus ambitieux\n",
    "\n",
    "Nous recommen√ßons avec un texte plus complexe: la premi√®re fable de _L‚Äô√âcole des femmes_ de Moli√®re [disponible sur wikisource]([Wikisource](https://fr.wikisource.org/wiki/L%E2%80%99%C3%89cole_des_femmes/%C3%89dition_Chasles,_1888):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "otX2BLgAeNir",
    "outputId": "b19e645d-31a0-42c7-cd6c-086febdd84c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 10246  100 10246    0     0  49871      0 --:--:-- --:--:-- --:--:-- 50722\n",
      "Sc√®ne I\n",
      "Chrysalde, Arnolphe\n",
      "chrysalde.\n",
      "Vous venez, dites-vous, pour lui donner la main ?\n",
      "arnolphe.\n",
      "Oui. Je veux terminer la chose dans demain.\n",
      "chrysalde.\n",
      "Nous sommes ici seuls ; et l‚Äôon peut, ce me semble,\n",
      "Sans craindre d‚Äô√™tre ou√Øs, y discourir ensemble.\n",
      "Voulez-vous qu‚Äôen ami je vous ouvre mon c≈ìur ?\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/gabays/32M7131/master/Cours_03/docs/moliere.txt > $DIRECTORY/moliere.txt\n",
    "!head -10 $DIRECTORY/moliere.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLn2jWsTOje9"
   },
   "source": [
    "J'annote ce fichier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zY9o1TUfe3_u",
    "outputId": "9104cd3f-40d4-40c2-c8eb-77f9e0dc59d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mGetting the tagger\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:09<00:00,  9.16s/it]\n",
      "token\tlemma\tPOS\tmorph\ttreated\n",
      "Sc√®ne\tsc√®ne\tNOMcom\tNOMB.=s|GENRE=f\tSc√®ne\n",
      "I\tI\tPROper\tNOMB.=s|GENRE=f\tI\n",
      "Chrysalde\tChrysalde\tNOMpro\tNOMB.=s|GENRE=f\tChrysalde\n",
      ",\t,\tPONfbl\tMORPH=empty\t,\n",
      "Arnolphe\tArnolphe\tNOMpro\tNOMB.=s|GENRE=m\tArnolphe\n",
      "chrysalde\tchrysalde\tNOMpro\tNOMB.=s|GENRE=f\tchrysalde\n",
      ".\t.\tPONfrt\tMORPH=empty\t.\n",
      "Vous\tvous\tPROper\tPERS.=2|NOMB.=p\tVous\n",
      "venez\tvenir\tVERcjg\tMODE=ind|TEMPS=pst|PERS.=2|NOMB.=p\tvenez\n"
     ]
    }
   ],
   "source": [
    "!pie-extended tag fr $DIRECTORY/moliere.txt\n",
    "!head -10 $DIRECTORY/moliere-pie.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avQc5QIzeujG"
   },
   "source": [
    "### 1.3 Un essai en latin\n",
    " Recommen√ßons avec un texte en latin. Il faut d'abord charger le mod√®le latin et les _addons_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4fNui7BSln5N",
    "outputId": "3114cd91-c0f4-4799-c46b-a9faf536e599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mStarting downloading...\u001b[0m\n",
      "12 files to download\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- latin-straight.json downloaded\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- latin-pos.json downloaded\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- latin-needs.json downloaded\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- Mood_Tense_Voice.tar downloaded\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- Gend.tar downloaded\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- Person.tar downloaded\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- Deg.tar downloaded\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- lemma.tar downloaded\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- pos.tar downloaded\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- Numb.tar downloaded\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- Dis.tar downloaded\n",
      "[‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà]\n",
      "- Case.tar downloaded\n",
      "\u001b[1mFinished !\u001b[0m\n",
      "\u001b[1mInstalling add-ons\u001b[0m\n",
      "\u001b[1mDone\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pie-extended download lasla\n",
    "!pie-extended install-addons lasla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RHmzoutlz9x"
   },
   "source": [
    "T√©l√©chargeons d√©sormais un texte, le _Pro Quinctio_ de Cic√©ron disponible sur [_The Latin Library_](https://www.thelatinlibrary.com/cicero/quinc.shtml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPBi_8TNlzkl",
    "outputId": "c129dd68-4c3e-41a1-b90b-3195af14f96e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 58714  100 58714    0     0   255k      0 --:--:-- --:--:-- --:--:--  258k\n",
      "M. TVLLI CICERONIS PRO P. QVINCTIO ORATIO\n",
      "\n",
      "I. Quae res in civitate duae plurimum possunt, eae contra nos ambae faciunt in hoc tempore, summa gratia et eloquentia; quarum alterum, C. Aquili, vereor, alteram metuo. Eloquentia Q. Hortensi ne me in dicendo impediat, non nihil commoveor, gratia Sex. Naevi ne P. Quinctio noceat, id vero non mediocriter pertimesco. Neque hoc tanto opere querendum videretur, haec summa in illis esse, si in nobis essent saltem mediocria; verum ita se res habet, ut ego, qui neque usu satis et ingenio parum possum, cum patrono disertissimo comparer, P. Quinctius, cui tenues opes, nullae facultates, exiguae amicorum copiae sunt, cum adversario gratiosissimo contendat. Illud quoque nobis accedit incommodum, quod M. Iunius, qui hanc causam aliquotiens apud te egit, homo et in aliis causis exercitatus et in hac multum ac saepe versatus, hoc tempore abest nova legatione impeditus, et ad me ventum est qui, ut summa haberem cetera, temporis quidem certe vix satis habui ut rem tantam, tot controversiis implicatam, possem cognoscere. Ita quod mihi consuevit in ceteris causis esse adiumento, id quoque in hac causa deficit. Nam, quod ingenio minus possum, subsidium mihi diligentia comparavi; quae quanta sit, nisi tempus et spatium datum sit, intellegi non potest. Quae quo plura sunt, C. Aquili, eo te et hos qui tibi in consilio sunt meliore mente nostra verba audire oportebit, ut multis incommodis veritas debilitata tandem aequitate talium virorum recreetur. Quod si tu iudex nullo praesidio fuisse videbere contra vim et gratiam solitudini atque inopiae, si apud hoc consilium ex opibus, non ex veritate causa pendetur, profecto nihil est iam sanctum atque sincerum in civitate, nihil est quod humilitatem cuiusquam gravitas et virtus iudicis consoletur. Certe aut apud te et hos qui tibi adsunt veritas valebit, aut ex hoc loco repulsa vi et gratia locum ubi consistat reperire non poterit.\n",
      "\n",
      "II. Non eo dico, C. Aquili, quo mihi veniat in dubium tua fides et constantia, aut quo non in his quos tibi advocavisti viris lectissimis civitatis spem summam habere P. Quinctius, debeat. Quid ergo est? Primum magnitudo periculi summo timore hominem adficit, quod uno iudicio de fortunis omnibus decernit, idque dum cogitat, non minus saepe ei venit in mentem potestatis quam aequitatis tuae, propterea quod omnes quorum in alterius manu vita posita est saepius illud cogitant, quid possit is cuius in dicione ac potestate sunt quam quid debeat facere. Deinde habet adversarium P. Quinctius verbo Sex. Naevium, re ura huiusce aetatis homines disertissimos, fortissimos, florentissimos nostrae civitatis, qui communi studio summis opibus Sex Naevium defendunt, si id est defendere, cupiditati alterius obtemperare quo is facilius quem velit iniquo iudicio opprimere possit. Nam quid hoc iniquius aut indignius, C. Aquili, dici aut commemorari potest, quam me qui caput alterius, famam fortunasque defendam priore loco causam dicere? cum praesertim Q. Hortensius qui in hoc iudicio partis accusatoris obtinet contra me sit dicturus, cui summam copiam facultatemque dicendi natura largita est. Ita fit ut ego qui tela depellere et volneribus mederi debeam tum id facere cogar cum etiam telum adversarius nullum iecerit, illis autem id tempus impugnandi detur cum et vitandi illorum impetus potestas adempta nobis erit et, si qua in re, id quod parati sunt facere, falsum crimen quasi venenatum aliquod telum iecerint, medicinae faciendae locus non erit. Id accidit praetoris iniquitate et iniuria, primum quod contra omnium consuetudinem iudicium prius de probro quam de re maluit fieri, deinde quod ita constituit id ipsum iudicium ut reus, ante quam verbum accusatoris audisset, causam dicere cogeretur. Quod eorum gratia et potentia factum ao est qui, quasi sua res aut honos agatur, ita diligenter Sex. Naevi studio et cupiditati morem gerunt et in eius modi rebus opes suas experiuntur, in quibus, quo plus propter virtutem nobilitatemque possunt, eo minus quantum possint debent ostendere.\n",
      "\n",
      "Cum tot tantisque difficultatibus adfectus atque adflictus in tuam, C. Aquili fidem, veritatem, misericordiam P. Quinctius confugerit, cum adhuc ei propter vim adversariorum non ius par, non agendi potestas; eadem, non magistratus aequus reperiri potuerit, cum ei summam per iniuriam omnia inimica atque infesta fuerint, te, C. Aquili, vosque qui in consilio adestis, orat atque obsecrat ut multis iniuriis iactatam atque agitatam aequitatem in hoc tandem loco consistere et confirmari patiamini.\n",
      "\n",
      "III. Id quo facilius facere possitis, dabo operam ut a principio res quem ad modum gesta et contracta sit cognoscatis. C. Quinctius fuit P. Quincti huius frater, sane ceterarum rerum pater familias et prudens et attentus, una in re paulo minus consideratus, qui societatem cum Sex. Naevio fecerit, viro bono, verum tamen non ita instituto ut iura societatis et officia certi patris familias nosse posset; non quo ei deesset ingenium; nam neque parum facetus scurra Sex. Naevius neque inhumanus praeco umquam est existimatus. Quid ergo est? Cum ei natura nihil melius quam vocem dedisset, pater nihil praeter libertatem reliquisset, vocem in quaestum contulit, libertate usus est quo impunius dicax esset. Qua re quidem socium tibi eum velles adiungere nihil erat nisi ut in tua pecunia condisceret qui pecuniae fructus esset; tamen inductus consuetudine ac familiaritate Quinctius fecit, ut dixi, societatem earum rerum quae in Gallia comparabantur. Erat ei pecuaria res ampla et rustica sane bene culta et fructuosa. Tollitur ab atriis Liciniis atque a praeconum consessu in Galliam Naevius et trans Alpis usque transfertur. Fit magna mutatio loci, non ingeni. Nam qui ab adulescentulo quaestum sibi instituisset sine impendio, postea quam nescio quid impendit et in commune contulit, mediocri quaestu contentus esse non poterat. Nec mirum, si is qui vocem venalem habuerat ea quae voce quaesiverat magno sibi quaestui fore putabat. Itaque hercule haud mediocriter de communi quodcumque poterat ad se in privatam domum sevocabat; qua in re ita diligens erat quasi ei qui magna fide societatem gererent arbitrium pro socio condemnari solerent. Verum his de rebus non necesse habeo dicere ea quae me P. Quinctius cupit commemorare; tametsi causa postulat, tamen quia postulat, non flagitat praeteribo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/gabays/32M7131/master/Cours_03/docs/Cicero_%20Pro_Quinctio.txt > $DIRECTORY/Cicero_%20Pro_Quinctio.txt\n",
    "!head -10 $DIRECTORY/Cicero_%20Pro_Quinctio.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuD9dfxmJb18"
   },
   "source": [
    "On annote et on observe le r√©sultat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6OEL6lIJmrVh",
    "outputId": "25d4a982-37f0-4bf7-8d73-382817168f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mGetting the tagger\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:47<00:00, 47.42s/it]\n",
      "token\tlemma\tpos\tmorph\tDis\ttreated\n",
      "M.\tMarcus\tNOMpro\tCase=Gen|Numb=Sing\t_\tM\n",
      "TVLLI\tTullius\tNOMpro\tCase=Gen|Numb=Sing\t_\tTULLI\n",
      "CICERONIS\tCiceroni\tNOMpro\tCase=Nom|Numb=Sing\t_\tCICERONIS\n",
      "PRO\tPro\tPRE\tMORPH=empty\t1\tPRO\n",
      "P.\tPublius\tNOMpro\tCase=Abl|Numb=Sing\t_\tP\n",
      "QVINCTIO\tQuintius\tNOMpro\tCase=Abl|Numb=Sing|Person=3\t_\tQUINCTIO\n",
      "ORATIO\tOratius\tNOMpro\tCase=Abl|Numb=Sing\t_\tORATIO\n",
      "I\t1\tADJcar\tMORPH=empty\t_\t1\n",
      ".\t.\tPUNC\tMORPH=empty\t_\t--IGN.--\n"
     ]
    }
   ],
   "source": [
    "!pie-extended tag lasla $DIRECTORY/Cicero_%20Pro_Quinctio.txt\n",
    "!head -10 $DIRECTORY/Cicero_%20Pro_Quinctio-pie.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bwOZivysc5p"
   },
   "source": [
    "## 2. Entra√Æner un mod√®le\n",
    "### 2.1 _Split_ pour l'entra√Ænement\n",
    "\n",
    "Nous allons d'abord faire un _split_ et cr√©er les trois jeux de donn√©es n√©cessaires avec [_Protogenie_](https://github.com/hipster-philology/protogenie).\n",
    "\n",
    "R√©cup√©rons d'abord un jeu de donn√©es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x9wG8KKIs2so",
    "outputId": "40d802ec-9c61-463a-a2c7-c1878e515d19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1917k  100 1917k    0     0  6428k      0 --:--:-- --:--:-- --:--:-- 6501k\n",
      "form\tlemma\n",
      "1536_Flores_Deplourable.tsv\txxx\n",
      "form\tlemma\n",
      "GRIMALTE\tGrimalte\n",
      "AMANT\tamant\n",
      "DE\tde\n",
      "LA\tle\n",
      "dame\tdame\n",
      "Gradisse\tGradisse\n",
      "narre\tnarrer\n",
      "sommairement\tsommairement\n",
      "Les\tle\n",
      "amoureux\tamoureux\n",
      "regredz\tregret\n",
      "de\tde\n",
      "Flamete\tFlammette\n",
      ",\t,\n",
      "qui\tqui\n",
      "furent\t√™tre\n",
      "occasion\toccasion\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/gabays/32M7131/main/Cours_03/train/lemma_Empty.txt > $DIRECTORY/lemma_Empty.txt\n",
    "!head -20 $DIRECTORY/lemma_Empty.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZQ6FN0zt1cC"
   },
   "source": [
    "Ensuite nous installons _Protogenie_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VB1QLtCUt4Cy",
    "outputId": "137ebd72-6b4b-483e-a3cf-b9f7db3690fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protogenie\n",
      "  Using cached protogenie-0.0.7-py2.py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: click<=8.0,>=7.0 in /opt/ebsofts/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from protogenie) (7.1.2)\n",
      "Collecting lxml (from protogenie)\n",
      "  Downloading lxml-5.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: regex in /opt/ebsofts/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from protogenie) (2020.10.11)\n",
      "Using cached protogenie-0.0.7-py2.py3-none-any.whl (23 kB)\n",
      "Downloading lxml-5.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml, protogenie\n",
      "Successfully installed lxml-5.1.0 protogenie-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install protogenie --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9SGk4-Et9Fp"
   },
   "source": [
    "On charge un fichier de config (pour apprendre √† les fabriquer, il existe une [documentation](https://github.com/hipster-philology/protogenie/blob/master/DOCUMENTATION.md)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EYBcNPBQuJe6",
    "outputId": "e4df25b3-ad50-4bbe-c0f7-7fddf4b4c20f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   570  100   570    0     0   3049      0 --:--:-- --:--:-- --:--:--  3114\n",
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<?xml-model href=\"protogeneia/schema.rng\" schematypens=\"http://relaxng.org/ns/structure/1.0\"?>\n",
      "<config>\n",
      "    <default-header>\n",
      "        <header type=\"explicit\">\n",
      "            <key>form</key>\n",
      "            <key>lemma</key>\n",
      "        </header>\n",
      "    </default-header>\n",
      "    <output column_marker=\"TAB\">\n",
      "        <header name=\"default\"/>\n",
      "    </output>\n",
      "    <corpora>\n",
      "        <corpus path=\"lemma_Empty.txt\" column_marker=\"TAB\">\n",
      "            <splitter name=\"empty_line\"/>\n",
      "            <header type=\"default\"/>\n",
      "        </corpus>\n",
      "    </corpora>\n",
      "</config>\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/gabays/32M7131/main/Cours_03/train/lemma_Empty.xml > $DIRECTORY/lemma_Empty.xml\n",
    "!cat $DIRECTORY/lemma_Empty.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3aZBlmfuNy2"
   },
   "source": [
    "On fait le split -- ici 10% en `dev`, 10% en `eval` (ou `test`) et 80% en `train`, mais cette r√©partition peut changer en fonction de la quantit√© de donn√©es dont on dispose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/users/g/gabays/.local/lib/python3.8/site-packages/protogenie/configs.py:157: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if not header_node or header_node.get(\"name\", \"default\") == \"default\":\n",
      "=============\n",
      "Processing...\n",
      "/home/users/g/gabays/Lemmatisation/lemma_Empty.txt has been transformed\n",
      "\t20221 tokens in test dataset\n",
      "\t19128 tokens in dev dataset\n",
      "\t153746 tokens in train dataset\n"
     ]
    }
   ],
   "source": [
    "!protogenie build $DIRECTORY/lemma_Empty.xml -d 0.1 -e 0.1 -t 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sur Colab, il est possible qu'il faille changer la valeur de l'attribut `path` dans `lemma_Empty.xml`:\n",
    "```xml\n",
    "<corpus path=\"lemma_Empty.txt\" column_marker=\"TAB\">\n",
    "```\n",
    "en \n",
    "```xml\n",
    "<corpus path=\"/content/lemma_Empty.txt\" column_marker=\"TAB\">\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2mJAStU2sih"
   },
   "source": [
    "### 2.2 Entrainement\n",
    "On va entra√Æner avec [_Pie_](https://github.com/emanjavacas/pie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "saEY2pqf3FF6",
    "outputId": "8e041389-17ef-4037-d278-c6ad0a116e01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: autodisambiguator 0.0.1\n",
      "Uninstalling autodisambiguator-0.0.1:\n",
      "  Successfully uninstalled autodisambiguator-0.0.1\n",
      "Found existing installation: colorama 0.4.6\n",
      "Uninstalling colorama-0.4.6:\n",
      "  Successfully uninstalled colorama-0.4.6\n",
      "Found existing installation: JSON-minify 0.3.0\n",
      "Uninstalling JSON-minify-0.3.0:\n",
      "  Successfully uninstalled JSON-minify-0.3.0\n",
      "Found existing installation: lightning-utilities 0.11.1\n",
      "Uninstalling lightning-utilities-0.11.1:\n",
      "  Successfully uninstalled lightning-utilities-0.11.1\n",
      "Found existing installation: lxml 5.1.0\n",
      "Uninstalling lxml-5.1.0:\n",
      "  Successfully uninstalled lxml-5.1.0\n",
      "Found existing installation: numpy 1.23.5\n",
      "Uninstalling numpy-1.23.5:\n",
      "  Successfully uninstalled numpy-1.23.5\n",
      "Found existing installation: nvidia-cublas-cu11 11.10.3.66\n",
      "Uninstalling nvidia-cublas-cu11-11.10.3.66:\n",
      "  Successfully uninstalled nvidia-cublas-cu11-11.10.3.66\n",
      "Found existing installation: nvidia-cuda-nvrtc-cu11 11.7.99\n",
      "Uninstalling nvidia-cuda-nvrtc-cu11-11.7.99:\n",
      "  Successfully uninstalled nvidia-cuda-nvrtc-cu11-11.7.99\n",
      "Found existing installation: nvidia-cuda-runtime-cu11 11.7.99\n",
      "Uninstalling nvidia-cuda-runtime-cu11-11.7.99:\n",
      "  Successfully uninstalled nvidia-cuda-runtime-cu11-11.7.99\n",
      "Found existing installation: nvidia-cudnn-cu11 8.5.0.96\n",
      "Uninstalling nvidia-cudnn-cu11-8.5.0.96:\n",
      "  Successfully uninstalled nvidia-cudnn-cu11-8.5.0.96\n",
      "Found existing installation: PaPie 0.4.2\n",
      "Uninstalling PaPie-0.4.2:\n",
      "  Successfully uninstalled PaPie-0.4.2\n",
      "Found existing installation: pie-extended 0.1.2\n",
      "Uninstalling pie-extended-0.1.2:\n",
      "  Successfully uninstalled pie-extended-0.1.2\n",
      "Found existing installation: protogenie 0.0.7\n",
      "Uninstalling protogenie-0.0.7:\n",
      "  Successfully uninstalled protogenie-0.0.7\n",
      "Found existing installation: pytorch-optimizer 2.7.0\n",
      "Uninstalling pytorch-optimizer-2.7.0:\n",
      "  Successfully uninstalled pytorch-optimizer-2.7.0\n",
      "Found existing installation: PyYAML 6.0.1\n",
      "Uninstalling PyYAML-6.0.1:\n",
      "  Successfully uninstalled PyYAML-6.0.1\n",
      "Found existing installation: requests 2.25.1\n",
      "Uninstalling requests-2.25.1:\n",
      "  Successfully uninstalled requests-2.25.1\n",
      "Found existing installation: termcolor 2.4.0\n",
      "Uninstalling termcolor-2.4.0:\n",
      "  Successfully uninstalled termcolor-2.4.0\n",
      "Found existing installation: terminaltables 3.1.10\n",
      "Uninstalling terminaltables-3.1.10:\n",
      "  Successfully uninstalled terminaltables-3.1.10\n",
      "Found existing installation: torch 1.13.1\n",
      "Uninstalling torch-1.13.1:\n",
      "  Successfully uninstalled torch-1.13.1\n",
      "Found existing installation: torchmetrics 1.3.2\n",
      "Uninstalling torchmetrics-1.3.2:\n",
      "  Successfully uninstalled torchmetrics-1.3.2\n",
      "Found existing installation: tqdm 4.66.2\n",
      "Uninstalling tqdm-4.66.2:\n",
      "  Successfully uninstalled tqdm-4.66.2\n",
      "Found existing installation: typing 3.7.4.3\n",
      "Uninstalling typing-3.7.4.3:\n",
      "  Successfully uninstalled typing-3.7.4.3\n",
      "Found existing installation: typing_extensions 4.10.0\n",
      "Uninstalling typing_extensions-4.10.0:\n",
      "  Successfully uninstalled typing_extensions-4.10.0\n",
      "Found existing installation: Unidecode 1.1.2\n",
      "Uninstalling Unidecode-1.1.2:\n",
      "  Successfully uninstalled Unidecode-1.1.2\n",
      "Collecting nlp-pie\n",
      "  Using cached nlp_pie-0.3.8-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting lxml>=4.2.1 (from nlp-pie)\n",
      "  Using cached lxml-5.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting JSON-minify>=0.3.0 (from nlp-pie)\n",
      "  Using cached JSON_minify-0.3.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting gensim>=3.4.0 (from nlp-pie)\n",
      "  Downloading gensim-4.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting tqdm>=4.23.3 (from nlp-pie)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting numpy<1.18.0,>=1.14.3 (from nlp-pie)\n",
      "  Downloading numpy-1.17.5-cp38-cp38-manylinux1_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting termcolor>=1.1.0 (from nlp-pie)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting scikit-learn<0.23.0,>=0.19.1 (from nlp-pie)\n",
      "  Downloading scikit_learn-0.22.2.post1-cp38-cp38-manylinux1_x86_64.whl.metadata (7.2 kB)\n",
      "Collecting terminaltables==3.1.0 (from nlp-pie)\n",
      "  Using cached terminaltables-3.1.0-py3-none-any.whl\n",
      "Collecting torch<=1.7.1,>=1.3.1 (from nlp-pie)\n",
      "  Downloading torch-1.7.1-cp38-cp38-manylinux1_x86_64.whl.metadata (22 kB)\n",
      "Collecting pyyaml==5.1b3 (from nlp-pie)\n",
      "  Using cached PyYAML-5.1b3-cp38-cp38-linux_x86_64.whl\n",
      "Collecting typing<4.0 (from nlp-pie)\n",
      "  Using cached typing-3.7.4.3-py3-none-any.whl\n",
      "Requirement already satisfied: click<8.0,>=7.0 in /opt/ebsofts/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from nlp-pie) (7.1.2)\n",
      "INFO: pip is looking at multiple versions of gensim to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting gensim>=3.4.0 (from nlp-pie)\n",
      "  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.6 kB)\n",
      "  Downloading gensim-4.3.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (8.6 kB)\n",
      "  Downloading gensim-4.2.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (8.5 kB)\n",
      "Collecting scipy>=0.18.1 (from gensim>=3.4.0->nlp-pie)\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smart-open>=1.8.1 (from gensim>=3.4.0->nlp-pie)\n",
      "  Downloading smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/ebsofts/Python/3.8.6-GCCcore-10.2.0/lib/python3.8/site-packages (from scikit-learn<0.23.0,>=0.19.1->nlp-pie) (0.17.0)\n",
      "Collecting typing-extensions (from torch<=1.7.1,>=1.3.1->nlp-pie)\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting scipy>=0.18.1 (from gensim>=3.4.0->nlp-pie)\n",
      "  Downloading scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading scipy-1.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading scipy-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading scipy-1.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "  Downloading scipy-1.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "  Downloading scipy-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting wrapt (from smart-open>=1.8.1->gensim>=3.4.0->nlp-pie)\n",
      "  Downloading wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Downloading nlp_pie-0.3.8-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.3/84.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gensim-4.2.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached JSON_minify-0.3.0-py2.py3-none-any.whl (5.2 kB)\n",
      "Using cached lxml-5.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
      "Downloading numpy-1.17.5-cp38-cp38-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-0.22.2.post1-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading torch-1.7.1-cp38-cp38-manylinux1_x86_64.whl (776.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m776.8/776.8 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Downloading scipy-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.6/41.6 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Downloading wrapt-1.16.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m83.4/83.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: terminaltables, JSON-minify, wrapt, typing-extensions, typing, tqdm, termcolor, pyyaml, numpy, lxml, torch, smart-open, scipy, scikit-learn, gensim, nlp-pie\n",
      "Successfully installed JSON-minify-0.3.0 gensim-4.2.0 lxml-5.1.0 nlp-pie-0.3.8 numpy-1.17.5 pyyaml-5.1b3 scikit-learn-0.22.2.post1 scipy-1.8.1 smart-open-7.0.4 termcolor-2.4.0 terminaltables-3.1.0 torch-1.7.1 tqdm-4.66.2 typing-3.7.4.3 typing-extensions-4.10.0 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip freeze --user --exclude-editable | xargs pip uninstall -y\n",
    "!pip install nlp-pie --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRqa1P3IAfA9"
   },
   "source": [
    "On a besoin de renommer les fichiers cr√©√©s avec _Protog√©nie_ en `.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "9SGEWJ-e4I6J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: cannot move 'output' to 'Lemmatisation/output': File exists\n"
     ]
    }
   ],
   "source": [
    "!mv output/dev/lemma_Empty.txt output/dev/lemma_Empty.tsv\n",
    "!mv output/test/lemma_Empty.txt output/test/lemma_Empty.tsv\n",
    "!mv output/train/lemma_Empty.txt output/train/lemma_Empty.tsv\n",
    "!mv output $DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9Gxq65l6Buy"
   },
   "source": [
    "Sur colab: rajouter `/content/` devant `output/`.\n",
    "\n",
    "Nous avons aussi besoin d'un fichier de configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xqm6DFu06Fig",
    "outputId": "41a827e6-74d2-4c38-f957-5fbdd812a29a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1952  100  1952    0     0   9932      0 --:--:-- --:--:-- --:--:-- 10061\n",
      "{\n",
      "   \"modelname\":\"mymodel\",\n",
      "   \"modelpath\":\"./\",\n",
      "   \"run_test\":false,\n",
      "   \"input_path\":\"Lemmatisation/output/train/lemma_Empty.tsv\",\n",
      "   \"test_path\":\"Lemmatisation/output/test/lemma_Empty.tsv\",\n",
      "   \"dev_path\":\"Lemmatisation/output/dev/lemma_Empty.tsv\",\n",
      "   \"header\":true,\n",
      "   \"sep\":\"\\t\",\n",
      "   \"breakline_ref\":\"lemma\",\n",
      "   \"breakline_data\":\"NONE\",\n",
      "   \"char_max_size\":500,\n",
      "   \"word_max_size\":20000,\n",
      "   \"max_sent_len\":35,\n",
      "   \"max_sents\":1000000,\n",
      "   \"char_min_freq\":1,\n",
      "   \"word_min_freq\":1,\n",
      "   \"char_eos\":true,\n",
      "   \"char_bos\":true,\n",
      "   \"char_lower\":false,\n",
      "   \"tasks\":[\n",
      "      {\n",
      "         \"name\":\"lemma\",\n",
      "         \"target\":true,\n",
      "         \"context\":\"sentence\",\n",
      "         \"level\":\"char\",\n",
      "         \"decoder\":\"attentional\",\n",
      "         \"settings\":{\n",
      "            \"bos\":true,\n",
      "            \"eos\":true,\n",
      "            \"lower\":false,\n",
      "            \"target\":\"lemma\"\n",
      "         },\n",
      "         \"layer\":-1\n",
      "      }\n",
      "   ],\n",
      "   \"task_defaults\":{\n",
      "      \"level\":\"token\",\n",
      "      \"layer\":-1,\n",
      "      \"decoder\":\"linear\",\n",
      "      \"context\":\"sentence\"\n",
      "   },\n",
      "   \"patience\":7,\n",
      "   \"factor\":0.5,\n",
      "   \"threshold\":0.0001,\n",
      "   \"min_weight\":0.2,\n",
      "   \"include_lm\":true,\n",
      "   \"lm_shared_softmax\":true,\n",
      "   \"lm_schedule\":{\n",
      "      \"patience\":2,\n",
      "      \"factor\":0.5,\n",
      "      \"weight\":0.2,\n",
      "      \"mode\":\"min\"\n",
      "   },\n",
      "   \"batch_size\":128,\n",
      "   \"epochs\":10,\n",
      "   \"dropout\":0.25,\n",
      "   \"word_dropout\":0,\n",
      "   \"lr\":0.001,\n",
      "   \"lr_patience\":4,\n",
      "   \"optimizer\":\"Adam\",\n",
      "   \"clip_norm\":5,\n",
      "   \"linear_layers\":1,\n",
      "   \"hidden_size\":256,\n",
      "   \"num_layers\":1,\n",
      "   \"cell\":\"GRU\",\n",
      "   \"wemb_dim\":0,\n",
      "   \"merge_type\":\"concat\",\n",
      "   \"cemb_dim\":400,\n",
      "   \"cemb_type\":\"rnn\",\n",
      "   \"cemb_layers\":2,\n",
      "   \"decoder_layers\":3,\n",
      "   \"custom_cemb_cell\":false,\n",
      "   \"checks_per_epoch\":1,\n",
      "   \"report_freq\":200,\n",
      "   \"verbose\":true,\n",
      "   \"device\":\"cuda\",\n",
      "   \"buffer_size\":10000,\n",
      "   \"minimize_pad\":false,\n",
      "   \"shuffle\":true,\n",
      "   \"pretrain_embeddings\":false,\n",
      "   \"load_pretrained_embeddings\":\"\",\n",
      "   \"load_pretrained_encoder\":\"\",\n",
      "   \"freeze_embeddings\":false,\n",
      "   \"scorer\":\"general\",\n",
      "   \"cache_dataset\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl https://raw.githubusercontent.com/gabays/32M7131/main/Cours_03/train/config.json > $DIRECTORY/config.json\n",
    "!cat $DIRECTORY/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJzaEqNjB9Fo"
   },
   "source": [
    "Ce fichier contient toutes les informations n√©cessaires pour l'entra√Ænement. Le param√©trage via le fichier de configuration est extr√™mement complexe, et il est difficile (impossible?) de conna√Ætre √† l'avance les meilleurs choix √† op√©rer: il faut souvent faire de nombreux tests.\n",
    "\n",
    "Sur colab: changer la valeur de `input_path`, `test_path` et `dev_path` en rempla√ßant `Lemmatisation` par `/content`.\n",
    "\n",
    "Si vous voulez un peu comprendre le contenu, voici quelques explications:\n",
    "```json\n",
    "{\n",
    "  // * General\n",
    "  // model name to be used for saving\n",
    "  \"modelname\": \"latest-fro\",\n",
    "  // model path to be used for saving\n",
    "  \"modelpath\": \"./\",\n",
    "  // run test (no serialization)\n",
    "  \"run_test\": false,\n",
    "  // max length of sentences (longer sentence will be split)\n",
    "  \"max_sent_len\": 35,\n",
    "  // maximum number of sentences to process\n",
    "  \"max_sents\": 1000000,\n",
    "  // * Data\n",
    "  // path or unix-like expression to file(s)/dir with training data:\n",
    "  // e.g. \"datasets/capitula_classic/train0.tsv\"\"\n",
    "  \"input_path\": \"corpus_for_train/train/CORPUS.tsv\",\n",
    "  // path to test set (same format as input_path)\n",
    "  \"test_path\": \"corpus_for_train/test/CORPUS.tsv\",\n",
    "  // path to dev set (same format as input_path)\n",
    "  \"dev_path\": \"corpus_for_train/dev/CORPUS.tsv\",\n",
    "  // data to use as reference for breaking lines (e.g. \"pos\")\n",
    "  \"breakline_ref\": \"POS\",\n",
    "  // needed to decide for a sentence boundary (e.g. \"$.\")\n",
    "  \"breakline_data\": \"INUTILE\",\n",
    "  // maximum vocabulary size\n",
    "  \"char_max_size\": 500,\n",
    "  // maximum vocabulary size for word input\n",
    "  \"word_max_size\": 20000,\n",
    "  // min freq per item to be incorporated in the vocabulary (only used if *_max_size is 0)\n",
    "  \"char_min_freq\": 1,\n",
    "  \"word_min_freq\": 1,\n",
    "  // char-level encoding\n",
    "  \"char_eos\": true,\n",
    "  \"char_bos\": true,\n",
    "  // tab-format only:\n",
    "  \"header\": true,\n",
    "  // separator for csv-like files\n",
    "  \"sep\": \"\\t\",\n",
    "  // task-related config\n",
    "  \"tasks\": [\n",
    "    // each task's name refers to the corresponding data field\n",
    "    // this behaviour can be changed in case the name differs from the data field\n",
    "    // by using a \"target\" key that refers to the target data field\n",
    "    // e.g. {\"name\": \"lemma-char\", \"settings\": {\"target\": \"lemma\"}}\n",
    "    // e.g. {\"name\": \"lemma-word\", \"settings\": {\"target\": \"lemma\"}}\n",
    "    {\n",
    "      \"name\": \"lemma\",\n",
    "      \"target\": true,\n",
    "      \"context\": \"sentence\",\n",
    "      \"level\": \"char\",\n",
    "      \"decoder\": \"attentional\",\n",
    "      \"settings\": {\n",
    "        \"bos\": true,\n",
    "        \"eos\": true,\n",
    "        \"lower\": true,\n",
    "        \"target\": \"lemma\"\n",
    "      },\n",
    "      \"layer\": -1\n",
    "    },\n",
    "    {\"name\": \"POS\"}\n",
    "  ],\n",
    "  \"task_defaults\": {\n",
    "    \"level\": \"token\",\n",
    "    \"layer\": -1,\n",
    "    \"decoder\": \"linear\",\n",
    "    \"context\": \"sentence\"\n",
    "  },\n",
    "  // general task schedule params (can be overwritten in the \"settings\" entry of each)\n",
    "  \"patience\": 1000000, // default to very large value\n",
    "  \"factor\": 1, // decrease the loss weight by this amount (0, 1)\n",
    "  \"threshold\": 0, // minimum decrease in loss to be considered an improvement\n",
    "  \"min_weight\": 0, // minimum value a task weight can be decreased to\n",
    "\n",
    "  // whether to include autoregressive loss\n",
    "  \"include_lm\": true,\n",
    "  // whether to share the output layer for both fwd and bwd lm\n",
    "  \"lm_shared_softmax\": false,\n",
    "  // lm settings in case it's included as an extra task\n",
    "  \"lm_schedule\": {\n",
    "    \"patience\": 100, \"factor\": 0.5, \"weight\": 0.2, \"mode\": \"min\"\n",
    "  }\n",
    "}\n",
    "  ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWO-357V6Wdl"
   },
   "source": [
    "Et on entraine. Le nombre d'it√©rations (_epochs_) est volontairement limit√© √† 10 pour ne pas gaspiller trop d'√©nergie inutilement. En augmentant ce nombre, on obtiendrait un bien meilleur r√©sultat.\n",
    "\n",
    "Vous allez voir appara√Ætre les scores sur le `dev` our chaque it√©ration. Pour lire les r√©sultats, [cf. pr√©cision et rappel sur wikipedia](https://fr.wikipedia.org/wiki/Pr%C3%A9cision_et_rappel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NVNetesS6g4x",
    "outputId": "f1b328eb-e42d-46ac-e511-d61221da9941"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: typing 3.7.4.3\n",
      "Uninstalling typing-3.7.4.3:\n",
      "  Successfully uninstalled typing-3.7.4.3\n",
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "WARNING:root:\n",
      "It seems like you downloaded `pie` instead of git-cloning it or installing it with pip.\n",
      "We won't be able to check compatibility between pretrained models and `pie` version.\n",
      "\n",
      "\n",
      "::: Loaded Config :::\n",
      "\n",
      "batch_size: 128\n",
      "breakline_data: NONE\n",
      "breakline_ref: lemma\n",
      "buffer_size: 10000\n",
      "cache_dataset: true\n",
      "cell: GRU\n",
      "cemb_dim: 400\n",
      "cemb_layers: 2\n",
      "cemb_type: rnn\n",
      "char_bos: true\n",
      "char_eos: true\n",
      "char_lower: false\n",
      "char_max_size: 500\n",
      "char_min_freq: 1\n",
      "checks_per_epoch: 1\n",
      "clip_norm: 5\n",
      "config_path: Lemmatisation/config.json\n",
      "custom_cemb_cell: false\n",
      "decoder_layers: 3\n",
      "dev_path: Lemmatisation/output/dev/lemma_Empty.tsv\n",
      "device: cuda\n",
      "drop_diacritics: false\n",
      "dropout: 0.25\n",
      "epochs: 10\n",
      "factor: 0.5\n",
      "freeze_embeddings: false\n",
      "header: true\n",
      "hidden_size: 256\n",
      "include_lm: true\n",
      "init_rnn: default\n",
      "input_path: Lemmatisation/output/train/lemma_Empty.tsv\n",
      "linear_layers: 1\n",
      "lm_schedule:\n",
      "  factor: 0.5\n",
      "  mode: min\n",
      "  patience: 2\n",
      "  weight: 0.2\n",
      "lm_shared_softmax: true\n",
      "load_pretrained_embeddings: ''\n",
      "load_pretrained_encoder: ''\n",
      "lr: 0.001\n",
      "lr_factor: 0.75\n",
      "lr_patience: 4\n",
      "max_sent_len: 35\n",
      "max_sents: 1000000\n",
      "merge_type: concat\n",
      "min_lr: 1.0e-06\n",
      "min_weight: 0.2\n",
      "minimize_pad: false\n",
      "modelname: mymodel\n",
      "modelpath: ./\n",
      "num_layers: 1\n",
      "optimizer: Adam\n",
      "patience: 7\n",
      "pretrain_embeddings: false\n",
      "report_freq: 200\n",
      "run_test: false\n",
      "scorer: general\n",
      "sep: \"\\t\"\n",
      "shuffle: true\n",
      "task_defaults:\n",
      "  context: sentence\n",
      "  decoder: linear\n",
      "  layer: -1\n",
      "  level: token\n",
      "tasks:\n",
      "- context: sentence\n",
      "  decoder: attentional\n",
      "  layer: -1\n",
      "  level: char\n",
      "  name: lemma\n",
      "  settings:\n",
      "    bos: true\n",
      "    eos: true\n",
      "    lower: false\n",
      "    target: lemma\n",
      "  target: true\n",
      "tasks_order:\n",
      "- lemma\n",
      "- pos\n",
      "test_path: Lemmatisation/output/test/lemma_Empty.tsv\n",
      "threshold: 0.0001\n",
      "utfnorm: false\n",
      "utfnorm_type: NFKD\n",
      "verbose: true\n",
      "wemb_dim: 0\n",
      "word_dropout: 0\n",
      "word_lower: false\n",
      "word_max_size: 20000\n",
      "word_min_freq: 1\n",
      "\n",
      "Using seed: 130347\n",
      "::: Available tasks :::\n",
      "\n",
      "- lemma\n",
      "\n",
      "::: Fitting data :::\n",
      "\n",
      "\n",
      "::: Vocabulary :::\n",
      "\n",
      "- word            types=19402/19402=1.00 tokens=155714/155714=1.00\n",
      "- char            types=111/111=1.00 tokens=630092/630092=1.00\n",
      "\n",
      "::: Tasks :::\n",
      "\n",
      "- lemma           target=lemma  level=char   vocab=108   \n",
      "\n",
      "Initializing GRU with scheme: xavier_uniform\n",
      "Initializing GRU with scheme: xavier_uniform\n",
      "Initializing GRU with scheme: xavier_uniform\n",
      "::: Model :::\n",
      "\n",
      "SimpleModel(\n",
      "  (cemb): RNNEmbedding(\n",
      "    (emb): Embedding(115, 400, padding_idx=2)\n",
      "    (rnn): GRU(400, 400, num_layers=2, dropout=0.25, bidirectional=True)\n",
      "  )\n",
      "  (encoder): RNNEncoder(\n",
      "    (rnn): ModuleList(\n",
      "      (0): GRU(800, 256, bidirectional=True)\n",
      "    )\n",
      "  )\n",
      "  (lemma_decoder): AttentionalDecoder(\n",
      "    (embs): Embedding(108, 400)\n",
      "    (rnn): GRU(912, 800, num_layers=2, dropout=0.25)\n",
      "    (attn): Attention(\n",
      "      (scorer): GeneralScorer(\n",
      "        (W_a): Linear(in_features=800, out_features=800, bias=False)\n",
      "      )\n",
      "      (linear_out): Linear(in_features=1600, out_features=800, bias=False)\n",
      "    )\n",
      "    (proj): Linear(in_features=800, out_features=108, bias=True)\n",
      "  )\n",
      "  (lm_fwd_decoder): LinearDecoder(\n",
      "    (decoder): Linear(in_features=256, out_features=19404, bias=True)\n",
      "  )\n",
      "  (lm_bwd_decoder): LinearDecoder(\n",
      "    (decoder): Linear(in_features=256, out_features=19404, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "::: Model parameters :::\n",
      "\n",
      "21475624/21475624 trainable/total\n",
      "\n",
      "Starting training\n",
      "\n",
      "Evaluation check every 57/58 batches\n",
      "\n",
      "::: Task schedules :::\n",
      "\n",
      "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
      "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"-inf\"/>\n",
      "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"inf\"/>\n",
      "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"inf\"/>\n",
      "</TaskScheduler>\n",
      "\n",
      "::: LR schedule :::\n",
      "\n",
      "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
      "\n",
      "\n",
      "Evaluating model on dev set...\n",
      "\n",
      "7it [00:01,  3.79it/s]\n",
      "\n",
      "::: Dev losses :::\n",
      "\n",
      "lemma: 2.2119\n",
      "lm_fwd: 7.2159\n",
      "lm_bwd: 7.3387\n",
      "\n",
      "7it [00:01,  6.58it/s]\n",
      "\n",
      "## lemma\n",
      "\n",
      "|                  | accuracy | precision | recall | support |\n",
      "|------------------|----------|-----------|--------|---------|\n",
      "| all              | 0.1374   | 0.0006    | 0.0011 | 18498   |\n",
      "| known-tokens     | 0.1504   | 0.001     | 0.0012 | 16894   |\n",
      "| unknown-tokens   | 0.0006   | 0.0001    | 0.0007 | 1604    |\n",
      "| ambiguous-tokens | 0.0688   | 0.0009    | 0.0014 | 6815    |\n",
      "| unknown-targets  | 0.0      | 0.0       | 0.0    | 695     |\n",
      "\n",
      "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
      "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.1374\"/>\n",
      "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.215931688036237\"/>\n",
      "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.33868796484811\"/>\n",
      "</TaskScheduler>\n",
      "\n",
      "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
      "\n",
      "\n",
      "Evaluating model on dev set...\n",
      "\n",
      "7it [00:01,  4.56it/s]\n",
      "\n",
      "::: Dev losses :::\n",
      "\n",
      "lemma: 1.7639\n",
      "lm_fwd: 7.1855\n",
      "lm_bwd: 7.2653\n",
      "\n",
      "7it [00:01,  3.81it/s]\n",
      "\n",
      "## lemma\n",
      "\n",
      "|                  | accuracy | precision | recall | support |\n",
      "|------------------|----------|-----------|--------|---------|\n",
      "| all              | 0.3569   | 0.0036    | 0.0049 | 18498   |\n",
      "| known-tokens     | 0.3907   | 0.0053    | 0.0067 | 16894   |\n",
      "| unknown-tokens   | 0.0012   | 0.0002    | 0.0011 | 1604    |\n",
      "| ambiguous-tokens | 0.5266   | 0.0178    | 0.0189 | 6815    |\n",
      "| unknown-targets  | 0.0      | 0.0       | 0.0    | 695     |\n",
      "\n",
      "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
      "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.3569\"/>\n",
      "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.185507161276681\"/>\n",
      "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.265271391187396\"/>\n",
      "</TaskScheduler>\n",
      "\n",
      "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
      "\n",
      "\n",
      "Evaluating model on dev set...\n",
      "\n",
      "7it [00:01,  3.81it/s]\n",
      "\n",
      "::: Dev losses :::\n",
      "\n",
      "lemma: 1.1281\n",
      "lm_fwd: 7.1228\n",
      "lm_bwd: 7.2100\n",
      "\n",
      "7it [00:02,  2.64it/s]\n",
      "\n",
      "## lemma\n",
      "\n",
      "|                  | accuracy | precision | recall | support |\n",
      "|------------------|----------|-----------|--------|---------|\n",
      "| all              | 0.5271   | 0.0138    | 0.0133 | 18498   |\n",
      "| known-tokens     | 0.5762   | 0.0201    | 0.0188 | 16894   |\n",
      "| unknown-tokens   | 0.0094   | 0.0036    | 0.0043 | 1604    |\n",
      "| ambiguous-tokens | 0.7733   | 0.0568    | 0.0551 | 6815    |\n",
      "| unknown-targets  | 0.0      | 0.0       | 0.0    | 695     |\n",
      "\n",
      "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
      "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.5271\"/>\n",
      "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.1228118624006\"/>\n",
      "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.209986277988979\"/>\n",
      "</TaskScheduler>\n",
      "\n",
      "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
      "\n",
      "\n",
      "Evaluating model on dev set...\n",
      "\n",
      "7it [00:02,  3.28it/s]\n",
      "\n",
      "::: Dev losses :::\n",
      "\n",
      "lemma: 0.7939\n",
      "lm_fwd: 7.0897\n",
      "lm_bwd: 7.1949\n",
      "\n",
      "7it [00:03,  2.19it/s]\n",
      "\n",
      "## lemma\n",
      "\n",
      "|                  | accuracy | precision | recall | support |\n",
      "|------------------|----------|-----------|--------|---------|\n",
      "| all              | 0.6289   | 0.032     | 0.0314 | 18498   |\n",
      "| known-tokens     | 0.6862   | 0.0477    | 0.0469 | 16894   |\n",
      "| unknown-tokens   | 0.0256   | 0.0092    | 0.0098 | 1604    |\n",
      "| ambiguous-tokens | 0.8282   | 0.1062    | 0.1122 | 6815    |\n",
      "| unknown-targets  | 0.0      | 0.0       | 0.0    | 695     |\n",
      "\n",
      "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
      "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.6289\"/>\n",
      "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.089664595467704\"/>\n",
      "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.194855349404471\"/>\n",
      "</TaskScheduler>\n",
      "\n",
      "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
      "\n",
      "\n",
      "Evaluating model on dev set...\n",
      "\n",
      "7it [00:02,  3.35it/s]\n",
      "\n",
      "::: Dev losses :::\n",
      "\n",
      "lemma: 0.5820\n",
      "lm_fwd: 7.0600\n",
      "lm_bwd: 7.1750\n",
      "\n",
      "7it [00:02,  2.55it/s]\n",
      "\n",
      "## lemma\n",
      "\n",
      "|                  | accuracy | precision | recall | support |\n",
      "|------------------|----------|-----------|--------|---------|\n",
      "| all              | 0.6884   | 0.0619    | 0.0591 | 18498   |\n",
      "| known-tokens     | 0.7494   | 0.0937    | 0.0893 | 16894   |\n",
      "| unknown-tokens   | 0.0461   | 0.0203    | 0.0215 | 1604    |\n",
      "| ambiguous-tokens | 0.8735   | 0.1757    | 0.1832 | 6815    |\n",
      "| unknown-targets  | 0.0115   | 0.006     | 0.006  | 695     |\n",
      "\n",
      "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
      "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.6884\"/>\n",
      "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.060012749263218\"/>\n",
      "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.1749686513628275\"/>\n",
      "</TaskScheduler>\n",
      "\n",
      "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
      "\n",
      "\n",
      "Evaluating model on dev set...\n",
      "\n",
      "7it [00:02,  3.44it/s]\n",
      "\n",
      "::: Dev losses :::\n",
      "\n",
      "lemma: 0.4175\n",
      "lm_fwd: 7.0385\n",
      "lm_bwd: 7.1567\n",
      "\n",
      "7it [00:02,  2.42it/s]\n",
      "\n",
      "## lemma\n",
      "\n",
      "|                  | accuracy | precision | recall | support |\n",
      "|------------------|----------|-----------|--------|---------|\n",
      "| all              | 0.7542   | 0.1132    | 0.1092 | 18498   |\n",
      "| known-tokens     | 0.8177   | 0.1728    | 0.1665 | 16894   |\n",
      "| unknown-tokens   | 0.0854   | 0.0423    | 0.0421 | 1604    |\n",
      "| ambiguous-tokens | 0.8935   | 0.2543    | 0.2763 | 6815    |\n",
      "| unknown-targets  | 0.0317   | 0.0168    | 0.0163 | 695     |\n",
      "\n",
      "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
      "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.7542\"/>\n",
      "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.038529872894287\"/>\n",
      "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.156723635537284\"/>\n",
      "</TaskScheduler>\n",
      "\n",
      "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
      "\n",
      "\n",
      "Evaluating model on dev set...\n",
      "\n",
      "7it [00:02,  3.32it/s]\n",
      "\n",
      "::: Dev losses :::\n",
      "\n",
      "lemma: 0.3187\n",
      "lm_fwd: 7.0107\n",
      "lm_bwd: 7.1201\n",
      "\n",
      "7it [00:03,  2.06it/s]\n",
      "\n",
      "## lemma\n",
      "\n",
      "|                  | accuracy | precision | recall | support |\n",
      "|------------------|----------|-----------|--------|---------|\n",
      "| all              | 0.7977   | 0.175     | 0.1704 | 18498   |\n",
      "| known-tokens     | 0.8591   | 0.2635    | 0.258  | 16894   |\n",
      "| unknown-tokens   | 0.1502   | 0.077     | 0.0762 | 1604    |\n",
      "| ambiguous-tokens | 0.9036   | 0.3381    | 0.3643 | 6815    |\n",
      "| unknown-targets  | 0.0647   | 0.0343    | 0.0338 | 695     |\n",
      "\n",
      "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
      "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.7977\"/>\n",
      "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.010719912392752\"/>\n",
      "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.120087964194162\"/>\n",
      "</TaskScheduler>\n",
      "\n",
      "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
      "\n",
      "\n",
      "Evaluating model on dev set...\n",
      "\n",
      "7it [00:01,  3.94it/s]\n",
      "\n",
      "::: Dev losses :::\n",
      "\n",
      "lemma: 0.2595\n",
      "lm_fwd: 6.9628\n",
      "lm_bwd: 7.0817\n",
      "\n",
      "7it [00:02,  2.38it/s]\n",
      "\n",
      "## lemma\n",
      "\n",
      "|                  | accuracy | precision | recall | support |\n",
      "|------------------|----------|-----------|--------|---------|\n",
      "| all              | 0.8252   | 0.2243    | 0.2194 | 18498   |\n",
      "| known-tokens     | 0.884    | 0.3356    | 0.3312 | 16894   |\n",
      "| unknown-tokens   | 0.2064   | 0.1078    | 0.1064 | 1604    |\n",
      "| ambiguous-tokens | 0.9073   | 0.3701    | 0.4062 | 6815    |\n",
      "| unknown-targets  | 0.0791   | 0.0422    | 0.0419 | 695     |\n",
      "\n",
      "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
      "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.8252\"/>\n",
      "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"6.962767532893589\"/>\n",
      "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.081716537475586\"/>\n",
      "</TaskScheduler>\n",
      "\n",
      "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
      "\n",
      "\n",
      "Evaluating model on dev set...\n",
      "\n",
      "7it [00:02,  3.40it/s]\n",
      "\n",
      "::: Dev losses :::\n",
      "\n",
      "lemma: 0.2168\n",
      "lm_fwd: 6.9204\n",
      "lm_bwd: 7.0217\n",
      "\n",
      "7it [00:03,  2.15it/s]\n",
      "\n",
      "## lemma\n",
      "\n",
      "|                  | accuracy | precision | recall | support |\n",
      "|------------------|----------|-----------|--------|---------|\n",
      "| all              | 0.8488   | 0.2805    | 0.2771 | 18498   |\n",
      "| known-tokens     | 0.9049   | 0.4229    | 0.4205 | 16894   |\n",
      "| unknown-tokens   | 0.2581   | 0.1423    | 0.1409 | 1604    |\n",
      "| ambiguous-tokens | 0.9137   | 0.4457    | 0.4716 | 6815    |\n",
      "| unknown-targets  | 0.1094   | 0.0585    | 0.0582 | 695     |\n",
      "\n",
      "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
      "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.8488\"/>\n",
      "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"6.920354638780866\"/>\n",
      "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.021717412131173\"/>\n",
      "</TaskScheduler>\n",
      "\n",
      "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
      "\n",
      "\n",
      "Evaluating model on dev set...\n",
      "\n",
      "7it [00:01,  3.63it/s]\n",
      "\n",
      "::: Dev losses :::\n",
      "\n",
      "lemma: 0.1902\n",
      "lm_fwd: 6.8770\n",
      "lm_bwd: 6.9906\n",
      "\n",
      "7it [00:03,  2.33it/s]\n",
      "\n",
      "## lemma\n",
      "\n",
      "|                  | accuracy | precision | recall | support |\n",
      "|------------------|----------|-----------|--------|---------|\n",
      "| all              | 0.8593   | 0.3136    | 0.3091 | 18498   |\n",
      "| known-tokens     | 0.9152   | 0.4766    | 0.4745 | 16894   |\n",
      "| unknown-tokens   | 0.27     | 0.1522    | 0.1503 | 1604    |\n",
      "| ambiguous-tokens | 0.9177   | 0.4692    | 0.4975 | 6815    |\n",
      "| unknown-targets  | 0.1237   | 0.0678    | 0.0673 | 695     |\n",
      "\n",
      "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
      "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.8593\"/>\n",
      "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"6.876976830618722\"/>\n",
      "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"6.9906173433576315\"/>\n",
      "</TaskScheduler>\n",
      "\n",
      "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
      "\n",
      "Evaluating model on test set\n",
      "8it [00:03,  2.24it/s]\n",
      "\n",
      "## lemma\n",
      "\n",
      "|                  | accuracy | precision | recall | support |\n",
      "|------------------|----------|-----------|--------|---------|\n",
      "| all              | 0.8697   | 0.3312    | 0.3306 | 18883   |\n",
      "| known-tokens     | 0.9204   | 0.4932    | 0.4944 | 17366   |\n",
      "| unknown-tokens   | 0.29     | 0.1581    | 0.1582 | 1517    |\n",
      "| ambiguous-tokens | 0.921    | 0.5027    | 0.5345 | 6975    |\n",
      "| unknown-targets  | 0.1493   | 0.0768    | 0.0768 | 683     |\n",
      "\n",
      "Saved best model to: [./mymodel-lemma-2024_03_26-13_13_02.tar]\n",
      "7it [00:01,  3.66it/s]\n",
      "Bye!\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall typing --yes\n",
    "!pie train $DIRECTORY/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K6I7Y-o8gjvy",
    "outputId": "6b7fb83b-5e61-4cab-fe33-123848951274"
   },
   "outputs": [],
   "source": [
    "!mv mymodel-lemma-*.tar mymodel.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OW0AXvF-QudR"
   },
   "source": [
    "On peut d√©sormais utiliser ce mod√®le avec _Pie_:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0Oj_fEwQ28V",
    "outputId": "e0447318-7335-468f-f996-e1d60379d878"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: not a git repository (or any parent up to mount point /)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n",
      "WARNING:root:\n",
      "It seems like you downloaded `pie` instead of git-cloning it or installing it with pip.\n",
      "We won't be able to check compatibility between pretrained models and `pie` version.\n",
      "\n",
      " - model: mymodel.tar\n",
      " - tasks: lemma\n",
      "Tagging file [Lemmatisation/moliere.txt]...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146/146 [00:04<00:00, 30.27it/s]\n"
     ]
    }
   ],
   "source": [
    "! pie tag $DIRECTORY/moliere.txt mymodel.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUvBwgTtHVqD"
   },
   "source": [
    "Attention, _Pie_ n'est pas _Pie extended_. Pour ajouter le mod√®le √† ce dernier vous pouvez suivre [le tuto](https://github.com/hipster-philology/nlp-pie-taggers#add-a-model).\n",
    "\n",
    "Il faut d√©sormais un mod√®le pour la POS ‚Äì mais vous avez d√©sormais les armes pour le faire vous-m√™me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.6\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
