{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gabays/32M7131/blob/main/Cours_03/Cours03.ipynb)\n",
        "\n",
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Licence Creative Commons\" style=\"border-width:0;float:right;\\\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a>\n",
        "\n",
        "Distant Reading 2: linguistique computationnelle\n",
        "\n",
        "# **Lemmatiser un texte**\n",
        "\n",
        "Simon Gabay\n",
        "\n",
        "\n",
        "\n",
        "🚨 Pour les entraînements, on va avoir besoin d'un GPU à notre meilleur ami Google: vous pouvez en demander en allant sur dans le menu en haut à gauche et en choisissant `Execution` > `Modifier le type d'exécution` puis en choisissant `GPU`.\n",
        "\n",
        "⚠️ <font color='red'>Attention! l'usage des GPU est limité!!!! Il faut les utiliser avec parcimonie, sinon il faut payer!! Ou bien vous pouvez tout faire en ligne de commande sur les clusters HPC de l'uni.</font> Pour rappel, une documentation est disponible [ici](https://github.com/FoNDUE-HTR/Documentation/blob/master/CLUSTERS.md), mais concerne l'entraînement de modèles d'OCR.\n",
        "\n"
      ],
      "metadata": {
        "id": "YQAm1X2uzJHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Annoter\n",
        "\n",
        "### 1.1 Premier test\n",
        "\n",
        "Il nous faut d'abord installer [_pie-extended_](https://pypi.org/project/pie-extended/) pour utiliser le lemmatiseur [_Pie_](https://github.com/emanjavacas/pie). Il existe bien d'autres outils: l'un des plus populaires est [_spaCy_](https://spacy.io), qui couvre les principales langues (allemand, français, anglais, espagnol, anglais…), mais nous nous intéressons à des langues moins bien dotées et d'une nature différente (pré-orthographiques) qui nécessitent des solutions spécifiques."
      ],
      "metadata": {
        "id": "AGw9TcXHhnil"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rxn4vDYUyzLA",
        "outputId": "34cfc6c0-378a-40d6-a2a0-7cab2d5bf020",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pie-extended\n",
            "  Downloading pie_extended-0.0.42-py2.py3-none-any.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorama>=0.4.4\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting unidecode>=1.1.1\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.24.0 in /usr/local/lib/python3.8/dist-packages (from pie-extended) (1.22.4)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.8/dist-packages (from pie-extended) (2.25.1)\n",
            "Collecting click<8.0,>=7.0\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autodisambiguator<1.0.0,>=0.0.1\n",
            "  Downloading autodisambiguator-0.0.1-py2.py3-none-any.whl (11 kB)\n",
            "Collecting PaPie<0.4.0\n",
            "  Downloading PaPie-0.3.12-py2.py3-none-any.whl (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 KB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from pie-extended) (2022.6.2)\n",
            "Requirement already satisfied: lxml>=4.2.1 in /usr/local/lib/python3.8/dist-packages (from PaPie<0.4.0->pie-extended) (4.9.2)\n",
            "Requirement already satisfied: tqdm>=4.23.3 in /usr/local/lib/python3.8/dist-packages (from PaPie<0.4.0->pie-extended) (4.64.1)\n",
            "Collecting typing\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting JSON-minify>=0.3.0\n",
            "  Downloading JSON_minify-0.3.0-py2.py3-none-any.whl (5.2 kB)\n",
            "Collecting torch-optimizer==0.1.0\n",
            "  Downloading torch_optimizer-0.1.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from PaPie<0.4.0->pie-extended) (2.2.0)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from PaPie<0.4.0->pie-extended) (3.6.0)\n",
            "Collecting pyyaml<6.0\n",
            "  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m662.4/662.4 KB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<=1.13.1,>=1.3.1 in /usr/local/lib/python3.8/dist-packages (from PaPie<0.4.0->pie-extended) (1.13.1+cu116)\n",
            "Collecting scikit-learn<0.23.0,>=0.19.1\n",
            "  Downloading scikit_learn-0.22.2.post1-cp38-cp38-manylinux1_x86_64.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting terminaltables~=3.1.0\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Collecting pytorch-ranger>=0.1.1\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.25.0->pie-extended) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.25.0->pie-extended) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.25.0->pie-extended) (1.26.14)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.25.0->pie-extended) (2022.12.7)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.4.0->PaPie<0.4.0->pie-extended) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.4.0->PaPie<0.4.0->pie-extended) (6.3.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.4.0->PaPie<0.4.0->pie-extended) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<0.23.0,>=0.19.1->PaPie<0.4.0->pie-extended) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<=1.13.1,>=1.3.1->PaPie<0.4.0->pie-extended) (4.5.0)\n",
            "Building wheels for collected packages: typing\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26323 sha256=1dfacb359c8133f3795b43b513c4aa9ca25db4dad3403c5d6e7f057d08d29c3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/5d/01/3083e091b57809dad979ea543def62d9d878950e3e74f0c930\n",
            "Successfully built typing\n",
            "Installing collected packages: JSON-minify, unidecode, typing, terminaltables, pyyaml, colorama, click, autodisambiguator, scikit-learn, pytorch-ranger, torch-optimizer, PaPie, pie-extended\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.3\n",
            "    Uninstalling click-8.1.3:\n",
            "      Successfully uninstalled click-8.1.3\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.1\n",
            "    Uninstalling scikit-learn-1.2.1:\n",
            "      Successfully uninstalled scikit-learn-1.2.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "flask 2.2.3 requires click>=8.0, but you have click 7.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed JSON-minify-0.3.0 PaPie-0.3.12 autodisambiguator-0.0.1 click-7.1.2 colorama-0.4.6 pie-extended-0.0.42 pytorch-ranger-0.1.1 pyyaml-5.4.1 scikit-learn-0.22.2.post1 terminaltables-3.1.10 torch-optimizer-0.1.0 typing-3.7.4.3 unidecode-1.3.6\n"
          ]
        }
      ],
      "source": [
        "!pip install pie-extended"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous téléchargeons les modèles pour le français (dit `fr`) -- une liste des modèles disponibles est [en ligne](https://pypi.org/project/pie-extended)."
      ],
      "metadata": {
        "id": "q18vBoCG1Hf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pie-extended download fr"
      ],
      "metadata": {
        "id": "Pe7aPaOx0PXA",
        "outputId": "80396633-d947-4332-eb34-de4b5f4e9c1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "\u001b[1mStarting downloading...\u001b[0m\n",
            "8 files to download\n",
            "[██████████████████████████████████████████████████]\n",
            "- lemma.tar downloaded\n",
            "[██████████████████████████████████████████████████]\n",
            "- pos.tar downloaded\n",
            "[██████████████████████████████████████████████████]\n",
            "- mode.tar downloaded\n",
            "[██████████████████████████████████████████████████]\n",
            "- temps.tar downloaded\n",
            "[██████████████████████████████████████████████████]\n",
            "- pers.tar downloaded\n",
            "[██████████████████████████████████████████████████]\n",
            "- nomb.tar downloaded\n",
            "[██████████████████████████████████████████████████]\n",
            "- genre.tar downloaded\n",
            "[██████████████████████████████████████████████████]\n",
            "- cas.tar downloaded\n",
            "\u001b[1mFinished !\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous créons un fichier `essai.txt` un avec la phrase _je m'appelle Simon_:"
      ],
      "metadata": {
        "id": "vb0cpYyzcfOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"je m'appelle Simon\" >> essai.txt"
      ],
      "metadata": {
        "id": "T9FxB_S40awP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avec `pie extended` j'annote (`tag`) avec le modèle `fr`:"
      ],
      "metadata": {
        "id": "Rm9WiW_acsuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pie-extended tag fr /content/essai.txt"
      ],
      "metadata": {
        "id": "Tfc5lld71XiN",
        "outputId": "89698fe6-b076-42f3-f195-efe559678e20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "\u001b[1mGetting the tagger\u001b[0m\n",
            "2023-03-07 13:50:06,279 : Model /usr/local/lib/python3.8/dist-packages/pie_extended/downloads/fr/lemma.tar was serialized with a previous version of `pie`. This might result in issues. Model commit is 050815e, whereas current `pie` commit is a90a8ff.\n",
            "2023-03-07 13:50:07,149 : Model /usr/local/lib/python3.8/dist-packages/pie_extended/downloads/fr/pos.tar was serialized with a previous version of `pie`. This might result in issues. Model commit is 050815e, whereas current `pie` commit is a90a8ff.\n",
            "2023-03-07 13:50:07,291 : Model /usr/local/lib/python3.8/dist-packages/pie_extended/downloads/fr/mode.tar was serialized with a previous version of `pie`. This might result in issues. Model commit is 050815e, whereas current `pie` commit is a90a8ff.\n",
            "2023-03-07 13:50:07,419 : Model /usr/local/lib/python3.8/dist-packages/pie_extended/downloads/fr/temps.tar was serialized with a previous version of `pie`. This might result in issues. Model commit is 050815e, whereas current `pie` commit is a90a8ff.\n",
            "2023-03-07 13:50:07,553 : Model /usr/local/lib/python3.8/dist-packages/pie_extended/downloads/fr/pers.tar was serialized with a previous version of `pie`. This might result in issues. Model commit is 050815e, whereas current `pie` commit is a90a8ff.\n",
            "2023-03-07 13:50:07,681 : Model /usr/local/lib/python3.8/dist-packages/pie_extended/downloads/fr/nomb.tar was serialized with a previous version of `pie`. This might result in issues. Model commit is 050815e, whereas current `pie` commit is a90a8ff.\n",
            "2023-03-07 13:50:07,818 : Model /usr/local/lib/python3.8/dist-packages/pie_extended/downloads/fr/genre.tar was serialized with a previous version of `pie`. This might result in issues. Model commit is 050815e, whereas current `pie` commit is a90a8ff.\n",
            "2023-03-07 13:50:07,947 : Model /usr/local/lib/python3.8/dist-packages/pie_extended/downloads/fr/cas.tar was serialized with a previous version of `pie`. This might result in issues. Model commit is 050815e, whereas current `pie` commit is a90a8ff.\n",
            "100% 1/1 [00:00<00:00,  3.70it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il me crée automatiquement un fichier du même nom en ajoutant `-pie` au nom avant l'extension. Je peux regarder le résultat en ouvrant le fichier avec la commande bash `cat`:"
      ],
      "metadata": {
        "id": "vhxFlfn1dDmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/essai-pie.txt"
      ],
      "metadata": {
        "id": "9kYaUBrj2gk7",
        "outputId": "d26eb476-c4e2-40d7-f767-d1bfe32f2a03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token\tlemma\tPOS\tmorph\ttreated\r\n",
            "je\tje\tPROper\tPERS.=1|NOMB.=s|CAS=n\tje\r\n",
            "m'\tje\tPROper\tPERS.=1|NOMB.=s|CAS=r\tm'\r\n",
            "appelle\tappeler\tVERcjg\tMODE=ind|TEMPS=pst|PERS.=1|NOMB.=s\tappelle\r\n",
            "Simon\tSimon\tNOMpro\tNOMB.=s\tSimon\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Un test plus ambitieux\n",
        "\n",
        "Nous recommençons avec un texte plus complexe: la première fable de _L’École des femmes_ de Molière [disponible sur wikisource]([Wikisource](https://fr.wikisource.org/wiki/L%E2%80%99%C3%89cole_des_femmes/%C3%89dition_Chasles,_1888):"
      ],
      "metadata": {
        "id": "AJsOdi6PhsLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/gabays/32M7131/master/Cours_03/docs/moliere.txt > /content/moliere.txt\n",
        "!head -10 /content/moliere.txt"
      ],
      "metadata": {
        "id": "otX2BLgAeNir",
        "outputId": "b19e645d-31a0-42c7-cd6c-086febdd84c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 10246  100 10246    0     0  59569      0 --:--:-- --:--:-- --:--:-- 59225\n",
            "Scène I\n",
            "Chrysalde, Arnolphe\n",
            "chrysalde.\n",
            "Vous venez, dites-vous, pour lui donner la main ?\n",
            "arnolphe.\n",
            "Oui. Je veux terminer la chose dans demain.\n",
            "chrysalde.\n",
            "Nous sommes ici seuls ; et l’on peut, ce me semble,\n",
            "Sans craindre d’être ouïs, y discourir ensemble.\n",
            "Voulez-vous qu’en ami je vous ouvre mon cœur ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "J'annote ce fichier:"
      ],
      "metadata": {
        "id": "eLn2jWsTOje9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pie-extended tag fr /content/moliere.txt\n",
        "!head -10 /content/moliere-pie.txt"
      ],
      "metadata": {
        "id": "zY9o1TUfe3_u",
        "outputId": "9104cd3f-40d4-40c2-c8eb-77f9e0dc59d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "\u001b[1mGetting the tagger\u001b[0m\n",
            "2023-03-07 13:52:18,966 : Model /usr/local/lib/python3.8/dist-packages/pie_extended/downloads/fr/lemma.tar was serialized with a previous version of `pie`. This might result in issues. Model commit is 050815e, whereas current `pie` commit is a90a8ff.\n",
            "2023-03-07 13:52:19,768 : Model /usr/local/lib/python3.8/dist-packages/pie_extended/downloads/fr/pos.tar was serialized with a previous version of `pie`. This might result in issues. Model commit is 050815e, whereas current `pie` commit is a90a8ff.\n",
            "2023-03-07 13:52:19,905 : Model /usr/local/lib/python3.8/dist-packages/pie_extended/downloads/fr/mode.tar was serialized with a previous version of `pie`. This might result in issues. Model commit is 050815e, whereas current `pie` commit is a90a8ff.\n",
            "2023-03-07 13:52:20,026 : Model /usr/local/lib/python3.8/dist-packages/pie_extended/downloads/fr/temps.tar was serialized with a previous version of `pie`. This might result in issues. Model commit is 050815e, whereas current `pie` commit is a90a8ff.\n",
            "2023-03-07 13:52:20,146 : Model /usr/local/lib/python3.8/dist-packages/pie_extended/downloads/fr/pers.tar was serialized with a previous version of `pie`. This might result in issues. Model commit is 050815e, whereas current `pie` commit is a90a8ff.\n",
            "2023-03-07 13:52:20,269 : Model /usr/local/lib/python3.8/dist-packages/pie_extended/downloads/fr/nomb.tar was serialized with a previous version of `pie`. This might result in issues. Model commit is 050815e, whereas current `pie` commit is a90a8ff.\n",
            "2023-03-07 13:52:20,401 : Model /usr/local/lib/python3.8/dist-packages/pie_extended/downloads/fr/genre.tar was serialized with a previous version of `pie`. This might result in issues. Model commit is 050815e, whereas current `pie` commit is a90a8ff.\n",
            "2023-03-07 13:52:20,529 : Model /usr/local/lib/python3.8/dist-packages/pie_extended/downloads/fr/cas.tar was serialized with a previous version of `pie`. This might result in issues. Model commit is 050815e, whereas current `pie` commit is a90a8ff.\n",
            "100% 1/1 [00:17<00:00, 17.05s/it]\n",
            "token\tlemma\tPOS\tmorph\ttreated\n",
            "Scène\tscène\tNOMcom\tNOMB.=s|GENRE=f\tScène\n",
            "I\tI\tPROper\tNOMB.=s|GENRE=f\tI\n",
            "Chrysalde\tChrysalde\tNOMpro\tNOMB.=s|GENRE=f\tChrysalde\n",
            ",\t,\tPONfbl\tMORPH=empty\t,\n",
            "Arnolphe\tArnolphe\tNOMpro\tNOMB.=s|GENRE=m\tArnolphe\n",
            "chrysalde\tchrysalde\tNOMpro\tNOMB.=s|GENRE=f\tchrysalde\n",
            ".\t.\tPONfrt\tMORPH=empty\t.\n",
            "Vous\tvous\tPROper\tPERS.=2|NOMB.=p\tVous\n",
            "venez\tvenir\tVERcjg\tMODE=ind|TEMPS=pst|PERS.=2|NOMB.=p\tvenez\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Un essai en latin\n",
        " Recommençons avec un texte en latin. Il faut d'abord charger le modèle latin et les _addons_"
      ],
      "metadata": {
        "id": "avQc5QIzeujG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pie-extended download lasla\n",
        "!pie-extended install-addons lasla"
      ],
      "metadata": {
        "id": "4fNui7BSln5N",
        "outputId": "3114cd91-c0f4-4799-c46b-a9faf536e599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "\u001b[1mStarting downloading...\u001b[0m\n",
            "12 files to download\n",
            "[████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████]\n",
            "- latin-straight.json downloaded\n",
            "[████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████]\n",
            "- latin-pos.json downloaded\n",
            "[██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████]\n",
            "- latin-needs.json downloaded\n",
            "[██████████████████████████████████████████████████]\n",
            "- Mood_Tense_Voice.tar downloaded\n",
            "[██████████████████████████████████████████████████]\n",
            "- Gend.tar downloaded\n",
            "[██████████████████████████████████████████████████]\n",
            "- Person.tar downloaded\n",
            "[██████████████████████████████████████████████████]\n",
            "- Deg.tar downloaded\n",
            "[██████████████████████████████████████████████████]\n",
            "- lemma.tar downloaded\n",
            "[██████████████████████████████████████████████████]\n",
            "- pos.tar downloaded\n",
            "[██████████████████████████████████████████████████]\n",
            "- Numb.tar downloaded\n",
            "[██████████████████████████████████████████████████]\n",
            "- Dis.tar downloaded\n",
            "[██████████████████████████████████████████████████]\n",
            "- Case.tar downloaded\n",
            "\u001b[1mFinished !\u001b[0m\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "\u001b[1mInstalling add-ons\u001b[0m\n",
            "\u001b[1mDone\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Téléchargeons désormais un texte, le _Pro Quinctio_ de Cicéron disponible sur [_The Latin Library_](https://www.thelatinlibrary.com/cicero/quinc.shtml)."
      ],
      "metadata": {
        "id": "2RHmzoutlz9x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/gabays/32M7131/master/Cours_03/docs/Cicero_%20Pro_Quinctio.txt > /content/Cicero_%20Pro_Quinctio.txt\n",
        "!head -10 /content/Cicero_%20Pro_Quinctio.txt"
      ],
      "metadata": {
        "id": "BPBi_8TNlzkl",
        "outputId": "c129dd68-4c3e-41a1-b90b-3195af14f96e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 58714  100 58714    0     0   275k      0 --:--:-- --:--:-- --:--:--  275k\n",
            "M. TVLLI CICERONIS PRO P. QVINCTIO ORATIO\n",
            "\n",
            "I. Quae res in civitate duae plurimum possunt, eae contra nos ambae faciunt in hoc tempore, summa gratia et eloquentia; quarum alterum, C. Aquili, vereor, alteram metuo. Eloquentia Q. Hortensi ne me in dicendo impediat, non nihil commoveor, gratia Sex. Naevi ne P. Quinctio noceat, id vero non mediocriter pertimesco. Neque hoc tanto opere querendum videretur, haec summa in illis esse, si in nobis essent saltem mediocria; verum ita se res habet, ut ego, qui neque usu satis et ingenio parum possum, cum patrono disertissimo comparer, P. Quinctius, cui tenues opes, nullae facultates, exiguae amicorum copiae sunt, cum adversario gratiosissimo contendat. Illud quoque nobis accedit incommodum, quod M. Iunius, qui hanc causam aliquotiens apud te egit, homo et in aliis causis exercitatus et in hac multum ac saepe versatus, hoc tempore abest nova legatione impeditus, et ad me ventum est qui, ut summa haberem cetera, temporis quidem certe vix satis habui ut rem tantam, tot controversiis implicatam, possem cognoscere. Ita quod mihi consuevit in ceteris causis esse adiumento, id quoque in hac causa deficit. Nam, quod ingenio minus possum, subsidium mihi diligentia comparavi; quae quanta sit, nisi tempus et spatium datum sit, intellegi non potest. Quae quo plura sunt, C. Aquili, eo te et hos qui tibi in consilio sunt meliore mente nostra verba audire oportebit, ut multis incommodis veritas debilitata tandem aequitate talium virorum recreetur. Quod si tu iudex nullo praesidio fuisse videbere contra vim et gratiam solitudini atque inopiae, si apud hoc consilium ex opibus, non ex veritate causa pendetur, profecto nihil est iam sanctum atque sincerum in civitate, nihil est quod humilitatem cuiusquam gravitas et virtus iudicis consoletur. Certe aut apud te et hos qui tibi adsunt veritas valebit, aut ex hoc loco repulsa vi et gratia locum ubi consistat reperire non poterit.\n",
            "\n",
            "II. Non eo dico, C. Aquili, quo mihi veniat in dubium tua fides et constantia, aut quo non in his quos tibi advocavisti viris lectissimis civitatis spem summam habere P. Quinctius, debeat. Quid ergo est? Primum magnitudo periculi summo timore hominem adficit, quod uno iudicio de fortunis omnibus decernit, idque dum cogitat, non minus saepe ei venit in mentem potestatis quam aequitatis tuae, propterea quod omnes quorum in alterius manu vita posita est saepius illud cogitant, quid possit is cuius in dicione ac potestate sunt quam quid debeat facere. Deinde habet adversarium P. Quinctius verbo Sex. Naevium, re ura huiusce aetatis homines disertissimos, fortissimos, florentissimos nostrae civitatis, qui communi studio summis opibus Sex Naevium defendunt, si id est defendere, cupiditati alterius obtemperare quo is facilius quem velit iniquo iudicio opprimere possit. Nam quid hoc iniquius aut indignius, C. Aquili, dici aut commemorari potest, quam me qui caput alterius, famam fortunasque defendam priore loco causam dicere? cum praesertim Q. Hortensius qui in hoc iudicio partis accusatoris obtinet contra me sit dicturus, cui summam copiam facultatemque dicendi natura largita est. Ita fit ut ego qui tela depellere et volneribus mederi debeam tum id facere cogar cum etiam telum adversarius nullum iecerit, illis autem id tempus impugnandi detur cum et vitandi illorum impetus potestas adempta nobis erit et, si qua in re, id quod parati sunt facere, falsum crimen quasi venenatum aliquod telum iecerint, medicinae faciendae locus non erit. Id accidit praetoris iniquitate et iniuria, primum quod contra omnium consuetudinem iudicium prius de probro quam de re maluit fieri, deinde quod ita constituit id ipsum iudicium ut reus, ante quam verbum accusatoris audisset, causam dicere cogeretur. Quod eorum gratia et potentia factum ao est qui, quasi sua res aut honos agatur, ita diligenter Sex. Naevi studio et cupiditati morem gerunt et in eius modi rebus opes suas experiuntur, in quibus, quo plus propter virtutem nobilitatemque possunt, eo minus quantum possint debent ostendere.\n",
            "\n",
            "Cum tot tantisque difficultatibus adfectus atque adflictus in tuam, C. Aquili fidem, veritatem, misericordiam P. Quinctius confugerit, cum adhuc ei propter vim adversariorum non ius par, non agendi potestas; eadem, non magistratus aequus reperiri potuerit, cum ei summam per iniuriam omnia inimica atque infesta fuerint, te, C. Aquili, vosque qui in consilio adestis, orat atque obsecrat ut multis iniuriis iactatam atque agitatam aequitatem in hoc tandem loco consistere et confirmari patiamini.\n",
            "\n",
            "III. Id quo facilius facere possitis, dabo operam ut a principio res quem ad modum gesta et contracta sit cognoscatis. C. Quinctius fuit P. Quincti huius frater, sane ceterarum rerum pater familias et prudens et attentus, una in re paulo minus consideratus, qui societatem cum Sex. Naevio fecerit, viro bono, verum tamen non ita instituto ut iura societatis et officia certi patris familias nosse posset; non quo ei deesset ingenium; nam neque parum facetus scurra Sex. Naevius neque inhumanus praeco umquam est existimatus. Quid ergo est? Cum ei natura nihil melius quam vocem dedisset, pater nihil praeter libertatem reliquisset, vocem in quaestum contulit, libertate usus est quo impunius dicax esset. Qua re quidem socium tibi eum velles adiungere nihil erat nisi ut in tua pecunia condisceret qui pecuniae fructus esset; tamen inductus consuetudine ac familiaritate Quinctius fecit, ut dixi, societatem earum rerum quae in Gallia comparabantur. Erat ei pecuaria res ampla et rustica sane bene culta et fructuosa. Tollitur ab atriis Liciniis atque a praeconum consessu in Galliam Naevius et trans Alpis usque transfertur. Fit magna mutatio loci, non ingeni. Nam qui ab adulescentulo quaestum sibi instituisset sine impendio, postea quam nescio quid impendit et in commune contulit, mediocri quaestu contentus esse non poterat. Nec mirum, si is qui vocem venalem habuerat ea quae voce quaesiverat magno sibi quaestui fore putabat. Itaque hercule haud mediocriter de communi quodcumque poterat ad se in privatam domum sevocabat; qua in re ita diligens erat quasi ei qui magna fide societatem gererent arbitrium pro socio condemnari solerent. Verum his de rebus non necesse habeo dicere ea quae me P. Quinctius cupit commemorare; tametsi causa postulat, tamen quia postulat, non flagitat praeteribo.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On annote et on observe le résultat:"
      ],
      "metadata": {
        "id": "OuD9dfxmJb18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pie-extended tag lasla /content/Cicero_%20Pro_Quinctio.txt\n",
        "!head -10 /content/Cicero_%20Pro_Quinctio-pie.txt"
      ],
      "metadata": {
        "id": "6OEL6lIJmrVh",
        "outputId": "25d4a982-37f0-4bf7-8d73-382817168f2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "\u001b[1mGetting the tagger\u001b[0m\n",
            "100% 1/1 [01:03<00:00, 63.89s/it]\n",
            "token\tlemma\tpos\tmorph\tDis\ttreated\n",
            "M.\tMarcus\tNOMpro\tCase=Gen|Numb=Sing\t_\tM\n",
            "TVLLI\tTullius\tNOMpro\tCase=Gen|Numb=Sing\t_\tTULLI\n",
            "CICERONIS\tCiceroni\tNOMpro\tCase=Nom|Numb=Sing\t_\tCICERONIS\n",
            "PRO\tPro\tPRE\tMORPH=empty\t1\tPRO\n",
            "P.\tPublius\tNOMpro\tCase=Abl|Numb=Sing\t_\tP\n",
            "QVINCTIO\tQuintius\tNOMpro\tCase=Abl|Numb=Sing|Person=3\t_\tQUINCTIO\n",
            "ORATIO\tOratius\tNOMpro\tCase=Abl|Numb=Sing\t_\tORATIO\n",
            "I\t1\tADJcar\tMORPH=empty\t_\t1\n",
            ".\t.\tPUNC\tMORPH=empty\t_\t--IGN.--\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Entraîner un modèle\n",
        "### 2.1 _Split_ pour l'entraînement\n",
        "\n",
        "Nous allons d'abord faire un _split_ et créer les trois jeux de données nécessaires avec [_Protogenie_](https://github.com/hipster-philology/protogenie).\n",
        "\n",
        "Récupérons d'abord un jeu de données."
      ],
      "metadata": {
        "id": "3bwOZivysc5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/gabays/32M7131/main/Cours_03/train/lemma_Empty.txt > /content/lemma_Empty.txt\n",
        "!head -20 /content/lemma_Empty.txt"
      ],
      "metadata": {
        "id": "x9wG8KKIs2so",
        "outputId": "40d802ec-9c61-463a-a2c7-c1878e515d19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 1917k  100 1917k    0     0  4917k      0 --:--:-- --:--:-- --:--:-- 4917k\n",
            "form\tlemma\n",
            "1536_Flores_Deplourable.tsv\txxx\n",
            "form\tlemma\n",
            "GRIMALTE\tGrimalte\n",
            "AMANT\tamant\n",
            "DE\tde\n",
            "LA\tle\n",
            "dame\tdame\n",
            "Gradisse\tGradisse\n",
            "narre\tnarrer\n",
            "sommairement\tsommairement\n",
            "Les\tle\n",
            "amoureux\tamoureux\n",
            "regredz\tregret\n",
            "de\tde\n",
            "Flamete\tFlammette\n",
            ",\t,\n",
            "qui\tqui\n",
            "furent\têtre\n",
            "occasion\toccasion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ensuite nous installons _Protogenie_."
      ],
      "metadata": {
        "id": "VZQ6FN0zt1cC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install protogenie"
      ],
      "metadata": {
        "id": "VB1QLtCUt4Cy",
        "outputId": "137ebd72-6b4b-483e-a3cf-b9f7db3690fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting protogenie\n",
            "  Downloading protogenie-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from protogenie) (4.9.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from protogenie) (2022.6.2)\n",
            "Requirement already satisfied: click<=8.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from protogenie) (7.1.2)\n",
            "Installing collected packages: protogenie\n",
            "Successfully installed protogenie-0.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On charge un fichier de config (pour apprendre à les fabriquer, il existe une [documentation](https://github.com/hipster-philology/protogenie/blob/master/DOCUMENTATION.md)).\n",
        "\n"
      ],
      "metadata": {
        "id": "s9SGk4-Et9Fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/gabays/32M7131/main/Cours_03/train/lemma_Empty.xml > /content/lemma_Empty.xml\n",
        "!cat /content/lemma_Empty.xml"
      ],
      "metadata": {
        "id": "EYBcNPBQuJe6",
        "outputId": "e4df25b3-ad50-4bbe-c0f7-7fddf4b4c20f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   579  100   579    0     0   3912      0 --:--:-- --:--:-- --:--:--  3885\r100   579  100   579    0     0   3912      0 --:--:-- --:--:-- --:--:--  3885\n",
            "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
            "<?xml-model href=\"protogeneia/schema.rng\" schematypens=\"http://relaxng.org/ns/structure/1.0\"?>\n",
            "<config>\n",
            "    <default-header>\n",
            "        <header type=\"explicit\">\n",
            "            <key>form</key>\n",
            "            <key>lemma</key>\n",
            "        </header>\n",
            "    </default-header>\n",
            "    <output column_marker=\"TAB\">\n",
            "        <header name=\"default\"/>\n",
            "    </output>\n",
            "    <corpora>\n",
            "        <corpus path=\"/content/lemma_Empty.txt\" column_marker=\"TAB\">\n",
            "            <splitter name=\"empty_line\"/>\n",
            "            <header type=\"default\"/>\n",
            "        </corpus>\n",
            "    </corpora>\n",
            "</config>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On fait le split -- ici 10% en `dev`, 10% en `eval` (ou `test`) et 80% en `train`, mais cette répartition peut changer en fonction de la quantité de données dont on dispose."
      ],
      "metadata": {
        "id": "H3aZBlmfuNy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!protogenie build /content/lemma_Empty.xml -d 0.1 -e 0.1 -t 0.8"
      ],
      "metadata": {
        "id": "1Lbf38U-1XIu",
        "outputId": "a3a650fd-0952-4bb8-af52-573c99a1ced7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/protogenie/configs.py:138: FutureWarning: The behavior of this method will change in future versions. Use specific 'len(elem)' or 'elem is not None' test instead.\n",
            "  if not header_node or header_node.get(\"name\", \"default\") == \"default\":\n",
            "=============\n",
            "Processing...\n",
            "/content/lemma_Empty.txt has been transformed\n",
            "\t17982 tokens in test dataset\n",
            "\t19960 tokens in dev dataset\n",
            "\t155153 tokens in train dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Entrainement\n",
        "On va entraîner avec [_Pie_](https://github.com/emanjavacas/pie)."
      ],
      "metadata": {
        "id": "y2mJAStU2sih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nlp-pie"
      ],
      "metadata": {
        "id": "saEY2pqf3FF6",
        "outputId": "8e041389-17ef-4037-d278-c6ad0a116e01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nlp-pie\n",
            "  Downloading nlp_pie-0.3.8-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.3 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.3/84.3 KB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from nlp-pie) (7.1.2)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from nlp-pie) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.23.3 in /usr/local/lib/python3.8/dist-packages (from nlp-pie) (4.64.1)\n",
            "Collecting pyyaml==5.1b3\n",
            "  Downloading PyYAML-5.1b3.tar.gz (273 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.9/273.9 KB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting numpy<1.18.0,>=1.14.3\n",
            "  Downloading numpy-1.17.5-cp38-cp38-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: JSON-minify>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from nlp-pie) (0.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from nlp-pie) (2.2.0)\n",
            "Collecting terminaltables==3.1.0\n",
            "  Downloading terminaltables-3.1.0.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing<4.0 in /usr/local/lib/python3.8/dist-packages (from nlp-pie) (3.7.4.3)\n",
            "Requirement already satisfied: lxml>=4.2.1 in /usr/local/lib/python3.8/dist-packages (from nlp-pie) (4.9.2)\n",
            "Collecting torch<=1.7.1,>=1.3.1\n",
            "  Downloading torch-1.7.1-cp38-cp38-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.8/776.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<0.23.0,>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from nlp-pie) (0.22.2.post1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.4.0->nlp-pie) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.4.0->nlp-pie) (6.3.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.8/dist-packages (from gensim>=3.4.0->nlp-pie) (1.10.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<0.23.0,>=0.19.1->nlp-pie) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch<=1.7.1,>=1.3.1->nlp-pie) (4.5.0)\n",
            "Collecting scipy>=0.18.1\n",
            "  Downloading scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.8.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyyaml, terminaltables\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.1b3-cp38-cp38-linux_x86_64.whl size=44128 sha256=5899bb9bd9a73056518a9a1e4f54ad00f0b196c25cefb17a3737b448ef8ae852\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/8c/89/6a86b1310360a4f32ab71f831ff5ccf0015256f89cbfcb1e58\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-py3-none-any.whl size=15355 sha256=abd17168e514d1770339af19d9c6cb259c007841af94749e5fcdb497f01ccfa3\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/8f/5f/253d0105a55bd84ee61ef0d37dbf70421e61e0cd70cef7c5e1\n",
            "Successfully built pyyaml terminaltables\n",
            "Installing collected packages: terminaltables, pyyaml, numpy, torch, scipy, nlp-pie\n",
            "  Attempting uninstall: terminaltables\n",
            "    Found existing installation: terminaltables 3.1.10\n",
            "    Uninstalling terminaltables-3.1.10:\n",
            "      Successfully uninstalled terminaltables-3.1.10\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 5.4.1\n",
            "    Uninstalling PyYAML-5.4.1:\n",
            "      Successfully uninstalled PyYAML-5.4.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "xarray 2022.12.0 requires numpy>=1.20, but you have numpy 1.17.5 which is incompatible.\n",
            "xarray-einstats 0.5.1 requires numpy>=1.20, but you have numpy 1.17.5 which is incompatible.\n",
            "torchvision 0.14.1+cu116 requires torch==1.13.1, but you have torch 1.7.1 which is incompatible.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.7.1 which is incompatible.\n",
            "tensorflow 2.11.0 requires numpy>=1.20, but you have numpy 1.17.5 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.17.5 which is incompatible.\n",
            "sklearn-pandas 2.2.0 requires numpy>=1.18.1, but you have numpy 1.17.5 which is incompatible.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "plotnine 0.10.1 requires numpy>=1.19.0, but you have numpy 1.17.5 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.17.5 which is incompatible.\n",
            "mizani 0.8.1 requires numpy>=1.19.0, but you have numpy 1.17.5 which is incompatible.\n",
            "jaxlib 0.4.4+cuda11.cudnn82 requires numpy>=1.20, but you have numpy 1.17.5 which is incompatible.\n",
            "jax 0.4.4 requires numpy>=1.20, but you have numpy 1.17.5 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.22.2.post1 which is incompatible.\n",
            "gym 0.25.2 requires numpy>=1.18.0, but you have numpy 1.17.5 which is incompatible.\n",
            "dask 2022.2.1 requires pyyaml>=5.3.1, but you have pyyaml 5.1b3 which is incompatible.\n",
            "cupy-cuda11x 11.0.0 requires numpy<1.26,>=1.20, but you have numpy 1.17.5 which is incompatible.\n",
            "cmdstanpy 1.1.0 requires numpy>=1.21, but you have numpy 1.17.5 which is incompatible.\n",
            "aeppl 0.0.33 requires numpy>=1.18.1, but you have numpy 1.17.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nlp-pie-0.3.8 numpy-1.17.5 pyyaml-5.1b3 scipy-1.8.1 terminaltables-3.1.0 torch-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On a besoin de renommer les fichiers créés avec _Protogénie_ en `.tsv`"
      ],
      "metadata": {
        "id": "CRqa1P3IAfA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/output/dev/lemma_Empty.txt /content/output/dev/lemma_Empty.tsv\n",
        "!mv /content/output/test/lemma_Empty.txt /content/output/test/lemma_Empty.tsv\n",
        "!mv /content/output/train/lemma_Empty.txt /content/output/train/lemma_Empty.tsv"
      ],
      "metadata": {
        "id": "9SGEWJ-e4I6J"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous avons aussi besoin d'un fichier de configuration"
      ],
      "metadata": {
        "id": "y9Gxq65l6Buy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/gabays/32M7131/main/Cours_03/train/config.json > /content/config.json\n",
        "!cat /content/config.json"
      ],
      "metadata": {
        "id": "Xqm6DFu06Fig",
        "outputId": "41a827e6-74d2-4c38-f957-5fbdd812a29a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1937  100  1937    0     0  13358      0 --:--:-- --:--:-- --:--:-- 13358\n",
            "{\n",
            "   \"modelname\":\"mymodel\",\n",
            "   \"modelpath\":\"./\",\n",
            "   \"run_test\":false,\n",
            "   \"input_path\":\"/content/output/train/lemma_Empty.tsv\",\n",
            "   \"test_path\":\"/content/output/test/lemma_Empty.tsv\",\n",
            "   \"dev_path\":\"/content/output/dev/lemma_Empty.tsv\",\n",
            "   \"header\":true,\n",
            "   \"sep\":\"\\t\",\n",
            "   \"breakline_ref\":\"lemma\",\n",
            "   \"breakline_data\":\"NONE\",\n",
            "   \"char_max_size\":500,\n",
            "   \"word_max_size\":20000,\n",
            "   \"max_sent_len\":35,\n",
            "   \"max_sents\":1000000,\n",
            "   \"char_min_freq\":1,\n",
            "   \"word_min_freq\":1,\n",
            "   \"char_eos\":true,\n",
            "   \"char_bos\":true,\n",
            "   \"char_lower\":false,\n",
            "   \"tasks\":[\n",
            "      {\n",
            "         \"name\":\"lemma\",\n",
            "         \"target\":true,\n",
            "         \"context\":\"sentence\",\n",
            "         \"level\":\"char\",\n",
            "         \"decoder\":\"attentional\",\n",
            "         \"settings\":{\n",
            "            \"bos\":true,\n",
            "            \"eos\":true,\n",
            "            \"lower\":false,\n",
            "            \"target\":\"lemma\"\n",
            "         },\n",
            "         \"layer\":-1\n",
            "      }\n",
            "   ],\n",
            "   \"task_defaults\":{\n",
            "      \"level\":\"token\",\n",
            "      \"layer\":-1,\n",
            "      \"decoder\":\"linear\",\n",
            "      \"context\":\"sentence\"\n",
            "   },\n",
            "   \"patience\":7,\n",
            "   \"factor\":0.5,\n",
            "   \"threshold\":0.0001,\n",
            "   \"min_weight\":0.2,\n",
            "   \"include_lm\":true,\n",
            "   \"lm_shared_softmax\":true,\n",
            "   \"lm_schedule\":{\n",
            "      \"patience\":2,\n",
            "      \"factor\":0.5,\n",
            "      \"weight\":0.2,\n",
            "      \"mode\":\"min\"\n",
            "   },\n",
            "   \"batch_size\":128,\n",
            "   \"epochs\":10,\n",
            "   \"dropout\":0.25,\n",
            "   \"word_dropout\":0,\n",
            "   \"lr\":0.001,\n",
            "   \"lr_patience\":4,\n",
            "   \"optimizer\":\"Adam\",\n",
            "   \"clip_norm\":5,\n",
            "   \"linear_layers\":1,\n",
            "   \"hidden_size\":256,\n",
            "   \"num_layers\":1,\n",
            "   \"cell\":\"GRU\",\n",
            "   \"wemb_dim\":0,\n",
            "   \"merge_type\":\"concat\",\n",
            "   \"cemb_dim\":400,\n",
            "   \"cemb_type\":\"rnn\",\n",
            "   \"cemb_layers\":2,\n",
            "   \"decoder_layers\":3,\n",
            "   \"custom_cemb_cell\":false,\n",
            "   \"checks_per_epoch\":1,\n",
            "   \"report_freq\":200,\n",
            "   \"verbose\":true,\n",
            "   \"device\":\"cuda\",\n",
            "   \"buffer_size\":10000,\n",
            "   \"minimize_pad\":false,\n",
            "   \"shuffle\":true,\n",
            "   \"pretrain_embeddings\":false,\n",
            "   \"load_pretrained_embeddings\":\"\",\n",
            "   \"load_pretrained_encoder\":\"\",\n",
            "   \"freeze_embeddings\":false,\n",
            "   \"scorer\":\"general\",\n",
            "   \"cache_dataset\": true\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ce fichier contient toutes les informations nécessaires pour l'entraînement. Le paramétrage via le fichier de configuration est extrêmement complexe, et il est difficile (impossible?) de connaître à l'avance les meilleurs choix à opérer: il faut souvent faire de nombreux tests.\n",
        "\n",
        "Si vous voulez un peu comprendre le contenu, voici quelques explications:\n",
        "```json\n",
        "{\n",
        "  // * General\n",
        "  // model name to be used for saving\n",
        "  \"modelname\": \"latest-fro\",\n",
        "  // model path to be used for saving\n",
        "  \"modelpath\": \"./\",\n",
        "  // run test (no serialization)\n",
        "  \"run_test\": false,\n",
        "  // max length of sentences (longer sentence will be split)\n",
        "  \"max_sent_len\": 35,\n",
        "  // maximum number of sentences to process\n",
        "  \"max_sents\": 1000000,\n",
        "  // * Data\n",
        "  // path or unix-like expression to file(s)/dir with training data:\n",
        "  // e.g. \"datasets/capitula_classic/train0.tsv\"\"\n",
        "  \"input_path\": \"corpus_for_train/train/CORPUS.tsv\",\n",
        "  // path to test set (same format as input_path)\n",
        "  \"test_path\": \"corpus_for_train/test/CORPUS.tsv\",\n",
        "  // path to dev set (same format as input_path)\n",
        "  \"dev_path\": \"corpus_for_train/dev/CORPUS.tsv\",\n",
        "  // data to use as reference for breaking lines (e.g. \"pos\")\n",
        "  \"breakline_ref\": \"POS\",\n",
        "  // needed to decide for a sentence boundary (e.g. \"$.\")\n",
        "  \"breakline_data\": \"INUTILE\",\n",
        "  // maximum vocabulary size\n",
        "  \"char_max_size\": 500,\n",
        "  // maximum vocabulary size for word input\n",
        "  \"word_max_size\": 20000,\n",
        "  // min freq per item to be incorporated in the vocabulary (only used if *_max_size is 0)\n",
        "  \"char_min_freq\": 1,\n",
        "  \"word_min_freq\": 1,\n",
        "  // char-level encoding\n",
        "  \"char_eos\": true,\n",
        "  \"char_bos\": true,\n",
        "  // tab-format only:\n",
        "  \"header\": true,\n",
        "  // separator for csv-like files\n",
        "  \"sep\": \"\\t\",\n",
        "  // task-related config\n",
        "  \"tasks\": [\n",
        "    // each task's name refers to the corresponding data field\n",
        "    // this behaviour can be changed in case the name differs from the data field\n",
        "    // by using a \"target\" key that refers to the target data field\n",
        "    // e.g. {\"name\": \"lemma-char\", \"settings\": {\"target\": \"lemma\"}}\n",
        "    // e.g. {\"name\": \"lemma-word\", \"settings\": {\"target\": \"lemma\"}}\n",
        "    {\n",
        "      \"name\": \"lemma\",\n",
        "      \"target\": true,\n",
        "      \"context\": \"sentence\",\n",
        "      \"level\": \"char\",\n",
        "      \"decoder\": \"attentional\",\n",
        "      \"settings\": {\n",
        "        \"bos\": true,\n",
        "        \"eos\": true,\n",
        "        \"lower\": true,\n",
        "        \"target\": \"lemma\"\n",
        "      },\n",
        "      \"layer\": -1\n",
        "    },\n",
        "    {\"name\": \"POS\"}\n",
        "  ],\n",
        "  \"task_defaults\": {\n",
        "    \"level\": \"token\",\n",
        "    \"layer\": -1,\n",
        "    \"decoder\": \"linear\",\n",
        "    \"context\": \"sentence\"\n",
        "  },\n",
        "  // general task schedule params (can be overwritten in the \"settings\" entry of each)\n",
        "  \"patience\": 1000000, // default to very large value\n",
        "  \"factor\": 1, // decrease the loss weight by this amount (0, 1)\n",
        "  \"threshold\": 0, // minimum decrease in loss to be considered an improvement\n",
        "  \"min_weight\": 0, // minimum value a task weight can be decreased to\n",
        "\n",
        "  // whether to include autoregressive loss\n",
        "  \"include_lm\": true,\n",
        "  // whether to share the output layer for both fwd and bwd lm\n",
        "  \"lm_shared_softmax\": false,\n",
        "  // lm settings in case it's included as an extra task\n",
        "  \"lm_schedule\": {\n",
        "    \"patience\": 100, \"factor\": 0.5, \"weight\": 0.2, \"mode\": \"min\"\n",
        "  }\n",
        "}\n",
        "  ```\n",
        "\n"
      ],
      "metadata": {
        "id": "bJzaEqNjB9Fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Et on entraine. Le nombre d'itérations (_epochs_) est volontairement limité à 10 pour ne pas gaspiller trop d'énergie inutilement. En augmentant ce nombre, on obtiendrait un bien meilleur résultat.\n",
        "\n",
        "Vous allez voir apparaître les scores sur le `dev` our chaque itération. Pour lire les résultats, [cf. précision et rappel sur wikipedia](https://fr.wikipedia.org/wiki/Pr%C3%A9cision_et_rappel)."
      ],
      "metadata": {
        "id": "wWO-357V6Wdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pie train config.json"
      ],
      "metadata": {
        "id": "NVNetesS6g4x",
        "outputId": "f1b328eb-e42d-46ac-e511-d61221da9941",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "WARNING:root:\n",
            "It seems like you downloaded `pie` instead of git-cloning it or installing it with pip.\n",
            "We won't be able to check compatibility between pretrained models and `pie` version.\n",
            "\n",
            "\n",
            "::: Loaded Config :::\n",
            "\n",
            "batch_size: 128\n",
            "breakline_data: NONE\n",
            "breakline_ref: lemma\n",
            "buffer_size: 10000\n",
            "cache_dataset: true\n",
            "cell: GRU\n",
            "cemb_dim: 400\n",
            "cemb_layers: 2\n",
            "cemb_type: rnn\n",
            "char_bos: true\n",
            "char_eos: true\n",
            "char_lower: false\n",
            "char_max_size: 500\n",
            "char_min_freq: 1\n",
            "checks_per_epoch: 1\n",
            "clip_norm: 5\n",
            "config_path: config.json\n",
            "custom_cemb_cell: false\n",
            "decoder_layers: 3\n",
            "dev_path: /content/output/dev/lemma_Empty.tsv\n",
            "device: cuda\n",
            "drop_diacritics: false\n",
            "dropout: 0.25\n",
            "epochs: 10\n",
            "factor: 0.5\n",
            "freeze_embeddings: false\n",
            "header: true\n",
            "hidden_size: 256\n",
            "include_lm: true\n",
            "init_rnn: default\n",
            "input_path: /content/output/train/lemma_Empty.tsv\n",
            "linear_layers: 1\n",
            "lm_schedule:\n",
            "  factor: 0.5\n",
            "  mode: min\n",
            "  patience: 2\n",
            "  weight: 0.2\n",
            "lm_shared_softmax: true\n",
            "load_pretrained_embeddings: ''\n",
            "load_pretrained_encoder: ''\n",
            "lr: 0.001\n",
            "lr_factor: 0.75\n",
            "lr_patience: 4\n",
            "max_sent_len: 35\n",
            "max_sents: 1000000\n",
            "merge_type: concat\n",
            "min_lr: 1.0e-06\n",
            "min_weight: 0.2\n",
            "minimize_pad: false\n",
            "modelname: mymodel\n",
            "modelpath: ./\n",
            "num_layers: 1\n",
            "optimizer: Adam\n",
            "patience: 7\n",
            "pretrain_embeddings: false\n",
            "report_freq: 200\n",
            "run_test: false\n",
            "scorer: general\n",
            "sep: \"\\t\"\n",
            "shuffle: true\n",
            "task_defaults:\n",
            "  context: sentence\n",
            "  decoder: linear\n",
            "  layer: -1\n",
            "  level: token\n",
            "tasks:\n",
            "- context: sentence\n",
            "  decoder: attentional\n",
            "  layer: -1\n",
            "  level: char\n",
            "  name: lemma\n",
            "  settings:\n",
            "    bos: true\n",
            "    eos: true\n",
            "    lower: false\n",
            "    target: lemma\n",
            "  target: true\n",
            "tasks_order:\n",
            "- lemma\n",
            "- pos\n",
            "test_path: /content/output/test/lemma_Empty.tsv\n",
            "threshold: 0.0001\n",
            "utfnorm: false\n",
            "utfnorm_type: NFKD\n",
            "verbose: true\n",
            "wemb_dim: 0\n",
            "word_dropout: 0\n",
            "word_lower: false\n",
            "word_max_size: 20000\n",
            "word_min_freq: 1\n",
            "\n",
            "Using seed: 141558\n",
            "::: Available tasks :::\n",
            "\n",
            "- lemma\n",
            "\n",
            "::: Fitting data :::\n",
            "\n",
            "\n",
            "::: Vocabulary :::\n",
            "\n",
            "- word            types=19480/19480=1.00 tokens=155153/155153=1.00\n",
            "- char            types=111/111=1.00 tokens=628046/628046=1.00\n",
            "\n",
            "::: Tasks :::\n",
            "\n",
            "- lemma           target=lemma  level=char   vocab=108   \n",
            "\n",
            "Initializing GRU with scheme: xavier_uniform\n",
            "Initializing GRU with scheme: xavier_uniform\n",
            "Initializing GRU with scheme: xavier_uniform\n",
            "::: Model :::\n",
            "\n",
            "SimpleModel(\n",
            "  (cemb): RNNEmbedding(\n",
            "    (emb): Embedding(115, 400, padding_idx=2)\n",
            "    (rnn): GRU(400, 400, num_layers=2, dropout=0.25, bidirectional=True)\n",
            "  )\n",
            "  (encoder): RNNEncoder(\n",
            "    (rnn): ModuleList(\n",
            "      (0): GRU(800, 256, bidirectional=True)\n",
            "    )\n",
            "  )\n",
            "  (lemma_decoder): AttentionalDecoder(\n",
            "    (embs): Embedding(108, 400)\n",
            "    (rnn): GRU(912, 800, num_layers=2, dropout=0.25)\n",
            "    (attn): Attention(\n",
            "      (scorer): GeneralScorer(\n",
            "        (W_a): Linear(in_features=800, out_features=800, bias=False)\n",
            "      )\n",
            "      (linear_out): Linear(in_features=1600, out_features=800, bias=False)\n",
            "    )\n",
            "    (proj): Linear(in_features=800, out_features=108, bias=True)\n",
            "  )\n",
            "  (lm_fwd_decoder): LinearDecoder(\n",
            "    (decoder): Linear(in_features=256, out_features=19482, bias=True)\n",
            "  )\n",
            "  (lm_bwd_decoder): LinearDecoder(\n",
            "    (decoder): Linear(in_features=256, out_features=19482, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "::: Model parameters :::\n",
            "\n",
            "21495670/21495670 trainable/total\n",
            "\n",
            "Starting training\n",
            "\n",
            "Evaluation check every 57/58 batches\n",
            "\n",
            "::: Task schedules :::\n",
            "\n",
            "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
            "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"-inf\"/>\n",
            "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"inf\"/>\n",
            "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"inf\"/>\n",
            "</TaskScheduler>\n",
            "\n",
            "::: LR schedule :::\n",
            "\n",
            "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
            "\n",
            "\n",
            "Evaluating model on dev set...\n",
            "\n",
            "8it [00:03,  2.65it/s]\n",
            "\n",
            "::: Dev losses :::\n",
            "\n",
            "lemma: 2.2406\n",
            "lm_fwd: 7.2017\n",
            "lm_bwd: 7.3009\n",
            "\n",
            "8it [00:03,  2.63it/s]\n",
            "\n",
            "## lemma\n",
            "\n",
            "|                  | accuracy | precision | recall | support |\n",
            "|------------------|----------|-----------|--------|---------|\n",
            "| all              | 0.1087   | 0.0005    | 0.0006 | 19960   |\n",
            "| known-tokens     | 0.1182   | 0.0008    | 0.0009 | 18346   |\n",
            "| unknown-tokens   | 0.0      | 0.0       | 0.0    | 1614    |\n",
            "| ambiguous-tokens | 0.0      | 0.0       | 0.0    | 7772    |\n",
            "| unknown-targets  | 0.0      | 0.0       | 0.0    | 654     |\n",
            "\n",
            "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
            "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.1087\"/>\n",
            "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.201672315597534\"/>\n",
            "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.3008522391319275\"/>\n",
            "</TaskScheduler>\n",
            "\n",
            "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
            "\n",
            "\n",
            "Evaluating model on dev set...\n",
            "\n",
            "8it [00:01,  4.14it/s]\n",
            "\n",
            "::: Dev losses :::\n",
            "\n",
            "lemma: 1.7098\n",
            "lm_fwd: 7.1949\n",
            "lm_bwd: 7.2801\n",
            "\n",
            "8it [00:02,  3.18it/s]\n",
            "\n",
            "## lemma\n",
            "\n",
            "|                  | accuracy | precision | recall | support |\n",
            "|------------------|----------|-----------|--------|---------|\n",
            "| all              | 0.3417   | 0.0036    | 0.0039 | 19960   |\n",
            "| known-tokens     | 0.3717   | 0.0052    | 0.0056 | 18346   |\n",
            "| unknown-tokens   | 0.0006   | 0.0001    | 0.0005 | 1614    |\n",
            "| ambiguous-tokens | 0.4382   | 0.0157    | 0.0151 | 7772    |\n",
            "| unknown-targets  | 0.0      | 0.0       | 0.0    | 654     |\n",
            "\n",
            "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
            "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.3417\"/>\n",
            "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.19493556022644\"/>\n",
            "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.280098974704742\"/>\n",
            "</TaskScheduler>\n",
            "\n",
            "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
            "\n",
            "\n",
            "Evaluating model on dev set...\n",
            "\n",
            "8it [00:01,  4.32it/s]\n",
            "\n",
            "::: Dev losses :::\n",
            "\n",
            "lemma: 1.1550\n",
            "lm_fwd: 7.1385\n",
            "lm_bwd: 7.2084\n",
            "\n",
            "8it [00:03,  2.36it/s]\n",
            "\n",
            "## lemma\n",
            "\n",
            "|                  | accuracy | precision | recall | support |\n",
            "|------------------|----------|-----------|--------|---------|\n",
            "| all              | 0.507    | 0.0103    | 0.0096 | 19960   |\n",
            "| known-tokens     | 0.5512   | 0.0148    | 0.0141 | 18346   |\n",
            "| unknown-tokens   | 0.0043   | 0.0019    | 0.0021 | 1614    |\n",
            "| ambiguous-tokens | 0.7585   | 0.0561    | 0.0532 | 7772    |\n",
            "| unknown-targets  | 0.0      | 0.0       | 0.0    | 654     |\n",
            "\n",
            "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
            "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.507\"/>\n",
            "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.13852846622467\"/>\n",
            "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.208428680896759\"/>\n",
            "</TaskScheduler>\n",
            "\n",
            "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
            "\n",
            "\n",
            "Evaluating model on dev set...\n",
            "\n",
            "8it [00:01,  4.08it/s]\n",
            "\n",
            "::: Dev losses :::\n",
            "\n",
            "lemma: 0.6752\n",
            "lm_fwd: 7.0748\n",
            "lm_bwd: 7.1549\n",
            "\n",
            "8it [00:02,  3.10it/s]\n",
            "\n",
            "## lemma\n",
            "\n",
            "|                  | accuracy | precision | recall | support |\n",
            "|------------------|----------|-----------|--------|---------|\n",
            "| all              | 0.6278   | 0.0657    | 0.0585 | 19960   |\n",
            "| known-tokens     | 0.6784   | 0.0906    | 0.0808 | 18346   |\n",
            "| unknown-tokens   | 0.0527   | 0.0275    | 0.0255 | 1614    |\n",
            "| ambiguous-tokens | 0.8255   | 0.1521    | 0.1605 | 7772    |\n",
            "| unknown-targets  | 0.0245   | 0.0126    | 0.0122 | 654     |\n",
            "\n",
            "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
            "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.6278\"/>\n",
            "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.074831306934357\"/>\n",
            "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.154878616333008\"/>\n",
            "</TaskScheduler>\n",
            "\n",
            "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
            "\n",
            "\n",
            "Evaluating model on dev set...\n",
            "\n",
            "8it [00:01,  4.26it/s]\n",
            "\n",
            "::: Dev losses :::\n",
            "\n",
            "lemma: 0.4382\n",
            "lm_fwd: 7.0382\n",
            "lm_bwd: 7.1065\n",
            "\n",
            "8it [00:02,  2.98it/s]\n",
            "\n",
            "## lemma\n",
            "\n",
            "|                  | accuracy | precision | recall | support |\n",
            "|------------------|----------|-----------|--------|---------|\n",
            "| all              | 0.7371   | 0.1501    | 0.1368 | 19960   |\n",
            "| known-tokens     | 0.7897   | 0.2029    | 0.187  | 18346   |\n",
            "| unknown-tokens   | 0.14     | 0.0743    | 0.0712 | 1614    |\n",
            "| ambiguous-tokens | 0.8798   | 0.2604    | 0.2785 | 7772    |\n",
            "| unknown-targets  | 0.1131   | 0.0572    | 0.0572 | 654     |\n",
            "\n",
            "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
            "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.7371\"/>\n",
            "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.038161039352417\"/>\n",
            "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.106492817401886\"/>\n",
            "</TaskScheduler>\n",
            "\n",
            "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
            "\n",
            "\n",
            "Evaluating model on dev set...\n",
            "\n",
            "8it [00:01,  4.29it/s]\n",
            "\n",
            "::: Dev losses :::\n",
            "\n",
            "lemma: 0.3202\n",
            "lm_fwd: 6.9895\n",
            "lm_bwd: 7.0602\n",
            "\n",
            "8it [00:02,  2.97it/s]\n",
            "\n",
            "## lemma\n",
            "\n",
            "|                  | accuracy | precision | recall | support |\n",
            "|------------------|----------|-----------|--------|---------|\n",
            "| all              | 0.7838   | 0.2165    | 0.1978 | 19960   |\n",
            "| known-tokens     | 0.8335   | 0.2794    | 0.2586 | 18346   |\n",
            "| unknown-tokens   | 0.2187   | 0.1267    | 0.1209 | 1614    |\n",
            "| ambiguous-tokens | 0.895    | 0.3132    | 0.3261 | 7772    |\n",
            "| unknown-targets  | 0.1865   | 0.1005    | 0.101  | 654     |\n",
            "\n",
            "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
            "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.7838\"/>\n",
            "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"6.989481449127197\"/>\n",
            "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.060243904590607\"/>\n",
            "</TaskScheduler>\n",
            "\n",
            "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
            "\n",
            "\n",
            "Evaluating model on dev set...\n",
            "\n",
            "8it [00:01,  4.15it/s]\n",
            "\n",
            "::: Dev losses :::\n",
            "\n",
            "lemma: 0.2837\n",
            "lm_fwd: 6.9515\n",
            "lm_bwd: 7.0100\n",
            "\n",
            "8it [00:02,  3.13it/s]\n",
            "\n",
            "## lemma\n",
            "\n",
            "|                  | accuracy | precision | recall | support |\n",
            "|------------------|----------|-----------|--------|---------|\n",
            "| all              | 0.7869   | 0.2195    | 0.2037 | 19960   |\n",
            "| known-tokens     | 0.8382   | 0.2958    | 0.2788 | 18346   |\n",
            "| unknown-tokens   | 0.2038   | 0.1147    | 0.1093 | 1614    |\n",
            "| ambiguous-tokens | 0.8886   | 0.3309    | 0.3478 | 7772    |\n",
            "| unknown-targets  | 0.1407   | 0.0721    | 0.0721 | 654     |\n",
            "\n",
            "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
            "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.7869\"/>\n",
            "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"6.951501488685608\"/>\n",
            "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"7.009988367557526\"/>\n",
            "</TaskScheduler>\n",
            "\n",
            "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
            "\n",
            "\n",
            "Evaluating model on dev set...\n",
            "\n",
            "8it [00:01,  4.26it/s]\n",
            "\n",
            "::: Dev losses :::\n",
            "\n",
            "lemma: 0.2105\n",
            "lm_fwd: 6.8620\n",
            "lm_bwd: 6.9331\n",
            "\n",
            "8it [00:02,  2.97it/s]\n",
            "\n",
            "## lemma\n",
            "\n",
            "|                  | accuracy | precision | recall | support |\n",
            "|------------------|----------|-----------|--------|---------|\n",
            "| all              | 0.8389   | 0.3098    | 0.293  | 19960   |\n",
            "| known-tokens     | 0.8855   | 0.4064    | 0.3896 | 18346   |\n",
            "| unknown-tokens   | 0.3086   | 0.1864    | 0.1814 | 1614    |\n",
            "| ambiguous-tokens | 0.9049   | 0.3728    | 0.3858 | 7772    |\n",
            "| unknown-targets  | 0.237    | 0.1313    | 0.1318 | 654     |\n",
            "\n",
            "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
            "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.8389\"/>\n",
            "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"6.861984550952911\"/>\n",
            "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"6.9330639243125916\"/>\n",
            "</TaskScheduler>\n",
            "\n",
            "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
            "\n",
            "\n",
            "Evaluating model on dev set...\n",
            "\n",
            "8it [00:01,  4.02it/s]\n",
            "\n",
            "::: Dev losses :::\n",
            "\n",
            "lemma: 0.1857\n",
            "lm_fwd: 6.8010\n",
            "lm_bwd: 6.8722\n",
            "\n",
            "8it [00:02,  3.10it/s]\n",
            "\n",
            "## lemma\n",
            "\n",
            "|                  | accuracy | precision | recall | support |\n",
            "|------------------|----------|-----------|--------|---------|\n",
            "| all              | 0.8547   | 0.3422    | 0.3246 | 19960   |\n",
            "| known-tokens     | 0.8998   | 0.4496    | 0.4325 | 18346   |\n",
            "| unknown-tokens   | 0.342    | 0.2101    | 0.2047 | 1614    |\n",
            "| ambiguous-tokens | 0.9107   | 0.41      | 0.4159 | 7772    |\n",
            "| unknown-targets  | 0.2538   | 0.1417    | 0.1421 | 654     |\n",
            "\n",
            "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
            "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.8547\"/>\n",
            "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"6.800965487957001\"/>\n",
            "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"6.872240424156189\"/>\n",
            "</TaskScheduler>\n",
            "\n",
            "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
            "\n",
            "\n",
            "Evaluating model on dev set...\n",
            "\n",
            "8it [00:01,  4.21it/s]\n",
            "\n",
            "::: Dev losses :::\n",
            "\n",
            "lemma: 0.1632\n",
            "lm_fwd: 6.7487\n",
            "lm_bwd: 6.8097\n",
            "\n",
            "8it [00:03,  2.52it/s]\n",
            "\n",
            "## lemma\n",
            "\n",
            "|                  | accuracy | precision | recall | support |\n",
            "|------------------|----------|-----------|--------|---------|\n",
            "| all              | 0.8617   | 0.3615    | 0.3431 | 19960   |\n",
            "| known-tokens     | 0.9063   | 0.4763    | 0.4605 | 18346   |\n",
            "| unknown-tokens   | 0.3544   | 0.2187    | 0.2132 | 1614    |\n",
            "| ambiguous-tokens | 0.9125   | 0.4085    | 0.4198 | 7772    |\n",
            "| unknown-targets  | 0.2783   | 0.1588    | 0.1588 | 654     |\n",
            "\n",
            "<TaskScheduler patience=\"7\" factor=\"0.5\" threshold=\"0.0001\" min_weight=\"0.2\">\n",
            "    <Task name=\"lemma\" target=\"True\" steps=\"0\" mode=\"max\" weight=\"1.0\" best=\"0.8617\"/>\n",
            "    <Task name=\"lm_fwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"6.748747646808624\"/>\n",
            "    <Task name=\"lm_bwd\" patience=\"2\" factor=\"0.5\" weight=\"0.2\" mode=\"min\" steps=\"0\" best=\"6.809692740440369\"/>\n",
            "</TaskScheduler>\n",
            "\n",
            "<LrScheduler lr=\"0.001\" steps=\"0\" patience=\"4\" threshold=\"0.0\"/>\n",
            "\n",
            "Evaluating model on test set\n",
            "7it [00:03,  1.76it/s]\n",
            "\n",
            "## lemma\n",
            "\n",
            "|                  | accuracy | precision | recall | support |\n",
            "|------------------|----------|-----------|--------|---------|\n",
            "| all              | 0.8648   | 0.3783    | 0.3627 | 17982   |\n",
            "| known-tokens     | 0.9084   | 0.4928    | 0.4761 | 16525   |\n",
            "| unknown-tokens   | 0.3706   | 0.2315    | 0.2263 | 1457    |\n",
            "| ambiguous-tokens | 0.913    | 0.4217    | 0.4368 | 6896    |\n",
            "| unknown-targets  | 0.3041   | 0.177     | 0.1765 | 605     |\n",
            "\n",
            "Saved best model to: [./mymodel-lemma-2023_03_07-14_25_07.tar]\n",
            "8it [00:03,  2.60it/s]\n",
            "Bye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv *.tar mymodel.tar"
      ],
      "metadata": {
        "id": "K6I7Y-o8gjvy",
        "outputId": "6b7fb83b-5e61-4cab-fe33-123848951274",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cicero_%20Pro_Quinctio-pie.txt  lemma_Empty.txt            mymodel.tar\n",
            "Cicero_%20Pro_Quinctio.txt      lemma_Empty.xml            \u001b[0m\u001b[01;34moutput\u001b[0m/\n",
            "config.json                     moliere-pie.txt            \u001b[01;34msample_data\u001b[0m/\n",
            "essai-pie.txt                   moliere.txt\n",
            "essai.txt                       mymodel.results.lemma.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut désormais utiliser ce modèle avec _Pie_:\n"
      ],
      "metadata": {
        "id": "OW0AXvF-QudR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pie tag /content/moliere.txt /content/mymodel.tar"
      ],
      "metadata": {
        "id": "p0Oj_fEwQ28V",
        "outputId": "e0447318-7335-468f-f996-e1d60379d878",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "WARNING:root:\n",
            "It seems like you downloaded `pie` instead of git-cloning it or installing it with pip.\n",
            "We won't be able to check compatibility between pretrained models and `pie` version.\n",
            "\n",
            " - model: /content/mymodel.tar\n",
            " - tasks: lemma\n",
            "Tagging file [/content/moliere.txt]...\n",
            "100% 146/146 [00:14<00:00, 10.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention, _Pie_ n'est pas _Pie extended_. Pour ajouter le modèle à ce dernier vous pouvez suivre [le tuto](https://github.com/hipster-philology/nlp-pie-taggers#add-a-model).\n",
        "\n",
        "Il faut désormais un modèle pour la POS – mais vous avez désormais les armes pour le faire vous-même."
      ],
      "metadata": {
        "id": "TUvBwgTtHVqD"
      }
    }
  ]
}