{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-4b9q7CFk7"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gabays/32M7131/blob/main/Cours_04/Cours04.ipynb)\n",
        "\n",
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Licence Creative Commons\" style=\"border-width:0;float:right;\\\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a>\n",
        "\n",
        "Distant Reading 2: linguistique computationnelle\n",
        "\n",
        "# Entre normalisation et traduction\n",
        "\n",
        "Simon Gabay\n",
        "\n",
        "## Introduction\n",
        "\n",
        "L'idée de ce cours est de découvrir la traduction automatique de manière détournée, en s'intéressant à un problème de philologie: celui de la normalisation du texte. En effet, la sortie d'un OCR prend une forme proche de celle-ci:\n",
        "\n",
        ">QVe cette propoſtion, qu'vn eſpace eſt vuidé\n",
        "\n",
        "Il est évident que personne ne va éditer un texte comme cela, d'autant que les spécialistes du XVIIe s. ont pris l'habitude, dans leurs éditions, d'aligner le système graphique ancien sur le système graphique contemporain. Il nous faudrait donc un résultat du type\n",
        "\n",
        ">QVe cette propostion, qu'un espace est vidé\n",
        "\n",
        "Les philologues pourront déplorer que ce résultat masque un certain nombre de faits linguistiques, ce qui est vrai, mais cela a quelques vertus pour de possibles traitement informatiques, comme de \"lisser\" la langue et d'en retirer des aspérités qui pourraient gêner des algorithmes, comme en stylométrie.\n",
        "\n",
        "\n",
        "## 1. Préparer l'expérience\n",
        "Il faut d'abord installer les paquets nécessaires\n",
        "* _faireseq_ pour la gestion de l'entraînement\n",
        "* _sentencepiece_ pour créer des sous-mots\n",
        "* _sacrebleu_ pour l'évaluation du résultat avec un score BLEU\n",
        "* etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "scrolled": true,
        "id": "T6bZAqsJCFlA",
        "outputId": "6545dce6-79d4-470b-c520-92ed4f9df06a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fairseq@ git+https://github.com/pytorch/fairseq.git@5a75b079bf8911a327940c28794608e003a9fa52\n",
            "  Cloning https://github.com/pytorch/fairseq.git (to revision 5a75b079bf8911a327940c28794608e003a9fa52) to /tmp/pip-install-n3xtut0f/fairseq_559033242dbf46db8ed67657f79d2ed4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pytorch/fairseq.git /tmp/pip-install-n3xtut0f/fairseq_559033242dbf46db8ed67657f79d2ed4\n",
            "  Running command git rev-parse -q --verify 'sha^5a75b079bf8911a327940c28794608e003a9fa52'\n",
            "  Running command git fetch -q https://github.com/pytorch/fairseq.git 5a75b079bf8911a327940c28794608e003a9fa52\n",
            "  Running command git checkout -q 5a75b079bf8911a327940c28794608e003a9fa52\n",
            "  Resolved https://github.com/pytorch/fairseq.git to commit 5a75b079bf8911a327940c28794608e003a9fa52\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  From https://github.com/ngoyal2707/Megatron-LM\n",
            "   * branch            adb23324c222aad0aad89308e70302d996a5eaeb -> FETCH_HEAD\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (from fairseq@ git+https://github.com/pytorch/fairseq.git@5a75b079bf8911a327940c28794608e003a9fa52) (0.29.33)\n",
            "Collecting hydra-core<1.1\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 KB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from fairseq@ git+https://github.com/pytorch/fairseq.git@5a75b079bf8911a327940c28794608e003a9fa52) (1.22.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from fairseq@ git+https://github.com/pytorch/fairseq.git@5a75b079bf8911a327940c28794608e003a9fa52) (1.13.1+cu116)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.8/dist-packages (from fairseq@ git+https://github.com/pytorch/fairseq.git@5a75b079bf8911a327940c28794608e003a9fa52) (1.15.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from fairseq@ git+https://github.com/pytorch/fairseq.git@5a75b079bf8911a327940c28794608e003a9fa52) (2022.6.2)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from fairseq@ git+https://github.com/pytorch/fairseq.git@5a75b079bf8911a327940c28794608e003a9fa52) (4.64.1)\n",
            "Collecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 KB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from hydra-core<1.1->fairseq@ git+https://github.com/pytorch/fairseq.git@5a75b079bf8911a327940c28794608e003a9fa52) (5.12.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 KB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.8/dist-packages (from omegaconf<2.1->fairseq@ git+https://github.com/pytorch/fairseq.git@5a75b079bf8911a327940c28794608e003a9fa52) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from omegaconf<2.1->fairseq@ git+https://github.com/pytorch/fairseq.git@5a75b079bf8911a327940c28794608e003a9fa52) (4.5.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from sacrebleu>=1.4.12->fairseq@ git+https://github.com/pytorch/fairseq.git@5a75b079bf8911a327940c28794608e003a9fa52) (4.9.2)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from sacrebleu>=1.4.12->fairseq@ git+https://github.com/pytorch/fairseq.git@5a75b079bf8911a327940c28794608e003a9fa52) (0.8.10)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi->fairseq@ git+https://github.com/pytorch/fairseq.git@5a75b079bf8911a327940c28794608e003a9fa52) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources->hydra-core<1.1->fairseq@ git+https://github.com/pytorch/fairseq.git@5a75b079bf8911a327940c28794608e003a9fa52) (3.15.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-1.0.0a0+5a75b07-cp38-cp38-linux_x86_64.whl size=7707819 sha256=0ea67e1dab965886eab8946c84bec99e2f005b0cd03c5809a020c949425de285\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/16/46/c5f0272d9a3754ac4368725b06474d6810d52bb8ce4ebce9d8\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141231 sha256=a71a85fbbad82ab960d3a9bfd2fd3b68bdd68096d9419959cbc7e9769fbad779\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d0/ab/d43c02eaddc5b9004db86950802442ad9a26f279c619e28da0\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 colorama-0.4.6 fairseq-1.0.0a0+5a75b07 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.7.0 sacrebleu-2.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sacrebleu in /usr/local/lib/python3.8/dist-packages (2.3.1)\n",
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.8/dist-packages (1.0.7)\n",
            "Collecting omegaconf==2.0.5\n",
            "  Downloading omegaconf-2.0.5-py3-none-any.whl (36 kB)\n",
            "Collecting gdown==4.2.0\n",
            "  Downloading gdown-4.2.0.tar.gz (13 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from omegaconf==2.0.5) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.8/dist-packages (from omegaconf==2.0.5) (6.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown==4.2.0) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown==4.2.0) (2.25.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown==4.2.0) (3.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown==4.2.0) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown==4.2.0) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (1.22.4)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (2.7.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (2022.6.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.8/dist-packages (from sacrebleu) (4.9.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from hydra-core) (5.12.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.8 in /usr/local/lib/python3.8/dist-packages (from hydra-core) (4.8)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources->hydra-core) (3.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.2.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.2.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.2.0) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.2.0) (4.0.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown==4.2.0) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.2.0-py3-none-any.whl size=14262 sha256=8b9ccf45a92f7f289932b2f54e01a3cf87a4bd44da506b08085fdcb4f0a239a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/3c/51/52c46deda5cd1d59c6ce3d441ea5f3d155495dc294c4535a25\n",
            "Successfully built gdown\n",
            "Installing collected packages: sentencepiece, omegaconf, gdown\n",
            "  Attempting uninstall: omegaconf\n",
            "    Found existing installation: omegaconf 2.0.6\n",
            "    Uninstalling omegaconf-2.0.6:\n",
            "      Successfully uninstalled omegaconf-2.0.6\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.2.0 omegaconf-2.0.5 sentencepiece-0.1.97\n"
          ]
        }
      ],
      "source": [
        "!pip install fairseq@git+https://github.com/pytorch/fairseq.git@5a75b079bf8911a327940c28794608e003a9fa52 \n",
        "!pip install sentencepiece sacrebleu hydra-core omegaconf==2.0.5 gdown==4.2.0 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYs3zdFuCFlB"
      },
      "source": [
        "Télécharger les données et les modèles depuis le repo GitHub du cours et les structurer dans les dossiers `data/`, `models/`. Une partie du travail de préparation est fait via un script `structure_files.sh`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": true,
        "id": "tPDSiF6wCFlB",
        "outputId": "b996e446-79d9-4b8e-ff6d-26d54a3d3ec4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-05 18:02:43--  https://github.com/gabays/32M7131/releases/download/Norm/Normalisation-models.zip\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/609944064/426229d6-b699-4f7b-b942-7f27168e037f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230305%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230305T180243Z&X-Amz-Expires=300&X-Amz-Signature=49a89e67dbcaa9597346fc070847b56b0e74d3416d1533a0acecb828d5c83e05&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=609944064&response-content-disposition=attachment%3B%20filename%3DNormalisation-models.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-03-05 18:02:43--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/609944064/426229d6-b699-4f7b-b942-7f27168e037f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230305%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230305T180243Z&X-Amz-Expires=300&X-Amz-Signature=49a89e67dbcaa9597346fc070847b56b0e74d3416d1533a0acecb828d5c83e05&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=609944064&response-content-disposition=attachment%3B%20filename%3DNormalisation-models.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1248768034 (1.2G) [application/octet-stream]\n",
            "Saving to: ‘Normalisation-models.zip’\n",
            "\n",
            "Normalisation-model 100%[===================>]   1.16G  33.4MB/s    in 38s     \n",
            "\n",
            "2023-03-05 18:03:22 (31.0 MB/s) - ‘Normalisation-models.zip’ saved [1248768034/1248768034]\n",
            "\n",
            "Archive:  Normalisation-models.zip\n",
            "  inflating: French-normalisation-data-models/dict_denorm.txt  \n",
            "  inflating: French-normalisation-data-models/utils.py  \n",
            "  inflating: French-normalisation-data-models/structure_files.sh  \n",
            "  inflating: French-normalisation-data-models/align.py  \n",
            "  inflating: French-normalisation-data-models/bpe_joint_1000.model  \n",
            "  inflating: French-normalisation-data-models/dev.trg  \n",
            "  inflating: French-normalisation-data-models/bpe_joint_1000.vocab  \n",
            "  inflating: French-normalisation-data-models/dev.sp.decade.trg  \n",
            "  inflating: French-normalisation-data-models/dev.src  \n",
            "  inflating: French-normalisation-data-models/dev.norm.full.trg  \n",
            "  inflating: French-normalisation-data-models/dict_norm.txt  \n",
            "  inflating: French-normalisation-data-models/test.src  \n",
            "  inflating: French-normalisation-data-models/test.trg  \n",
            "  inflating: French-normalisation-data-models/train.trg  \n",
            "  inflating: French-normalisation-data-models/levenshtein.py  \n",
            "  inflating: French-normalisation-data-models/test.norm.full.trg  \n",
            "  inflating: French-normalisation-data-models/train.src  \n",
            "  inflating: French-normalisation-data-models/lefff-3.4.mlex.gz  \n",
            "  inflating: French-normalisation-data-models/lstm_denorm.pt  \n",
            "  inflating: French-normalisation-data-models/lstm_norm.pt  \n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/gabays/32M7131/releases/download/Norm/Normalisation-models.zip \n",
        "!unzip Normalisation-models.zip\n",
        "!mv -f French-normalisation-data-models data-models\n",
        "!mv data-models/structure_files.sh ./; bash structure_files.sh\n",
        "!rm Normalisation-models.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBYRG4b_CFlB"
      },
      "source": [
        "## 2. Préparation des données à normaliser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuMbPoVYCFlB"
      },
      "source": [
        "Fonctions pour\n",
        "1. lire le contenu d'un fichier ligne par ligne\n",
        "2. les lire depuis un fichier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JKcu9lKJCFlC"
      },
      "outputs": [],
      "source": [
        "# lire un fichier ligne par ligne\n",
        "def read_file(filename):\n",
        "  list_sents = []\n",
        "  with open(filename) as fp:\n",
        "    for line in fp:\n",
        "      list_sents.append(line.strip())\n",
        "  return list_sents\n",
        "\n",
        "# écrire une liste de phrases dans un fichier\n",
        "def write_file(list_sents, filename):\n",
        "    with open(filename, 'w') as fp:\n",
        "        for sent in list_sents:\n",
        "            fp.write(sent + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3SJRUksCFlC"
      },
      "source": [
        "On va pouvoir charger deux textes:\n",
        "1. `dev.src` qui contient un texte _source_, c'est-à-dire à normaliser\n",
        "2. `dev.trg` qui contient un texte _cible_ (en anglais _target_) normalisé à la main."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JTLASC6dCFlC"
      },
      "outputs": [],
      "source": [
        "dev_src = read_file('data/dev.src')\n",
        "dev_trg = read_file('data/dev.trg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD8yv0ZLCFlD"
      },
      "source": [
        "On peu regarder à quoi ressemblent ces deux fichiers que nous venons de charger:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qC0JodX2CFlD",
        "outputId": "67e721d5-b4fb-47f0-fb8f-cbfa87e70d44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src =  1.\n",
            "trg =  1.\n",
            "--\n",
            "src =  1. QVe cette propoſtion, qu'vn eſpace eſt vuidé, repugne au ſens commun.\n",
            "trg =  1. QUe cette proposition, qu'un espace est vidé, répugne au sens commun.\n",
            "--\n",
            "src =  1. QVe tous les corps ont repugnance à ſe ſeparer l'vn de l'autre, & admettre du vuide dans leur interualle;\n",
            "trg =  1. QUe tous les corps ont répugnance à se séparer l'un de l'autre, et admettre du vide dans leur intervalle;\n",
            "--\n",
            "src =  1. QVe tous les corps ont repugnance à ſe ſeparer l'vn de l'autre, & admettre ce vuide apparent dans leur interualle:\n",
            "trg =  1. QUe tous les corps ont répugnance à se séparer l'un de l'autre, et admettre ce vide apparent dans leur intervalle:\n",
            "--\n"
          ]
        }
      ],
      "source": [
        "for i in range(4):\n",
        "    print('src = ', dev_src[i])\n",
        "    print('trg = ', dev_trg[i])\n",
        "    print('--')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi37Kbq0CFlD"
      },
      "source": [
        "On va désormais charger le modèle de segmentation en sous-mots (`bpe_joint_1000.model`). On parle de _Byte Pair Encoding_ ou \"codage par paires d’octets\" ([Sennrich 2016](https://aclanthology.org/P16-1162/)): à partir d'une analyse de tous les caractères, l'algorithme effectue des opérations de fusion pour les paires les plus courantes (comme `ment`, que l'on retrouve souvent dans les adverbes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WB1pgYRzCFlD"
      },
      "outputs": [],
      "source": [
        "import sentencepiece\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file='data/bpe_joint_1000.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxB6zszBCFlD"
      },
      "source": [
        "On applique ce modèle de segmentation sur les données à normaliser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MIV2alvlCFlD"
      },
      "outputs": [],
      "source": [
        "dev_src_sp = spm.encode(dev_src, out_type=str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEs7dmxzCFlE"
      },
      "source": [
        "On sauvegarde le résultat dans un nouveau fichier `dev.sp.src`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2MV3G-IfCFlE"
      },
      "outputs": [],
      "source": [
        "write_file([' '.join(phrase) for phrase in dev_src_sp], 'data/dev.sp.src')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JVS8LetCFlE"
      },
      "source": [
        "Voilà un extrait du fichier que nous venons de créer, avec les fameux BPE ou sous-mots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "scrolled": true,
        "id": "o9HvML5NCFlE",
        "outputId": "7851909b-13eb-4527-d29a-c94375d8a26f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['▁1', '.'],\n",
              " ['▁1',\n",
              "  '.',\n",
              "  '▁Q',\n",
              "  'V',\n",
              "  'e',\n",
              "  '▁cette',\n",
              "  '▁prop',\n",
              "  'ost',\n",
              "  'ion',\n",
              "  ',',\n",
              "  '▁qu',\n",
              "  \"'\",\n",
              "  'vn',\n",
              "  '▁esp',\n",
              "  'ace',\n",
              "  '▁est',\n",
              "  '▁v',\n",
              "  'ui',\n",
              "  'd',\n",
              "  'é',\n",
              "  ',',\n",
              "  '▁re',\n",
              "  'p',\n",
              "  'u',\n",
              "  'gne',\n",
              "  '▁au',\n",
              "  '▁sens',\n",
              "  '▁comm',\n",
              "  'un',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "dev_src_sp[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BrvIw3yCFlE"
      },
      "source": [
        "On définitune fonction pour détokeniser une liste de phrases, c'est à dire de \"recoller\" les BPE pour retrouver nos phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cnc6csGECFlE"
      },
      "outputs": [],
      "source": [
        "def decode_sp(list_sents):\n",
        "    return [''.join(sent).replace(' ', '').replace('▁', ' ').strip() for sent in list_sents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLlMrNPOCFlE"
      },
      "source": [
        "Visualiser à quoi ressemble le texte détokenisé, qui doit ressembler au texte de départ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "blOxyvXPCFlF",
        "outputId": "fd9ef845-ae62-43e4-8e99-552c6ab94b14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1.',\n",
              " \"1. QVe cette propostion, qu'vn espace est vuidé, repugne au sens commun.\",\n",
              " \"1. QVe tous les corps ont repugnance à se separer l'vn de l'autre, & admettre du vuide dans leur interualle;\",\n",
              " \"1. QVe tous les corps ont repugnance à se separer l'vn de l'autre, & admettre ce vuide apparent dans leur interualle:\",\n",
              " '2.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "decode_sp(dev_src_sp[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM6KeAufCFlF"
      },
      "source": [
        "## 3. Appliquer le modèle de normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPMoT2m3CFlF"
      },
      "source": [
        "### 3.1 Etape par étape\n",
        "Appliquer le modèle de normalisation sur le début des données pre-traitées (ça prend moins de temps pour tester que normaliser tout le texte)\n",
        "\n",
        "Il y aura un message \"UserWarning\", mais vous pouvez l'ignorer - ce n'est pas grave.\n",
        "\n",
        "Explications:\n",
        "- `head -n 10` affiche les 10 premières phrases\n",
        "- ces 10 premières lignes sont donné à _fairseq-interactive_ qui effectue la traductin (dans notre cas la normlaisation)\n",
        "- le résultat va dans `data/dev.sp.norm.trg.10.output`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "scrolled": true,
        "id": "OSsTyPKZCFlF"
      },
      "outputs": [],
      "source": [
        "!head -n 10 data/dev.sp.src | fairseq-interactive models/norm/ --source-lang src --target-lang trg --path models/norm/lstm_norm.pt > data/dev.sp.norm.trg.10.output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regardons à quoi ressemble le résultat. Pour lire ce fichier, avec pour un exemple `i`:\n",
        "\n",
        "- `S-i`: le texte source\n",
        "- `H-i`: le score de l'hypothèse et l'hypothèse du modèle (c'est-à-dire la prédiction)\n",
        "- `P-i`: les scores de chaque sous-token produit par le modèle"
      ],
      "metadata": {
        "id": "LX32GRWDH7YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 26 data/dev.sp.norm.trg.10.output | tail -n 20"
      ],
      "metadata": {
        "id": "eMbCUL-YH0V8",
        "outputId": "42a9b42d-72e8-4c0c-82c3-bdca9c367f90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S-0\t▁1 .\n",
            "W-0\t0.595\tseconds\n",
            "H-0\t-0.00011481382534839213\t▁1 .\n",
            "D-0\t-0.00011481382534839213\t▁1 .\n",
            "P-0\t-0.0000 -0.0003 -0.0000\n",
            "S-1\t▁1 . ▁Q V e ▁cette ▁prop ost ion , ▁qu ' vn ▁esp ace ▁est ▁v ui d é , ▁re p u gne ▁au ▁sens ▁comm un .\n",
            "W-1\t0.082\tseconds\n",
            "H-1\t-0.039981186389923096\t▁1 . ▁Q U e ▁cette ▁prop ost ion , ▁qu ' un ▁esp ace ▁est ▁v ui d é , ▁rép u gne ▁au ▁sens ▁comm un .\n",
            "D-1\t-0.039981186389923096\t▁1 . ▁Q U e ▁cette ▁prop ost ion , ▁qu ' un ▁esp ace ▁est ▁v ui d é , ▁rép u gne ▁au ▁sens ▁comm un .\n",
            "P-1\t-0.0000 -0.0000 -0.0043 -0.0632 -0.0006 -0.0000 -0.0001 -0.9353 -0.0001 -0.0012 -0.0000 0.0000 -0.0001 -0.0078 -0.0070 -0.0000 -0.0022 -0.1168 -0.0001 -0.0000 -0.0000 -0.0389 -0.0157 -0.0053 -0.0000 -0.0000 -0.0001 -0.0000 -0.0004 -0.0000\n",
            "S-2\t▁1 . ▁Q V e ▁tous ▁les ▁cor p s ▁ont ▁re p u gn ance ▁à ▁se ▁se p are r ▁l ' vn ▁de ▁l ' autre , ▁& ▁ad m ettre ▁du ▁v ui de ▁dans ▁leur ▁in ter u al le ;\n",
            "W-2\t0.126\tseconds\n",
            "H-2\t-0.01945100724697113\t▁1 . ▁Q U e ▁tous ▁les ▁cor p s ▁ont ▁rép u gn ance ▁à ▁se ▁s ép are r ▁l ' un ▁de ▁l ' autre , ▁et ▁ad m ettre ▁du ▁v ui de ▁dans ▁leur ▁in ter v és le ;\n",
            "D-2\t-0.01945100724697113\t▁1 . ▁Q U e ▁tous ▁les ▁cor p s ▁ont ▁rép u gn ance ▁à ▁se ▁s ép are r ▁l ' un ▁de ▁l ' autre , ▁et ▁ad m ettre ▁du ▁v ui de ▁dans ▁leur ▁in ter v és le ;\n",
            "P-2\t-0.0000 -0.0001 -0.0040 -0.1684 -0.0004 -0.0000 -0.0000 -0.0000 -0.0007 -0.0000 -0.0001 -0.1220 -0.0063 -0.0002 -0.0137 -0.0000 -0.0000 -0.0002 -0.0001 -0.0248 -0.0022 -0.0003 -0.0000 -0.0000 -0.0000 -0.0000 -0.0000 -0.0000 -0.0002 -0.0001 -0.0000 -0.0000 -0.0000 -0.0000 -0.0383 -0.0173 -0.0006 -0.0000 -0.0000 -0.0000 -0.0066 -0.0016 -0.4856 -0.0007 -0.0002 -0.0000\n",
            "S-3\t▁1 . ▁Q V e ▁tous ▁les ▁cor p s ▁ont ▁re p u gn ance ▁à ▁se ▁se p are r ▁l ' vn ▁de ▁l ' autre , ▁& ▁ad m ettre ▁ce ▁v ui de ▁app ar ent ▁dans ▁leur ▁in ter u al le :\n",
            "W-3\t0.126\tseconds\n",
            "H-3\t-0.013001253828406334\t▁1 . ▁Q U e ▁tous ▁les ▁cor p s ▁ont ▁rép u gn ance ▁à ▁se ▁s ép are r ▁l ' un ▁de ▁l ' autre , ▁et ▁ad m ettre ▁ce ▁v ui de ▁app ar ent ▁dans ▁leur ▁in ter v és le :\n",
            "D-3\t-0.013001253828406334\t▁1 . ▁Q U e ▁tous ▁les ▁cor p s ▁ont ▁rép u gn ance ▁à ▁se ▁s ép are r ▁l ' un ▁de ▁l ' autre , ▁et ▁ad m ettre ▁ce ▁v ui de ▁app ar ent ▁dans ▁leur ▁in ter v és le :\n",
            "P-3\t-0.0000 -0.0001 -0.0034 -0.1700 -0.0003 -0.0000 -0.0000 -0.0000 -0.0007 -0.0000 -0.0001 -0.1209 -0.0067 -0.0001 -0.0144 -0.0000 -0.0000 -0.0003 -0.0001 -0.0211 -0.0022 -0.0003 -0.0000 -0.0000 -0.0000 -0.0000 -0.0000 -0.0001 -0.0002 -0.0001 -0.0000 -0.0000 -0.0000 -0.0001 -0.0109 -0.0210 -0.0014 -0.0002 -0.0009 -0.0014 -0.0000 -0.0000 -0.0000 -0.0031 -0.0120 -0.2442 -0.0004 -0.0001 -0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTRxz9K-CFlF"
      },
      "source": [
        "On définit une fonction pour extraire l'hypothèse (la ligne commençant par `H` donc) de ce fichier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SjmXAhX9CFlF"
      },
      "outputs": [],
      "source": [
        "def extract_hypothesis(filename):\n",
        "    outputs = []\n",
        "    with open(filename) as fp:\n",
        "        for line in fp:\n",
        "            # seulement les lignes qui commencet par H- (pour Hypothèse)\n",
        "            if 'H-' in line:\n",
        "                # prendre la 3ème colonne (c'est-à-dire l'indice 2)\n",
        "                outputs.append(line.strip().split('\\t')[2])\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37VZKIK8CFlF"
      },
      "source": [
        "On peut désormais extraire les hypothèses du fichier produit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Y0eoF2DDCFlF",
        "outputId": "4793aab8-c34c-448f-d627-3abe237236b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['▁1 .',\n",
              " \"▁1 . ▁Q U e ▁cette ▁prop ost ion , ▁qu ' un ▁esp ace ▁est ▁v ui d é , ▁rép u gne ▁au ▁sens ▁comm un .\",\n",
              " \"▁1 . ▁Q U e ▁tous ▁les ▁cor p s ▁ont ▁rép u gn ance ▁à ▁se ▁s ép are r ▁l ' un ▁de ▁l ' autre , ▁et ▁ad m ettre ▁du ▁v ui de ▁dans ▁leur ▁in ter v és le ;\"]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "dev_norm_10 = extract_hypothesis('data/dev.sp.norm.trg.10.output')\n",
        "dev_norm_10[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqSje0hdCFlG"
      },
      "source": [
        "On peut désormais détokeniser le résultat avec la fonction `decode_sp` que nous avons défini plus haut:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ikDPDxYdCFlG",
        "outputId": "43b0d0a5-6a27-446c-d424-fa91fc4858d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1.',\n",
              " \"1. QUe cette propostion, qu'un espace est vuidé, répugne au sens commun.\",\n",
              " \"1. QUe tous les corps ont répugnance à se séparer l'un de l'autre, et admettre du vuide dans leur intervésle;\"]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "dev_norm_10_postproc = decode_sp(dev_norm_10)\n",
        "dev_norm_10_postproc[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYJ6ewy3CFlG"
      },
      "source": [
        "Il ne reste plus qu'à sauvegarder le résultat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "gKVwreqyCFlG"
      },
      "outputs": [],
      "source": [
        "write_file(dev_norm_10_postproc, 'data/dev.norm.10.trg')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Tout d'un coup\n",
        "Comme on ne va pas tout refaire étape par étape à chaque fois, maintenant que nous avons compris le fonctionnement, on crée une fonction qui permet de tout faire d'un coup."
      ],
      "metadata": {
        "id": "GTUiGaWdU6pI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalise(sents):\n",
        "    # generate temporary file\n",
        "    filetmp = 'data/tmp_norm.sp.src.tmp'\n",
        "    # preprocessing\n",
        "    input_sp = spm.encode(sents, out_type=str)\n",
        "    # add decade token to each sentence\n",
        "    input_sp_sents = [' '.join(sent) for sent in input_sp]\n",
        "    write_file(input_sp_sents, filetmp)\n",
        "    #print(\"preprocessed = \", input_sp_sents)\n",
        "    # denormalisation\n",
        "    !cat data/tmp_norm.sp.src.tmp | fairseq-interactive models/norm --source-lang src --target-lang trg --path models/norm/lstm_norm.pt > data/tmp_norm.sp.src.output 2> /tmp/dev\n",
        "    # postprocessing\n",
        "    outputs = extract_hypothesis('data/tmp_norm.sp.src.output')\n",
        "    outputs_postproc = decode_sp(outputs)\n",
        "    return outputs_postproc"
      ],
      "metadata": {
        "id": "YJdVB2FsUxLM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La fonction s'utilise comme suit:"
      ],
      "metadata": {
        "id": "HV56r_8LU_Pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalise([\"1. QVe cette propostion, qu'vn espace est vuidé, repugne au sens commun.\",\n",
        "          \"Affectoit un mépris qui marquoit ſon eſtime,\"])"
      ],
      "metadata": {
        "id": "4CRm-yLGVD4o",
        "outputId": "23b5e381-544f-44af-a90b-8d0eb037aa51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"1. QUe cette propostion, qu'un espace est vuidé, répugne au sens commun.\",\n",
              " 'Affectait un mépris qui marquait son estime,']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si je veux traiter tout le fichier (attention, cela prend un peu de temps, même avec un GPU):"
      ],
      "metadata": {
        "id": "-LO2Jp3RVNTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_norm = normalise(dev_src)\n",
        "#Je sauvegarde\n",
        "write_file(dev_norm, 'data/dev.norm.trg')"
      ],
      "metadata": {
        "id": "fa95MmCXVMkW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDfClaIjCFlG"
      },
      "source": [
        "## 4. Contrôle qualité\n",
        "Nous pouvons faire deux choses pour contrôler la qualité de notre travail:\n",
        "1. Mesurer l'efficacité de notre modèle\n",
        "2. Comparer ces résultats avec une autre méthode\n",
        "\n",
        "### 4.1 Quelques métriques\n",
        "On se rappelle que pour ce premier cas de normalisation nous avons une version _gold_, qui nous permet de comparer les prédictions avec une version \"parfaite\". Afin de mesurer la distance entre le résultat obtenu et le résultat attendu, nous avons plusieurs métriques à disposition:\n",
        "- BLEU: la métrique d'évaluation la plus fréquemment utilisée en traduction automatique ([Papineni 2002](https://aclanthology.org/P02-1040/))\n",
        "- ChrF: _Character F-score_ (comme le score BLEU mais basé sur des n-grams de caractères, cf.[Popović 2015](https://aclanthology.org/W15-3049/))\n",
        "- TER: _Translation Edit Rate_ ([Snover 2006](https://aclanthology.org/2006.amta-papers.25/))\n",
        "\n",
        "Attention : puisque nous avons seulement normalisé 10 phrases, il faut seulement comparer contre les 10 première phrases de référence. Pour un résultat plus fiable, il faudrait calculer ces scores sur un plus grand nombre de phrases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "jnrKcz1VCFlG",
        "outputId": "964e3082-53f8-48b1-ff06-6a28ee68c64b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BLEU = 85.90 94.5/88.4/83.2/78.4 (BP = 1.000 ratio = 1.000 hyp_len = 199 ref_len = 199)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF, TER\n",
        "bleu = BLEU()\n",
        "bleu.corpus_score(dev_norm_10_postproc, [dev_trg[:10]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SieYL2K0CFlG",
        "outputId": "9ab93787-e128-453c-e2b1-d636736fddca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "chrF2 = 95.73"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "chrf = CHRF()\n",
        "chrf.corpus_score(dev_norm_10_postproc, [dev_trg[:10]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "CFUmRYFaCFlG",
        "outputId": "cbfda15b-0859-4212-9e7e-9c4ed7adc239",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TER = 6.55"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "ter = TER()\n",
        "ter.corpus_score(dev_norm_10_postproc, [dev_trg[:10]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmjbIv1iCFlG"
      },
      "source": [
        "Une évaluation plus adaptée : la précision au niveau de chaque mot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Td3NGxLLCFlG",
        "outputId": "7264a0c2-7a36-4990-d8f8-fc19cf215111",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['1', '.'], ['1', '.', 'QUe', 'cette', 'proposition>propostion', ',', \"qu'\", 'un', 'espace', 'est', 'vidé>vuidé', ',', 'répugne', 'au', 'sens', 'commun', '.'], ['1', '.', 'QUe', 'tous', 'les', 'corps', 'ont', 'répugnance', 'à', 'se', 'séparer', \"l'\", 'un', 'de', \"l'\", 'autre', ',', 'et', 'admettre', 'du', 'vide>vuide', 'dans', 'leur', 'intervalle>intervésle', ';'], ['1', '.', 'QUe', 'tous', 'les', 'corps', 'ont', 'répugnance', 'à', 'se', 'séparer', \"l'\", 'un', 'de', \"l'\", 'autre', ',', 'et', 'admettre', 'ce', 'vide>vuide', 'apparent', 'dans', 'leur', 'intervalle>intervésle', ':'], ['2', '.'], ['2', '.', 'Que', 'cette', 'horreur', 'ou', 'répugnance', \"qu'\", 'ont', 'tous', 'les', 'corps', ',', \"n'\", 'est', 'pas', 'plus', 'grande', 'pour', 'admettre', 'un', 'grand', 'vide>vuide', ',', \"qu'\", 'un', 'petit', ':'], ['2', '.', 'Que', 'cette', 'horreur', 'où', 'cette', 'répugnance', \"qu'\", 'ont', 'tous', 'les', 'corps', \"n'\", 'est', 'pas', 'plus', 'grande', 'pour', 'admettre', 'un', 'grand', 'vide>vuide', 'apparent', \"qu'\", 'un', 'petit', ':'], ['2', '.', 'Que', 'cette', 'proposition', ',', 'que', 'la', 'Nature', 'abhorre', 'le', 'vide>vuide', ',', 'et', 'néanmoins', \"l'\", 'admet', ',', \"l'\", 'accuse', \"d'\", 'impuissance', ',', 'ou', 'implique', 'contradiction', '.'], ['2', '.', \"Qu'\", 'il', \"n'\", 'est', 'pas', 'plein', 'de', \"l'\", 'air', 'que', 'quelques', 'Philosophes', 'disent', 'être', 'enfermé', 'dans', 'les', 'pores', 'de', 'tous', 'les', 'corps', ',', 'qui', 'se', 'trouverait', 'par', 'ce', 'moyen', ',', 'au', 'dedans', 'de', 'la', 'liqueur', 'qui', 'remplit', 'les', 'tuyaux>tuiaux', '.'], ['2', '.', 'Un', 'soufflet>souflet', 'bien', 'fermé', 'de', 'tous', 'côtés', 'fait', 'le', 'même', 'effet', ',', 'avec', 'une', 'pareille', 'préparation', ':']]\n"
          ]
        }
      ],
      "source": [
        "import align\n",
        "# d'abord créer un fichier qui ne contient que les 10 première phrases du document cible\n",
        "!head -n 10 data/dev.trg > data/dev.10.trg\n",
        "align_dev_norm_10 = align.align('data/dev.10.trg', 'data/dev.norm.10.trg')\n",
        "\n",
        "print(align_dev_norm_10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCub5rjyCFlG"
      },
      "source": [
        "Le résultat de l'alignement est une liste de phrases, où chaque mot de la phrase est comme suit:\n",
        "\n",
        "- le mot tout seul s'il est pareil dans les deux textes (ex : `QUe`)\n",
        "- le mot du premier document et le mot du deuxième document, séparé par `>` s'ils sont différents (ex : `proposition>propostion`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MSe6kVkuCFlH",
        "outputId": "824ba0de-04bd-49f1-b13b-3d1e7b86b8f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy = 0.9490740740740741\n"
          ]
        }
      ],
      "source": [
        "num_diff = 0\n",
        "total = 0\n",
        "for sentence in align_dev_norm_10:\n",
        "    for word in sentence:\n",
        "        if '>' in word:\n",
        "            num_diff += 1\n",
        "        total += 1\n",
        "print('Accuracy = ' + str((total - num_diff)/total))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Utiliser ces métriques avec le test\n",
        "Si vous avez bien travaillé, vous avez prévu un jeu de test, qui n'a pas été vu pendant l'entraînement. Vous pouvez donc normaliser ce jeu de données, et appliquer les métriques que nous venons de voir pour évaluer proprement l'efficacité de notre modèle\n"
      ],
      "metadata": {
        "id": "A4PN-7_7YlUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#METTRE ICI VOTRE CODE"
      ],
      "metadata": {
        "id": "iDTGMwEjY7Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Construire une _baseline_\n",
        "La _baseline_ va être le score obtenu avec une méthode plus rudimentaire, pour bien contrôler que nous n'ontiendrions pas des résulats aussi bon sans tout le travail que nous venons de faire. Il est par exemple possible d'utiliser des expressions régulières."
      ],
      "metadata": {
        "id": "NQECKuKAV3By"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On crée une fonction qui va normaliser une phrase avec une fonction qui normalise un mot\n",
        "import utils\n",
        "from importlib import reload\n",
        "reload(utils)\n",
        "def normalise_sent(sent, normalise_word_function):\n",
        "    norm_sent = []\n",
        "    # On tokenise la phrase (de manière pas très très propre…) et on applique la normalisation choisie à chaque token\n",
        "    for word in utils.basic_tokenise(sent).split():\n",
        "        norm_sent.append(normalise_word_function(word))\n",
        "    return utils.detokenise(' '.join(norm_sent))"
      ],
      "metadata": {
        "id": "DXIhfYGcWhXO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parmi les différentes options qui s'offre à nous, nous pouvons… ne rien faire"
      ],
      "metadata": {
        "id": "Urr8eSQDXET5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function that returns the word itself\n",
        "def return_word(word):\n",
        "    return word"
      ],
      "metadata": {
        "id": "Dw_Y4DqPXOxt"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut désormais normaliser la phrase avec notre fonction `normalise_sent`, qui applique la fonction `return_word` pour chaque token"
      ],
      "metadata": {
        "id": "_454bjVsXTQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalise_word_function = return_word\n",
        "normalise_sent(\"QVe cette propoſtion, qu'vn eſpace eſt vuidé\", normalise_word_function)"
      ],
      "metadata": {
        "id": "lG5yGKN2XgPy",
        "outputId": "765e841e-87bf-45dc-d426-135b4a34664d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"QVe cette propoſtion, qu'vn eſpace eſt vuidé\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C'est super, mais ça sert à rien… Essayons de faire mieux avec une regex"
      ],
      "metadata": {
        "id": "yk8fpasZXhjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_long_s(word):\n",
        "    word = word.replace('ſ', 's')\n",
        "    return word"
      ],
      "metadata": {
        "id": "k2waj4BQXvcF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut tester cette nouvelle fonction:"
      ],
      "metadata": {
        "id": "2oJPYdlcX3Zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalise_word_function = replace_long_s\n",
        "normalise_sent(\"QVe cette propoſtion, qu'vn eſpace eſt vuidé\", normalise_word_function)"
      ],
      "metadata": {
        "id": "TnSBlJW5X6Ve",
        "outputId": "1aacdddc-eb92-4fcd-fa47-27fe1f5e23a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"QVe cette propostion, qu'vn espace est vuidé\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut maintenant tester avec plusieurs regex"
      ],
      "metadata": {
        "id": "JLuNsZTZYBSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def replace_regex(word):\n",
        "    word = word.replace('ſ', 's')\n",
        "    word = re.sub(\"([Qq])v\", r'\\1u', word)\n",
        "    word = re.sub(\"([Qq])V\", r'\\1U', word)\n",
        "    word = re.sub(\"('?)vn(e?)\", r'\\1un\\2', word)\n",
        "    return word\n",
        "\n",
        "normalise_word_function = replace_regex\n",
        "normalise_sent(\"QVe cette propoſtion, qu'vn eſpace eſt vuidé\", normalise_word_function)"
      ],
      "metadata": {
        "id": "iL1V23KFYK-e",
        "outputId": "82f9a340-3f42-417a-dde7-5cde59d0ba4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"QUe cette propostion, qu'un espace est vuidé\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Je n'ai plus qu'à traiter tout le fichier… et évaluer le résultat avec les métriques précédentes! à vous de jouer!"
      ],
      "metadata": {
        "id": "lOgJmR92YSpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#METTRE ICI VOTRE CODE"
      ],
      "metadata": {
        "id": "vDFJwap6YcBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Entraîner un modèle\n",
        "\n",
        "Nous allons entraîner un modèle de segmentation en sous-mots en avec le _toolkit_ [SentencePiece](https://github.com/google/sentencepiece/blob/master/README.md).\n",
        "\n",
        "Ce sera un modèle \"joint\", c'est-à-dire entraîné pour segmenter la langue source et la langue cible. On peut ainsi faire des sous-mots qui peuvent être partagés par les deux langues. Ce type de modèle est particulièrement utile pour deux langues proches lexicalement, comme c'est le cas entre le français du XVIIe s. et le français contemporain, mais pas pour les langues trop distantes (du français au chinois).\n",
        "\n",
        "La taille du vocabulaire ici est de 2000, mais ceci peut être changé. La taille du vocabulaire détermine combien de sous-tokens sont utilisés. Plus le vocabulaire est petit, plus le texte sera découpé, plus le vocabulaire est grand, moins le texte sera découpé (ça ressemblera plus à un découpage sur les espaces). La taille du vocabulaire dépend évidemment des cas (on dit qu'il est _task dependent_) et doit être testée pour être définie optimalement."
      ],
      "metadata": {
        "id": "MCe1K3Nmar9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On mélange la source et la cible pour faire un seul fichier\n",
        "!cat data/train.src data/train.trg > data/all_train.src-trg\n",
        "#On crée le vocabulaire\n",
        "sentencepiece.SentencePieceTrainer.train(input='data/all_train.src-trg', \n",
        "                               model_prefix='data/bpe_joint_2000', \n",
        "                               vocab_size=2000)"
      ],
      "metadata": {
        "id": "wXNe_CDsehG6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons désormais préparer nos trois jeux de données (`train`, `dev` et `test`)\n"
      ],
      "metadata": {
        "id": "xHsuxN2ZiQoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#On charge les jeux de données\n",
        "train_src = read_file('data/train.src')\n",
        "train_trg = read_file('data/train.trg')\n",
        "dev_src = read_file('data/dev.src')\n",
        "dev_trg = read_file('data/dev.trg')\n",
        "test_src = read_file('data/test.src')\n",
        "test_trg = read_file('data/test.trg')\n",
        "\n",
        "# On charge le modèle que nous venons de fabriquer avec SentencePiece\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file='data/bpe_joint_2000.model')\n",
        "\n",
        "# On applique le modèle aux jeux de données\n",
        "train_src_sp = spm.encode(train_src, out_type=str)\n",
        "train_trg_sp = spm.encode(train_trg, out_type=str)\n",
        "dev_src_sp = spm.encode(dev_src, out_type=str)\n",
        "dev_trg_sp = spm.encode(dev_trg, out_type=str)\n",
        "test_src_sp = spm.encode(test_src, out_type=str)\n",
        "test_trg_sp = spm.encode(test_trg, out_type=str)\n",
        "\n",
        "# On contrôle le résultat (src et trg doivent avoir la même longueur pour chaque type de jeu)\n",
        "print(len(train_src_sp), len(train_trg_sp))\n",
        "print(len(dev_src_sp), len(dev_trg_sp))\n",
        "print(len(test_src_sp), len(test_trg_sp))\n",
        "\n",
        "# On crée les fichiers\n",
        "write_file([' '.join(sent) for sent in train_src_sp], 'data/train.sp2000.src')\n",
        "write_file([' '.join(sent) for sent in train_trg_sp], 'data/train.sp2000.trg')\n",
        "write_file([' '.join(sent) for sent in dev_src_sp], 'data/dev.sp2000.src')\n",
        "write_file([' '.join(sent) for sent in dev_trg_sp], 'data/dev.sp2000.trg')\n",
        "write_file([' '.join(sent) for sent in test_src_sp], 'data/test.sp2000.src')\n",
        "write_file([' '.join(sent) for sent in test_trg_sp], 'data/test.sp2000.trg')"
      ],
      "metadata": {
        "id": "PHQQbipOigd4",
        "outputId": "637dd765-d74a-4b70-9ba0-a73f460c600a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17930 17930\n",
            "2443 2443\n",
            "5706 5706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour entrainer un modèle, il va d'abord falloir binariser les données. En gros tout devient des chiffres, ce qui permet à la machine d'aller plus vite. Ces chiffres correspondent aux entrées d'un dictionnaire où sont stockés les \"vrais\" mots."
      ],
      "metadata": {
        "id": "Jw-5slmBdsuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-preprocess --destdir data/data_norm_bin_2000/ \\\n",
        "                    -s trg -t src \\\n",
        "                    --trainpref data/train.sp2000 \\\n",
        "                    --validpref data/dev.sp2000 \\\n",
        "                    --testpref data/test.sp2000 \\\n",
        "                    --joined-dictionary"
      ],
      "metadata": {
        "id": "wSk6hw-tjFAI",
        "outputId": "3de14f16-0dad-4f2b-b3cc-dc23e39dbb75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-05 18:19:06 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data/data_norm_bin_2000/', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=True, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='trg', srcdict=None, suppress_crashes=False, target_lang='src', task='translation', tensorboard_logdir=None, testpref='data/test.sp2000', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='data/train.sp2000', use_plasma_view=False, user_dir=None, validpref='data/dev.sp2000', wandb_project=None, workers=1)\n",
            "2023-03-05 18:19:08 | INFO | fairseq_cli.preprocess | [trg] Dictionary: 2064 types\n",
            "2023-03-05 18:19:10 | INFO | fairseq_cli.preprocess | [trg] data/train.sp2000.trg: 17930 sents, 408300 tokens, 0.0% replaced by <unk>\n",
            "2023-03-05 18:19:10 | INFO | fairseq_cli.preprocess | [trg] Dictionary: 2064 types\n",
            "2023-03-05 18:19:11 | INFO | fairseq_cli.preprocess | [trg] data/dev.sp2000.trg: 2443 sents, 63490 tokens, 0.00788% replaced by <unk>\n",
            "2023-03-05 18:19:11 | INFO | fairseq_cli.preprocess | [trg] Dictionary: 2064 types\n",
            "2023-03-05 18:19:11 | INFO | fairseq_cli.preprocess | [trg] data/test.sp2000.trg: 5706 sents, 136949 tokens, 0.0117% replaced by <unk>\n",
            "2023-03-05 18:19:11 | INFO | fairseq_cli.preprocess | [src] Dictionary: 2064 types\n",
            "2023-03-05 18:19:14 | INFO | fairseq_cli.preprocess | [src] data/train.sp2000.src: 17930 sents, 418815 tokens, 0.0% replaced by <unk>\n",
            "2023-03-05 18:19:14 | INFO | fairseq_cli.preprocess | [src] Dictionary: 2064 types\n",
            "2023-03-05 18:19:14 | INFO | fairseq_cli.preprocess | [src] data/dev.sp2000.src: 2443 sents, 65107 tokens, 0.00768% replaced by <unk>\n",
            "2023-03-05 18:19:14 | INFO | fairseq_cli.preprocess | [src] Dictionary: 2064 types\n",
            "2023-03-05 18:19:15 | INFO | fairseq_cli.preprocess | [src] data/test.sp2000.src: 5706 sents, 140722 tokens, 0.0107% replaced by <unk>\n",
            "2023-03-05 18:19:15 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data/data_norm_bin_2000/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant on peut appeler `fairseq-train` pour enrainer un modèle LSTM ([Hochreiter 1997](https://www.bioinf.jku.at/publications/older/2604.pdf)).\n",
        "\n",
        "🚨 Il y en a pour plusieurs heures!\n",
        "\n",
        "Pour l'entraînement, plusieurs paramètres sont à disposition:\n",
        "\n",
        "- `--save-dir` permet de dire où le modèle sera traité. Le meilleur modèle, sauvegardé dans le dossier indiqué, prendra le nom de `checkpoint_best.pt`.\n",
        "- `--save-interval` permet de définir la fréquence de sauvegarde (_checkpoint_), la valeur fournie déterminant le nombre _n_ d'_epochs_ entre deux _checkpoints_. La dernière sauvegarde est disponible avec le nom `checkpoint_last.pt`.\n",
        "- `--arch lstm` permet de préciser que nous voulons une archicture LSTM et non _transformer_ (il faudrait alors utiliser `--arch transformer`).\n",
        "- Une série de paramètres va permettre de déterminer le nombre de couches pour l'encodeur (`--encoder-layers`) et le décodeur (`--decoder-layers`). Une couche convient aux problèmes très simples: plus on augmente le nombre de couches (2 ou 3 par exemple), meilleurs seront (théoriquement) les résultats mais plus dur sera l'entraînement.\n",
        "- On va ensuite définir la taille des _embeddings_ (vecteurs) qui représentent les tokens pour l'encodeur (`--encoder-embed-dim`), le décodeur (`--decoder-embed-dim`) et la sortie (`--decoder-out-embed-dim`) et la sortie. La valeur se situe entre 100 et 1000 (et tourne généralement autour de 300). Plus on augmente ce chiffre plus on ajoute d'information (mais la quantité d'information ajoutée va decrescendo), plus on réduit la valeur, moins on retient d'information (et on perd donc en sémantisme).\n",
        "- Il faut aussi définir la taille de la couche/représentation cachée pour l'encodeur (`--encoder-hidden-size`) et le décodeur (`--decoder-hidden-size`), soit le nombre de _features_ retenu.\n",
        "- `--encoder-bidirectional` précise que l'encodeur est bidirectionnel (Bi-LSTM) et non unidirectionnel (LSTM): on utilise le contexte gauche _et droit_ du token. La représentation finale des mots de l'entrée est la concaténation des représentations gauche-droite et droite-gauche.\n",
        "- `--dropout` permet de déterminer la proportion de neurones désactivés lors de l'entraînement pour éviter l'_overfitting_.\n",
        "- `--optimizer` permet de déterminer l'optimiseur pour la descente de gradient.\n",
        "- `--lr` définit le taux d'apprentissage (_learning rate_).\n",
        "- `--lr-scheduler` va déterminer l'ajustement du _lrearning rate_.\n",
        "- `--warmup-updates` va permettre d'utiliser un taux d'apprentissage très faible au début du processus d'entraînement, avant de passer au _lr_ normal. On va ainsi limiter l'effet des premières données vues par le modèle.\n",
        "- `--share-all-embeddings` précise que l'on choisit de partager les _embeddings_ entre l'encodeur et le décodeur pour profiter de la similarité lexicale entre la langue source et langue cible (et pour accélerer l'entraînement).\n",
        "- `--max-tokens` définit la _batch size_, soit le nombre d'exemples envoyés en une seule fois au modèle lors de l'apprentissage."
      ],
      "metadata": {
        "id": "sFw1iuTujHwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an empty model folder to store the model in\n",
        "!mkdir models/new_norm_lstm\n",
        "\n",
        "# call fairseq-train\n",
        "!fairseq-train \\\n",
        "        data/data_norm_bin_2000 \\\n",
        "        --save-dir models/new_norm_lstm \\\n",
        "        --save-interval 1 --patience 12 \\\n",
        "        --arch lstm \\\n",
        "        --encoder-layers 3 --decoder-layers 3 \\\n",
        "        --encoder-embed-dim 384 --decoder-embed-dim 384 --decoder-out-embed-dim 384 \\\n",
        "        --encoder-hidden-size 768 --encoder-bidirectional --decoder-hidden-size 768 \\\n",
        "        --dropout 0.3 \\\n",
        "        --criterion cross_entropy --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "        --lr 0.0001 --lr-scheduler inverse_sqrt \\\n",
        "        --warmup-updates 4000 \\\n",
        "        --share-all-embeddings \\\n",
        "        --max-tokens 3000 \\\n",
        "        --batch-size-valid 64"
      ],
      "metadata": {
        "id": "KuseEFZ6wlbQ",
        "outputId": "96a3b36b-f31c-41bb-8f3a-8d0014030d19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-05 18:19:30 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 3000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 3000, 'batch_size_valid': 64, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0001], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': 'models/new_norm_lstm', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 12, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='lstm', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_softmax_cutoff='10000,50000,200000', all_gather_list_size=16384, arch='lstm', azureml_logging=False, batch_size=None, batch_size_valid='64', best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, cpu=False, cpu_offload=False, criterion='cross_entropy', curriculum=0, data='data/data_norm_bin_2000', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention='1', decoder_dropout_in=0.3, decoder_dropout_out=0.3, decoder_embed_dim=384, decoder_embed_path=None, decoder_freeze_embed=False, decoder_hidden_size=768, decoder_layers=3, decoder_out_embed_dim=384, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_bidirectional=True, encoder_dropout_in=0.3, encoder_dropout_out=0.3, encoder_embed_dim=384, encoder_embed_path=None, encoder_freeze_embed=False, encoder_hidden_size=768, encoder_layers=3, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', heartbeat_timeout=-1, ignore_unused_valid_subsets=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0001], lr_scheduler='inverse_sqrt', max_epoch=0, max_source_positions=1024, max_target_positions=1024, max_tokens=3000, max_tokens_valid=3000, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_seed_provided=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, optimizer='adam', optimizer_overrides='{}', pad=1, patience=12, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='models/new_norm_lstm', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data/data_norm_bin_2000', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'tpu': False, 'lr': [0.0001]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0001]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None}\n",
            "2023-03-05 18:19:30 | INFO | fairseq.tasks.translation | [trg] dictionary: 2064 types\n",
            "2023-03-05 18:19:30 | INFO | fairseq.tasks.translation | [src] dictionary: 2064 types\n",
            "2023-03-05 18:19:31 | INFO | fairseq_cli.train | LSTMModel(\n",
            "  (encoder): LSTMEncoder(\n",
            "    (dropout_in_module): FairseqDropout()\n",
            "    (dropout_out_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(2064, 384, padding_idx=1)\n",
            "    (lstm): LSTM(384, 768, num_layers=3, dropout=0.3, bidirectional=True)\n",
            "  )\n",
            "  (decoder): LSTMDecoder(\n",
            "    (dropout_in_module): FairseqDropout()\n",
            "    (dropout_out_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(2064, 384, padding_idx=1)\n",
            "    (encoder_hidden_proj): Linear(in_features=1536, out_features=768, bias=True)\n",
            "    (encoder_cell_proj): Linear(in_features=1536, out_features=768, bias=True)\n",
            "    (layers): ModuleList(\n",
            "      (0): LSTMCell(1152, 768)\n",
            "      (1): LSTMCell(768, 768)\n",
            "      (2): LSTMCell(768, 768)\n",
            "    )\n",
            "    (attention): AttentionLayer(\n",
            "      (input_proj): Linear(in_features=768, out_features=1536, bias=False)\n",
            "      (output_proj): Linear(in_features=2304, out_features=768, bias=False)\n",
            "    )\n",
            "    (additional_fc): Linear(in_features=768, out_features=384, bias=True)\n",
            "  )\n",
            ")\n",
            "2023-03-05 18:19:31 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2023-03-05 18:19:31 | INFO | fairseq_cli.train | model: LSTMModel\n",
            "2023-03-05 18:19:31 | INFO | fairseq_cli.train | criterion: CrossEntropyCriterion\n",
            "2023-03-05 18:19:31 | INFO | fairseq_cli.train | num. shared model params: 57,177,984 (num. trained: 57,177,984)\n",
            "2023-03-05 18:19:31 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-03-05 18:19:31 | INFO | fairseq.data.data_utils | loaded 2,443 examples from: data/data_norm_bin_2000/valid.trg-src.trg\n",
            "2023-03-05 18:19:31 | INFO | fairseq.data.data_utils | loaded 2,443 examples from: data/data_norm_bin_2000/valid.trg-src.src\n",
            "2023-03-05 18:19:31 | INFO | fairseq.tasks.translation | data/data_norm_bin_2000 valid trg-src 2443 examples\n",
            "2023-03-05 18:19:33 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight\n",
            "2023-03-05 18:19:33 | INFO | fairseq.trainer | detected shared parameter: decoder.attention.input_proj.bias <- decoder.attention.output_proj.bias\n",
            "2023-03-05 18:19:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-03-05 18:19:33 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-03-05 18:19:33 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-03-05 18:19:33 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-03-05 18:19:33 | INFO | fairseq_cli.train | max tokens per device = 3000 and max sentences per device = None\n",
            "2023-03-05 18:19:33 | INFO | fairseq.trainer | Preparing to load checkpoint models/new_norm_lstm/checkpoint_last.pt\n",
            "2023-03-05 18:19:33 | INFO | fairseq.trainer | No existing checkpoint found models/new_norm_lstm/checkpoint_last.pt\n",
            "2023-03-05 18:19:33 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-03-05 18:19:33 | INFO | fairseq.data.data_utils | loaded 17,930 examples from: data/data_norm_bin_2000/train.trg-src.trg\n",
            "2023-03-05 18:19:33 | INFO | fairseq.data.data_utils | loaded 17,930 examples from: data/data_norm_bin_2000/train.trg-src.src\n",
            "2023-03-05 18:19:33 | INFO | fairseq.tasks.translation | data/data_norm_bin_2000 train trg-src 17930 examples\n",
            "2023-03-05 18:19:33 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16\n",
            "epoch 001:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:19:33 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-03-05 18:19:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.8/dist-packages/fairseq/utils.py:368: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001:  99% 167/168 [01:19<00:00,  2.60it/s, loss=11.347, ppl=2605.5, wps=5327.9, ups=2.15, wpb=2481.2, bsz=103.4, num_updates=100, lr=2.5e-06, gnorm=3.472, train_wall=47, gb_free=13.3, wall=47]2023-03-05 18:20:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 18.03it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 23.00it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 23.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 23.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 22.67it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 21.76it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 20.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 19.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 18.48it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 17.53it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 15.42it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 31/46 [00:01<00:01, 12.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  72% 33/46 [00:02<00:01, 10.71it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.94it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.52it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  7.32it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 39/46 [00:03<00:00,  7.05it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.45it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.92it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  93% 43/46 [00:03<00:00,  5.23it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  4.26it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.91it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.99it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:20:59 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 9.972 | ppl 1004.4 | wps 13944 | wpb 1415.4 | bsz 53.1 | num_updates 168\n",
            "2023-03-05 18:20:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 168 updates\n",
            "2023-03-05 18:20:59 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint1.pt\n",
            "2023-03-05 18:21:01 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint1.pt\n",
            "2023-03-05 18:21:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint1.pt (epoch 1 @ 168 updates, score 9.972) (writing took 8.406905909999978 seconds)\n",
            "2023-03-05 18:21:07 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-03-05 18:21:07 | INFO | train | epoch 001 | loss 11.045 | ppl 2113.45 | wps 4503.6 | ups 1.81 | wpb 2492.9 | bsz 106.7 | num_updates 168 | lr 4.2e-06 | gnorm 3.032 | train_wall 80 | gb_free 13.2 | wall 94\n",
            "epoch 002:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:21:07 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-03-05 18:21:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  99% 167/168 [01:23<00:00,  1.79it/s, loss=9.987, ppl=1015.07, wps=5050.7, ups=1.99, wpb=2540, bsz=108.9, num_updates=300, lr=7.5e-06, gnorm=1.693, train_wall=50, gb_free=13.3, wall=159]2023-03-05 18:22:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 16.46it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   9% 4/46 [00:00<00:02, 17.20it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  13% 6/46 [00:00<00:02, 18.10it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 9/46 [00:00<00:01, 19.69it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 19.68it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  28% 13/46 [00:00<00:01, 19.04it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 19.14it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 18.22it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  41% 19/46 [00:01<00:01, 17.43it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  46% 21/46 [00:01<00:01, 17.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 16.35it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 15.69it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 14.90it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 13.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  67% 31/46 [00:02<00:01, 11.24it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  72% 33/46 [00:02<00:01,  9.49it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.08it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.40it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  80% 37/46 [00:03<00:01,  6.70it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.58it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.37it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.24it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.87it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.78it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.92it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.65it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 46/46 [00:05<00:00,  3.75it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:22:36 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 9.29 | ppl 625.86 | wps 12761.4 | wpb 1415.4 | bsz 53.1 | num_updates 336 | best_loss 9.29\n",
            "2023-03-05 18:22:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 336 updates\n",
            "2023-03-05 18:22:36 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint2.pt\n",
            "2023-03-05 18:22:39 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint2.pt\n",
            "2023-03-05 18:22:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint2.pt (epoch 2 @ 336 updates, score 9.29) (writing took 8.214450557999953 seconds)\n",
            "2023-03-05 18:22:44 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-03-05 18:22:44 | INFO | train | epoch 002 | loss 10.005 | ppl 1027.21 | wps 4312 | ups 1.73 | wpb 2492.9 | bsz 106.7 | num_updates 336 | lr 8.4e-06 | gnorm 1.757 | train_wall 83 | gb_free 13.3 | wall 191\n",
            "epoch 003:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:22:44 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2023-03-05 18:22:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  99% 167/168 [01:25<00:00,  1.91it/s, loss=9.374, ppl=663.62, wps=4662.9, ups=1.87, wpb=2486.9, bsz=104.3, num_updates=500, lr=1.25e-05, gnorm=1.731, train_wall=53, gb_free=13.2, wall=275]2023-03-05 18:24:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 18.31it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 23.21it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 23.75it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 23.20it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 22.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 21.08it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.89it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 18.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 17.64it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 16.64it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 14.68it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  67% 31/46 [00:01<00:01, 12.27it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  72% 33/46 [00:02<00:01, 10.15it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.48it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.74it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.79it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.52it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.36it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.99it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.51it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.83it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.96it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.69it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.77it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:24:15 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 8.835 | ppl 456.53 | wps 13322.3 | wpb 1415.4 | bsz 53.1 | num_updates 504 | best_loss 8.835\n",
            "2023-03-05 18:24:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 504 updates\n",
            "2023-03-05 18:24:15 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint3.pt\n",
            "2023-03-05 18:24:18 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint3.pt\n",
            "2023-03-05 18:24:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint3.pt (epoch 3 @ 504 updates, score 8.835) (writing took 8.410713482999881 seconds)\n",
            "2023-03-05 18:24:24 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2023-03-05 18:24:24 | INFO | train | epoch 003 | loss 9.464 | ppl 706.3 | wps 4200.7 | ups 1.69 | wpb 2492.9 | bsz 106.7 | num_updates 504 | lr 1.26e-05 | gnorm 1.709 | train_wall 86 | gb_free 13.3 | wall 290\n",
            "epoch 004:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:24:24 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2023-03-05 18:24:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  99% 167/168 [01:25<00:00,  1.85it/s, loss=9.11, ppl=552.48, wps=3754.1, ups=1.51, wpb=2492.5, bsz=102.5, num_updates=600, lr=1.5e-05, gnorm=1.833, train_wall=53, gb_free=13.3, wall=341]2023-03-05 18:25:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   2% 1/46 [00:00<00:05,  8.64it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:   9% 4/46 [00:00<00:02, 15.49it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  13% 6/46 [00:00<00:02, 16.47it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  20% 9/46 [00:00<00:02, 18.29it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 18.33it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  28% 13/46 [00:00<00:01, 18.23it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 18.43it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 18.35it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  41% 19/46 [00:01<00:01, 18.30it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  46% 21/46 [00:01<00:01, 18.06it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 17.26it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 16.73it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 15.84it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 13.95it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  67% 31/46 [00:02<00:01, 11.75it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  72% 33/46 [00:02<00:01,  9.84it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.29it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.57it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.87it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.72it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.44it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.31it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.94it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.47it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.83it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.97it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.69it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 46/46 [00:05<00:00,  3.81it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:25:55 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 8.479 | ppl 356.68 | wps 12975.2 | wpb 1415.4 | bsz 53.1 | num_updates 672 | best_loss 8.479\n",
            "2023-03-05 18:25:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 672 updates\n",
            "2023-03-05 18:25:55 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint4.pt\n",
            "2023-03-05 18:25:58 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint4.pt\n",
            "2023-03-05 18:26:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint4.pt (epoch 4 @ 672 updates, score 8.479) (writing took 8.205919287999905 seconds)\n",
            "2023-03-05 18:26:04 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2023-03-05 18:26:04 | INFO | train | epoch 004 | loss 9.035 | ppl 524.72 | wps 4198.3 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 672 | lr 1.68e-05 | gnorm 1.929 | train_wall 86 | gb_free 13.3 | wall 390\n",
            "epoch 005:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:26:04 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2023-03-05 18:26:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  99% 167/168 [01:26<00:00,  1.68it/s, loss=8.743, ppl=428.53, wps=4952.8, ups=1.95, wpb=2542.4, bsz=106.4, num_updates=800, lr=2e-05, gnorm=2.243, train_wall=51, gb_free=13.5, wall=456]2023-03-05 18:27:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:   7% 3/46 [00:00<00:01, 24.12it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  13% 6/46 [00:00<00:01, 24.50it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  20% 9/46 [00:00<00:01, 24.41it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  26% 12/46 [00:00<00:01, 22.79it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 21.88it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  39% 18/46 [00:00<00:01, 20.88it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  46% 21/46 [00:01<00:01, 19.04it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 18.29it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 17.52it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 16.58it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 14.66it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  67% 31/46 [00:01<00:01, 12.23it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  72% 33/46 [00:02<00:01, 10.09it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.47it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.70it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.96it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.81it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.55it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.37it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.01it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.53it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  93% 43/46 [00:03<00:00,  4.86it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  4.00it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.71it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.74it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:27:35 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 8.294 | ppl 313.93 | wps 13363.1 | wpb 1415.4 | bsz 53.1 | num_updates 840 | best_loss 8.294\n",
            "2023-03-05 18:27:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 840 updates\n",
            "2023-03-05 18:27:35 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint5.pt\n",
            "2023-03-05 18:27:38 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint5.pt\n",
            "2023-03-05 18:27:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint5.pt (epoch 5 @ 840 updates, score 8.294) (writing took 8.43655700699992 seconds)\n",
            "2023-03-05 18:27:44 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2023-03-05 18:27:44 | INFO | train | epoch 005 | loss 8.719 | ppl 421.44 | wps 4178.1 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 840 | lr 2.1e-05 | gnorm 2.184 | train_wall 86 | gb_free 13.3 | wall 490\n",
            "epoch 006:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:27:44 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2023-03-05 18:27:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  99% 167/168 [01:26<00:00,  2.00it/s, loss=8.411, ppl=340.4, wps=5044.9, ups=2.02, wpb=2494.9, bsz=116.3, num_updates=1000, lr=2.5e-05, gnorm=2.161, train_wall=49, gb_free=13.3, wall=573]2023-03-05 18:29:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 18.11it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 23.74it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 23.52it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 23.03it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.89it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.97it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.61it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 17.99it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.60it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.89it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.64it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.32it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.88it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.20it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.76it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.08it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.88it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.62it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.45it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.06it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.56it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.90it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  4.02it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.73it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.85it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:29:15 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 8.035 | ppl 262.27 | wps 13365.6 | wpb 1415.4 | bsz 53.1 | num_updates 1008 | best_loss 8.035\n",
            "2023-03-05 18:29:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 1008 updates\n",
            "2023-03-05 18:29:15 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint6.pt\n",
            "2023-03-05 18:29:18 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint6.pt\n",
            "2023-03-05 18:29:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint6.pt (epoch 6 @ 1008 updates, score 8.035) (writing took 8.22944440900028 seconds)\n",
            "2023-03-05 18:29:24 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2023-03-05 18:29:24 | INFO | train | epoch 006 | loss 8.467 | ppl 353.97 | wps 4197.5 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 1008 | lr 2.52e-05 | gnorm 2.123 | train_wall 86 | gb_free 13.2 | wall 590\n",
            "epoch 007:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:29:24 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2023-03-05 18:29:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007:  99% 167/168 [01:26<00:00,  1.99it/s, loss=8.25, ppl=304.54, wps=3784.1, ups=1.51, wpb=2500.9, bsz=109.1, num_updates=1100, lr=2.75e-05, gnorm=2.226, train_wall=52, gb_free=13.3, wall=639]2023-03-05 18:30:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 17.58it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 23.12it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 22.90it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 22.74it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.67it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.61it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.36it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.19it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.71it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.96it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.63it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.34it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.76it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.12it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.73it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.04it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.84it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.58it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.41it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.04it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.56it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.90it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  4.01it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.61it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.73it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:30:55 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 7.828 | ppl 227.16 | wps 13218.7 | wpb 1415.4 | bsz 53.1 | num_updates 1176 | best_loss 7.828\n",
            "2023-03-05 18:30:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 1176 updates\n",
            "2023-03-05 18:30:55 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint7.pt\n",
            "2023-03-05 18:30:58 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint7.pt\n",
            "2023-03-05 18:31:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint7.pt (epoch 7 @ 1176 updates, score 7.828) (writing took 8.142822541999976 seconds)\n",
            "2023-03-05 18:31:03 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2023-03-05 18:31:03 | INFO | train | epoch 007 | loss 8.212 | ppl 296.52 | wps 4203.4 | ups 1.69 | wpb 2492.9 | bsz 106.7 | num_updates 1176 | lr 2.94e-05 | gnorm 2.245 | train_wall 86 | gb_free 13.3 | wall 690\n",
            "epoch 008:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:31:03 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2023-03-05 18:31:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008:  99% 167/168 [01:25<00:00,  1.27it/s, loss=8.013, ppl=258.31, wps=4919.6, ups=2, wpb=2457.3, bsz=103.5, num_updates=1300, lr=3.25e-05, gnorm=2.396, train_wall=50, gb_free=13.4, wall=753]2023-03-05 18:32:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 19.29it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 24.51it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 24.27it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 23.47it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 22.22it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 21.07it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.93it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 18.29it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 17.54it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 16.49it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 14.58it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  67% 31/46 [00:01<00:01, 12.22it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  72% 33/46 [00:02<00:01, 10.14it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.49it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.74it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.98it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.83it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.56it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.41it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.01it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.52it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  93% 43/46 [00:03<00:00,  4.87it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.99it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.71it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.83it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:32:35 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 7.555 | ppl 188.01 | wps 13391.2 | wpb 1415.4 | bsz 53.1 | num_updates 1344 | best_loss 7.555\n",
            "2023-03-05 18:32:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1344 updates\n",
            "2023-03-05 18:32:35 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint8.pt\n",
            "2023-03-05 18:32:37 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint8.pt\n",
            "2023-03-05 18:32:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint8.pt (epoch 8 @ 1344 updates, score 7.555) (writing took 8.226379176000137 seconds)\n",
            "2023-03-05 18:32:43 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2023-03-05 18:32:43 | INFO | train | epoch 008 | loss 7.973 | ppl 251.25 | wps 4197 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 1344 | lr 3.36e-05 | gnorm 2.486 | train_wall 86 | gb_free 13.3 | wall 790\n",
            "epoch 009:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:32:43 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2023-03-05 18:32:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009:  99% 167/168 [01:26<00:00,  1.85it/s, loss=7.743, ppl=214.26, wps=4704.9, ups=1.89, wpb=2493.7, bsz=105.2, num_updates=1500, lr=3.75e-05, gnorm=2.69, train_wall=53, gb_free=13.2, wall=870]2023-03-05 18:34:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   2% 1/46 [00:00<00:05,  7.66it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:   9% 4/46 [00:00<00:02, 16.42it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  15% 7/46 [00:00<00:01, 19.66it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  22% 10/46 [00:00<00:01, 19.65it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  26% 12/46 [00:00<00:01, 19.67it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 18.92it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  35% 16/46 [00:00<00:01, 18.54it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  39% 18/46 [00:00<00:01, 18.46it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  43% 20/46 [00:01<00:01, 17.58it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 16.69it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 16.24it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 15.57it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 14.51it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 12.59it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  70% 32/46 [00:02<00:01, 10.32it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  8.83it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.09it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.35it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  80% 37/46 [00:03<00:01,  6.62it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.50it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.33it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.25it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.90it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.44it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.78it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.95it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.68it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 46/46 [00:05<00:00,  3.80it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:34:15 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 7.328 | ppl 160.65 | wps 12961.4 | wpb 1415.4 | bsz 53.1 | num_updates 1512 | best_loss 7.328\n",
            "2023-03-05 18:34:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1512 updates\n",
            "2023-03-05 18:34:15 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint9.pt\n",
            "2023-03-05 18:34:18 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint9.pt\n",
            "2023-03-05 18:34:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint9.pt (epoch 9 @ 1512 updates, score 7.328) (writing took 8.154857300000003 seconds)\n",
            "2023-03-05 18:34:23 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2023-03-05 18:34:23 | INFO | train | epoch 009 | loss 7.763 | ppl 217.29 | wps 4184.3 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 1512 | lr 3.78e-05 | gnorm 2.796 | train_wall 86 | gb_free 13.3 | wall 890\n",
            "epoch 010:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:34:23 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2023-03-05 18:34:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010:  99% 167/168 [01:25<00:00,  2.17it/s, loss=7.569, ppl=189.94, wps=3756.6, ups=1.52, wpb=2474.1, bsz=112.1, num_updates=1600, lr=4e-05, gnorm=2.716, train_wall=52, gb_free=13.4, wall=936]2023-03-05 18:35:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:   7% 3/46 [00:00<00:01, 24.89it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  13% 6/46 [00:00<00:01, 24.30it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  20% 9/46 [00:00<00:01, 24.15it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  26% 12/46 [00:00<00:01, 22.95it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 21.80it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  39% 18/46 [00:00<00:01, 20.78it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  46% 21/46 [00:01<00:01, 19.00it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 18.27it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 17.51it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 16.51it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 14.54it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  67% 31/46 [00:01<00:01, 12.15it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  72% 33/46 [00:02<00:01, 10.10it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.46it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.70it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.80it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.52it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.37it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.00it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.53it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  93% 43/46 [00:03<00:00,  4.88it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  4.00it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.71it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.79it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:35:55 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 7.078 | ppl 135.13 | wps 13376.6 | wpb 1415.4 | bsz 53.1 | num_updates 1680 | best_loss 7.078\n",
            "2023-03-05 18:35:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1680 updates\n",
            "2023-03-05 18:35:55 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint10.pt\n",
            "2023-03-05 18:35:57 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint10.pt\n",
            "2023-03-05 18:36:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint10.pt (epoch 10 @ 1680 updates, score 7.078) (writing took 8.316915158000029 seconds)\n",
            "2023-03-05 18:36:03 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2023-03-05 18:36:03 | INFO | train | epoch 010 | loss 7.533 | ppl 185.17 | wps 4188.9 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 1680 | lr 4.2e-05 | gnorm 2.76 | train_wall 86 | gb_free 13.4 | wall 990\n",
            "epoch 011:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:36:03 | INFO | fairseq.trainer | begin training epoch 11\n",
            "2023-03-05 18:36:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 011:  99% 167/168 [01:26<00:00,  1.75it/s, loss=7.397, ppl=168.55, wps=4579.1, ups=1.83, wpb=2500.4, bsz=100.3, num_updates=1800, lr=4.5e-05, gnorm=2.787, train_wall=54, gb_free=13.3, wall=1054]2023-03-05 18:37:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   2% 1/46 [00:00<00:04,  9.67it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:   9% 4/46 [00:00<00:02, 20.26it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  13% 6/46 [00:00<00:02, 18.18it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  20% 9/46 [00:00<00:01, 19.67it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  26% 12/46 [00:00<00:01, 19.72it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 19.41it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  35% 16/46 [00:00<00:01, 19.00it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  41% 19/46 [00:01<00:01, 18.93it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  46% 21/46 [00:01<00:01, 18.71it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 17.86it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 17.02it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 16.10it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 14.19it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  67% 31/46 [00:01<00:01, 11.95it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  72% 33/46 [00:02<00:01,  9.99it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.37it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.62it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.89it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.76it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.49it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.34it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.96it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.49it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.86it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.99it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.70it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset: 100% 46/46 [00:05<00:00,  3.81it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:37:35 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 6.867 | ppl 116.72 | wps 13147.6 | wpb 1415.4 | bsz 53.1 | num_updates 1848 | best_loss 6.867\n",
            "2023-03-05 18:37:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1848 updates\n",
            "2023-03-05 18:37:35 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint11.pt\n",
            "2023-03-05 18:37:38 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint11.pt\n",
            "2023-03-05 18:37:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint11.pt (epoch 11 @ 1848 updates, score 6.867) (writing took 8.118088566000097 seconds)\n",
            "2023-03-05 18:37:43 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2023-03-05 18:37:43 | INFO | train | epoch 011 | loss 7.295 | ppl 157.04 | wps 4188.8 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 1848 | lr 4.62e-05 | gnorm 2.765 | train_wall 86 | gb_free 13.3 | wall 1090\n",
            "epoch 012:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:37:43 | INFO | fairseq.trainer | begin training epoch 12\n",
            "2023-03-05 18:37:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 012:  99% 167/168 [01:25<00:00,  1.85it/s, loss=6.968, ppl=125.2, wps=5007.2, ups=2, wpb=2504.9, bsz=113.5, num_updates=2000, lr=5e-05, gnorm=2.996, train_wall=50, gb_free=13.3, wall=1166]2023-03-05 18:39:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 18.54it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  13% 6/46 [00:00<00:01, 24.60it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  20% 9/46 [00:00<00:01, 24.68it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  26% 12/46 [00:00<00:01, 23.26it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 21.95it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  39% 18/46 [00:00<00:01, 20.88it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  46% 21/46 [00:01<00:01, 18.97it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 18.23it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 17.53it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 16.56it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 14.60it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  67% 31/46 [00:01<00:01, 12.17it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  72% 33/46 [00:02<00:01, 10.10it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.48it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.73it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.00it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.79it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.50it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.36it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.98it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.49it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  93% 43/46 [00:03<00:00,  4.84it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.98it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.70it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.74it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:39:14 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 6.771 | ppl 109.18 | wps 13315.9 | wpb 1415.4 | bsz 53.1 | num_updates 2016 | best_loss 6.771\n",
            "2023-03-05 18:39:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 2016 updates\n",
            "2023-03-05 18:39:14 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint12.pt\n",
            "2023-03-05 18:39:17 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint12.pt\n",
            "2023-03-05 18:39:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint12.pt (epoch 12 @ 2016 updates, score 6.771) (writing took 8.394594465999944 seconds)\n",
            "2023-03-05 18:39:23 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2023-03-05 18:39:23 | INFO | train | epoch 012 | loss 7.05 | ppl 132.53 | wps 4201.2 | ups 1.69 | wpb 2492.9 | bsz 106.7 | num_updates 2016 | lr 5.04e-05 | gnorm 2.918 | train_wall 86 | gb_free 13.3 | wall 1189\n",
            "epoch 013:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:39:23 | INFO | fairseq.trainer | begin training epoch 13\n",
            "2023-03-05 18:39:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 013:  99% 167/168 [01:25<00:00,  1.98it/s, loss=6.909, ppl=120.2, wps=3724.3, ups=1.48, wpb=2509.9, bsz=105.1, num_updates=2100, lr=5.25e-05, gnorm=3.072, train_wall=54, gb_free=13.4, wall=1233]2023-03-05 18:40:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 19.11it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 23.63it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 24.07it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 23.45it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 22.24it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.81it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.70it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.33it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.73it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.98it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.63it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.34it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.92it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.27it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.78it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.09it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.89it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.62it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.46it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.06it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.56it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  93% 43/46 [00:03<00:00,  4.89it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  4.02it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.73it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.84it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:40:54 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 6.285 | ppl 78 | wps 13353.5 | wpb 1415.4 | bsz 53.1 | num_updates 2184 | best_loss 6.285\n",
            "2023-03-05 18:40:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 2184 updates\n",
            "2023-03-05 18:40:54 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint13.pt\n",
            "2023-03-05 18:40:57 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint13.pt\n",
            "2023-03-05 18:41:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint13.pt (epoch 13 @ 2184 updates, score 6.285) (writing took 8.349765895000019 seconds)\n",
            "2023-03-05 18:41:02 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2023-03-05 18:41:02 | INFO | train | epoch 013 | loss 6.825 | ppl 113.42 | wps 4206.7 | ups 1.69 | wpb 2492.9 | bsz 106.7 | num_updates 2184 | lr 5.46e-05 | gnorm 2.97 | train_wall 86 | gb_free 13.3 | wall 1289\n",
            "epoch 014:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:41:02 | INFO | fairseq.trainer | begin training epoch 14\n",
            "2023-03-05 18:41:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 014:  99% 167/168 [01:25<00:00,  1.76it/s, loss=6.606, ppl=97.42, wps=4714.7, ups=1.9, wpb=2487.2, bsz=104.1, num_updates=2300, lr=5.75e-05, gnorm=3.07, train_wall=52, gb_free=13.3, wall=1350]2023-03-05 18:42:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 18.95it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 24.45it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 22.47it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 22.50it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.81it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.74it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.53it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.14it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.28it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.40it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.01it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 12.91it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  70% 32/46 [00:02<00:01, 10.65it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.02it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.68it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.01it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.83it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.58it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.41it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.05it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.57it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.90it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  4.00it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.72it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.83it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:42:34 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 6.074 | ppl 67.36 | wps 13232.2 | wpb 1415.4 | bsz 53.1 | num_updates 2352 | best_loss 6.074\n",
            "2023-03-05 18:42:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 2352 updates\n",
            "2023-03-05 18:42:34 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint14.pt\n",
            "2023-03-05 18:42:36 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint14.pt\n",
            "2023-03-05 18:42:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint14.pt (epoch 14 @ 2352 updates, score 6.074) (writing took 8.218932648999726 seconds)\n",
            "2023-03-05 18:42:42 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2023-03-05 18:42:42 | INFO | train | epoch 014 | loss 6.599 | ppl 96.97 | wps 4196.1 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 2352 | lr 5.88e-05 | gnorm 3.083 | train_wall 86 | gb_free 13.2 | wall 1389\n",
            "epoch 015:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:42:42 | INFO | fairseq.trainer | begin training epoch 15\n",
            "2023-03-05 18:42:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 015:  99% 167/168 [01:25<00:00,  1.95it/s, loss=6.243, ppl=75.76, wps=4965.8, ups=1.99, wpb=2490, bsz=120.3, num_updates=2500, lr=6.25e-05, gnorm=2.938, train_wall=50, gb_free=13.3, wall=1463]2023-03-05 18:44:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 18.59it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 23.01it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 23.48it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 23.01it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.96it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.99it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.77it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 18.14it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 17.47it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 16.54it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 14.63it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  67% 31/46 [00:01<00:01, 12.25it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  72% 33/46 [00:02<00:01, 10.19it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.51it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.74it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.97it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.80it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.54it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.40it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.02it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.51it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.85it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.98it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.71it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.81it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:44:13 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 5.754 | ppl 53.97 | wps 13340 | wpb 1415.4 | bsz 53.1 | num_updates 2520 | best_loss 5.754\n",
            "2023-03-05 18:44:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 2520 updates\n",
            "2023-03-05 18:44:13 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint15.pt\n",
            "2023-03-05 18:44:16 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint15.pt\n",
            "2023-03-05 18:44:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint15.pt (epoch 15 @ 2520 updates, score 5.754) (writing took 8.095334399999956 seconds)\n",
            "2023-03-05 18:44:21 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2023-03-05 18:44:21 | INFO | train | epoch 015 | loss 6.343 | ppl 81.17 | wps 4213.4 | ups 1.69 | wpb 2492.9 | bsz 106.7 | num_updates 2520 | lr 6.3e-05 | gnorm 3.001 | train_wall 86 | gb_free 13.3 | wall 1488\n",
            "epoch 016:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:44:22 | INFO | fairseq.trainer | begin training epoch 16\n",
            "2023-03-05 18:44:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 016:  99% 167/168 [01:26<00:00,  1.78it/s, loss=6.234, ppl=75.27, wps=3606.2, ups=1.47, wpb=2450.4, bsz=93.7, num_updates=2600, lr=6.5e-05, gnorm=3.006, train_wall=54, gb_free=13.2, wall=1531]2023-03-05 18:45:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:   2% 1/46 [00:00<00:05,  8.50it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:   9% 4/46 [00:00<00:02, 18.37it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  15% 7/46 [00:00<00:01, 20.05it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  22% 10/46 [00:00<00:01, 20.20it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  28% 13/46 [00:00<00:01, 19.88it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 19.59it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 18.94it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  41% 19/46 [00:01<00:01, 18.27it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  46% 21/46 [00:01<00:01, 17.84it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 17.17it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 16.38it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 15.34it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 13.63it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  67% 31/46 [00:01<00:01, 11.54it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  72% 33/46 [00:02<00:01,  9.74it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.24it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.51it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.82it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.69it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.45it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.31it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.93it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.45it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.82it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.97it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.69it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset: 100% 46/46 [00:05<00:00,  3.82it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:45:53 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 5.385 | ppl 41.78 | wps 13066.4 | wpb 1415.4 | bsz 53.1 | num_updates 2688 | best_loss 5.385\n",
            "2023-03-05 18:45:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 2688 updates\n",
            "2023-03-05 18:45:53 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint16.pt\n",
            "2023-03-05 18:45:56 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint16.pt\n",
            "2023-03-05 18:46:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint16.pt (epoch 16 @ 2688 updates, score 5.385) (writing took 8.179893386000003 seconds)\n",
            "2023-03-05 18:46:02 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2023-03-05 18:46:02 | INFO | train | epoch 016 | loss 6.054 | ppl 66.43 | wps 4180.6 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 2688 | lr 6.72e-05 | gnorm 3.04 | train_wall 86 | gb_free 13.3 | wall 1588\n",
            "epoch 017:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:46:02 | INFO | fairseq.trainer | begin training epoch 17\n",
            "2023-03-05 18:46:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 017:  99% 167/168 [01:26<00:00,  2.16it/s, loss=5.857, ppl=57.96, wps=4481.7, ups=1.83, wpb=2442.7, bsz=96.7, num_updates=2800, lr=7e-05, gnorm=3.28, train_wall=54, gb_free=13.3, wall=1648]2023-03-05 18:47:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 18.44it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 23.28it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 23.26it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 22.76it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.69it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.82it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.60it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.35it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.80it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 17.09it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.68it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.27it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.85it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.19it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.76it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.08it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.90it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.62it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.44it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.06it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.57it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.91it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  4.01it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.71it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.70it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:47:33 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 4.971 | ppl 31.37 | wps 13263.3 | wpb 1415.4 | bsz 53.1 | num_updates 2856 | best_loss 4.971\n",
            "2023-03-05 18:47:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 2856 updates\n",
            "2023-03-05 18:47:33 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint17.pt\n",
            "2023-03-05 18:47:36 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint17.pt\n",
            "2023-03-05 18:47:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint17.pt (epoch 17 @ 2856 updates, score 4.971) (writing took 8.399225886000295 seconds)\n",
            "2023-03-05 18:47:42 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2023-03-05 18:47:42 | INFO | train | epoch 017 | loss 5.729 | ppl 53.03 | wps 4186.8 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 2856 | lr 7.14e-05 | gnorm 3.118 | train_wall 86 | gb_free 13.2 | wall 1688\n",
            "epoch 018:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:47:42 | INFO | fairseq.trainer | begin training epoch 18\n",
            "2023-03-05 18:47:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 018:  99% 167/168 [01:26<00:00,  2.15it/s, loss=5.369, ppl=41.32, wps=4797, ups=1.89, wpb=2533.5, bsz=103, num_updates=3000, lr=7.5e-05, gnorm=2.957, train_wall=52, gb_free=13.3, wall=1764]2023-03-05 18:49:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 16.45it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 22.49it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 22.95it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 22.78it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.76it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.89it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.74it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.24it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.72it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.92it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.53it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.22it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.87it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.21it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.77it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.09it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.91it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.64it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.47it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.08it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.58it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.90it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  4.03it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.73it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.83it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:49:13 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 4.673 | ppl 25.52 | wps 13367 | wpb 1415.4 | bsz 53.1 | num_updates 3024 | best_loss 4.673\n",
            "2023-03-05 18:49:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 3024 updates\n",
            "2023-03-05 18:49:13 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint18.pt\n",
            "2023-03-05 18:49:16 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint18.pt\n",
            "2023-03-05 18:49:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint18.pt (epoch 18 @ 3024 updates, score 4.673) (writing took 8.274007593000078 seconds)\n",
            "2023-03-05 18:49:21 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2023-03-05 18:49:21 | INFO | train | epoch 018 | loss 5.385 | ppl 41.78 | wps 4201.5 | ups 1.69 | wpb 2492.9 | bsz 106.7 | num_updates 3024 | lr 7.56e-05 | gnorm 3.033 | train_wall 86 | gb_free 13.3 | wall 1788\n",
            "epoch 019:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:49:21 | INFO | fairseq.trainer | begin training epoch 19\n",
            "2023-03-05 18:49:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 019:  99% 167/168 [01:25<00:00,  1.83it/s, loss=5.098, ppl=34.25, wps=3858.9, ups=1.56, wpb=2480, bsz=111.9, num_updates=3100, lr=7.75e-05, gnorm=2.88, train_wall=51, gb_free=13.2, wall=1828]2023-03-05 18:50:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:   7% 3/46 [00:00<00:01, 22.62it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  13% 6/46 [00:00<00:01, 23.61it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  20% 9/46 [00:00<00:01, 23.40it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  26% 12/46 [00:00<00:01, 22.60it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 21.58it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  39% 18/46 [00:00<00:01, 20.67it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  46% 21/46 [00:01<00:01, 18.91it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 18.16it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 17.47it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 16.53it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 14.64it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  67% 31/46 [00:01<00:01, 12.14it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  72% 33/46 [00:02<00:01,  9.97it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.41it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.69it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.97it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.78it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.52it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.37it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.00it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.52it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.86it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  4.00it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.71it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.81it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:50:53 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 4.284 | ppl 19.48 | wps 13332.7 | wpb 1415.4 | bsz 53.1 | num_updates 3192 | best_loss 4.284\n",
            "2023-03-05 18:50:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 3192 updates\n",
            "2023-03-05 18:50:53 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint19.pt\n",
            "2023-03-05 18:50:55 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint19.pt\n",
            "2023-03-05 18:51:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint19.pt (epoch 19 @ 3192 updates, score 4.284) (writing took 8.14554249799994 seconds)\n",
            "2023-03-05 18:51:01 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2023-03-05 18:51:01 | INFO | train | epoch 019 | loss 5.065 | ppl 33.47 | wps 4210.8 | ups 1.69 | wpb 2492.9 | bsz 106.7 | num_updates 3192 | lr 7.98e-05 | gnorm 3.188 | train_wall 86 | gb_free 13.3 | wall 1887\n",
            "epoch 020:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:51:01 | INFO | fairseq.trainer | begin training epoch 20\n",
            "2023-03-05 18:51:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 020:  99% 167/168 [01:26<00:00,  2.32it/s, loss=4.809, ppl=28.04, wps=4699.9, ups=1.91, wpb=2466.5, bsz=106.5, num_updates=3300, lr=8.25e-05, gnorm=3.177, train_wall=52, gb_free=13.2, wall=1944]2023-03-05 18:52:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:   7% 3/46 [00:00<00:01, 22.77it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  13% 6/46 [00:00<00:01, 23.53it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  20% 9/46 [00:00<00:01, 23.92it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  26% 12/46 [00:00<00:01, 22.68it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 21.83it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  39% 18/46 [00:00<00:01, 20.83it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  46% 21/46 [00:01<00:01, 18.95it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 18.25it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 17.54it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 16.58it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 14.59it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  67% 31/46 [00:01<00:01, 12.17it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  72% 33/46 [00:02<00:01, 10.09it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.47it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.72it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.98it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.80it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.53it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.37it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.01it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.54it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  93% 43/46 [00:03<00:00,  4.86it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.98it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.70it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.81it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:52:32 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 3.94 | ppl 15.35 | wps 13360.3 | wpb 1415.4 | bsz 53.1 | num_updates 3360 | best_loss 3.94\n",
            "2023-03-05 18:52:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 3360 updates\n",
            "2023-03-05 18:52:32 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint20.pt\n",
            "2023-03-05 18:52:35 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint20.pt\n",
            "2023-03-05 18:52:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint20.pt (epoch 20 @ 3360 updates, score 3.94) (writing took 8.44254498700002 seconds)\n",
            "2023-03-05 18:52:41 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2023-03-05 18:52:41 | INFO | train | epoch 020 | loss 4.734 | ppl 26.61 | wps 4186.7 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 3360 | lr 8.4e-05 | gnorm 3.074 | train_wall 86 | gb_free 13.2 | wall 1987\n",
            "epoch 021:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:52:41 | INFO | fairseq.trainer | begin training epoch 21\n",
            "2023-03-05 18:52:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 021:  99% 167/168 [01:25<00:00,  2.12it/s, loss=4.317, ppl=19.94, wps=4990.2, ups=1.98, wpb=2526.2, bsz=110.6, num_updates=3500, lr=8.75e-05, gnorm=2.9, train_wall=50, gb_free=13.3, wall=2061]2023-03-05 18:54:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:   2% 1/46 [00:00<00:04,  9.23it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:   9% 4/46 [00:00<00:02, 19.69it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  15% 7/46 [00:00<00:01, 20.84it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  22% 10/46 [00:00<00:01, 20.82it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  28% 13/46 [00:00<00:01, 20.13it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  35% 16/46 [00:00<00:01, 19.33it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  41% 19/46 [00:00<00:01, 18.90it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  46% 21/46 [00:01<00:01, 18.06it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 17.23it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 16.62it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 15.94it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 14.19it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  67% 31/46 [00:01<00:01, 11.96it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  72% 33/46 [00:02<00:01,  9.94it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.38it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.64it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.91it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.76it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.49it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.34it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.99it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.53it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.86it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.98it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.70it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset: 100% 46/46 [00:05<00:00,  3.81it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:54:12 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 3.5 | ppl 11.31 | wps 13152.7 | wpb 1415.4 | bsz 53.1 | num_updates 3528 | best_loss 3.5\n",
            "2023-03-05 18:54:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 3528 updates\n",
            "2023-03-05 18:54:12 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint21.pt\n",
            "2023-03-05 18:54:15 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint21.pt\n",
            "2023-03-05 18:54:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint21.pt (epoch 21 @ 3528 updates, score 3.5) (writing took 8.237617537999995 seconds)\n",
            "2023-03-05 18:54:21 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2023-03-05 18:54:21 | INFO | train | epoch 021 | loss 4.395 | ppl 21.03 | wps 4198.5 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 3528 | lr 8.82e-05 | gnorm 2.823 | train_wall 86 | gb_free 13.3 | wall 2087\n",
            "epoch 022:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:54:21 | INFO | fairseq.trainer | begin training epoch 22\n",
            "2023-03-05 18:54:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 022:  99% 167/168 [01:25<00:00,  2.26it/s, loss=4.317, ppl=19.93, wps=3879.3, ups=1.57, wpb=2474.4, bsz=100.8, num_updates=3600, lr=9e-05, gnorm=2.851, train_wall=50, gb_free=13.3, wall=2125]2023-03-05 18:55:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 16.77it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 21.91it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 22.44it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 22.47it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.67it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.73it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.77it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.39it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.84it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 17.12it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.73it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.30it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.87it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.17it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.76it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.08it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.88it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.59it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.43it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.04it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.55it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.87it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.98it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.70it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.75it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:55:52 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 3.104 | ppl 8.6 | wps 13253.7 | wpb 1415.4 | bsz 53.1 | num_updates 3696 | best_loss 3.104\n",
            "2023-03-05 18:55:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 3696 updates\n",
            "2023-03-05 18:55:52 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint22.pt\n",
            "2023-03-05 18:55:55 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint22.pt\n",
            "2023-03-05 18:56:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint22.pt (epoch 22 @ 3696 updates, score 3.104) (writing took 8.489036982000016 seconds)\n",
            "2023-03-05 18:56:00 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2023-03-05 18:56:00 | INFO | train | epoch 022 | loss 4.068 | ppl 16.77 | wps 4193.9 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 3696 | lr 9.24e-05 | gnorm 2.947 | train_wall 86 | gb_free 13.3 | wall 2187\n",
            "epoch 023:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:56:01 | INFO | fairseq.trainer | begin training epoch 23\n",
            "2023-03-05 18:56:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 023:  99% 167/168 [01:25<00:00,  1.71it/s, loss=3.662, ppl=12.65, wps=4902.5, ups=1.91, wpb=2564.4, bsz=110.6, num_updates=3800, lr=9.5e-05, gnorm=2.557, train_wall=52, gb_free=13.2, wall=2242]2023-03-05 18:57:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:   2% 1/46 [00:00<00:05,  8.70it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:   9% 4/46 [00:00<00:02, 16.50it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  15% 7/46 [00:00<00:02, 19.04it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  22% 10/46 [00:00<00:01, 19.90it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  28% 13/46 [00:00<00:01, 20.32it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  35% 16/46 [00:00<00:01, 19.85it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  41% 19/46 [00:01<00:01, 19.28it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  46% 21/46 [00:01<00:01, 18.53it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 17.78it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 17.09it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 16.19it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 14.45it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  67% 31/46 [00:01<00:01, 12.01it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  72% 33/46 [00:02<00:01, 10.02it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.36it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.61it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.87it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.75it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.51it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.35it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.98it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.49it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.83it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.97it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.69it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset: 100% 46/46 [00:05<00:00,  3.81it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:57:32 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 2.671 | ppl 6.37 | wps 13155.8 | wpb 1415.4 | bsz 53.1 | num_updates 3864 | best_loss 2.671\n",
            "2023-03-05 18:57:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 3864 updates\n",
            "2023-03-05 18:57:32 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint23.pt\n",
            "2023-03-05 18:57:35 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint23.pt\n",
            "2023-03-05 18:57:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint23.pt (epoch 23 @ 3864 updates, score 2.671) (writing took 8.417461487999844 seconds)\n",
            "2023-03-05 18:57:41 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2023-03-05 18:57:41 | INFO | train | epoch 023 | loss 3.67 | ppl 12.73 | wps 4182.7 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 3864 | lr 9.66e-05 | gnorm 2.748 | train_wall 86 | gb_free 13.5 | wall 2287\n",
            "epoch 024:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:57:41 | INFO | fairseq.trainer | begin training epoch 24\n",
            "2023-03-05 18:57:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 024:  99% 167/168 [01:25<00:00,  1.86it/s, loss=3.322, ppl=10, wps=5050.5, ups=2, wpb=2519.9, bsz=113.6, num_updates=4000, lr=0.0001, gnorm=2.789, train_wall=49, gb_free=13.3, wall=2356]2023-03-05 18:59:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 19.59it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 24.72it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 24.04it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 23.21it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 22.06it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.89it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.60it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.38it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.79it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.92it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.48it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.17it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.80it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.18it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.74it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.05it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.85it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.60it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.44it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.05it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.54it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.85it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  4.00it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.72it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.64it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 18:59:12 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 2.3 | ppl 4.92 | wps 13205.2 | wpb 1415.4 | bsz 53.1 | num_updates 4032 | best_loss 2.3\n",
            "2023-03-05 18:59:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 4032 updates\n",
            "2023-03-05 18:59:12 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint24.pt\n",
            "2023-03-05 18:59:15 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint24.pt\n",
            "2023-03-05 18:59:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint24.pt (epoch 24 @ 4032 updates, score 2.3) (writing took 8.545903450999958 seconds)\n",
            "2023-03-05 18:59:21 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2023-03-05 18:59:21 | INFO | train | epoch 024 | loss 3.31 | ppl 9.92 | wps 4190 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 4032 | lr 9.96024e-05 | gnorm 2.719 | train_wall 86 | gb_free 13.3 | wall 2387\n",
            "epoch 025:   0% 0/168 [00:00<?, ?it/s]2023-03-05 18:59:21 | INFO | fairseq.trainer | begin training epoch 25\n",
            "2023-03-05 18:59:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 025:  99% 167/168 [01:25<00:00,  2.08it/s, loss=3.173, ppl=9.02, wps=3722.2, ups=1.53, wpb=2431.5, bsz=93.6, num_updates=4100, lr=9.8773e-05, gnorm=2.388, train_wall=51, gb_free=13.3, wall=2421]2023-03-05 19:00:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 025 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 19.52it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 24.45it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 24.13it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 23.35it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 22.17it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 21.14it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.79it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 18.11it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 17.43it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 16.57it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 14.64it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  67% 31/46 [00:01<00:01, 12.26it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  72% 33/46 [00:02<00:01, 10.13it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.49it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.75it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.97it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.80it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.54it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.39it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.02it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.53it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.85it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.97it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.70it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.80it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:00:52 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 2.041 | ppl 4.11 | wps 13338.6 | wpb 1415.4 | bsz 53.1 | num_updates 4200 | best_loss 2.041\n",
            "2023-03-05 19:00:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 4200 updates\n",
            "2023-03-05 19:00:52 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint25.pt\n",
            "2023-03-05 19:00:54 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint25.pt\n",
            "2023-03-05 19:01:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint25.pt (epoch 25 @ 4200 updates, score 2.041) (writing took 8.310307587999887 seconds)\n",
            "2023-03-05 19:01:00 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
            "2023-03-05 19:01:00 | INFO | train | epoch 025 | loss 2.98 | ppl 7.89 | wps 4209.5 | ups 1.69 | wpb 2492.9 | bsz 106.7 | num_updates 4200 | lr 9.759e-05 | gnorm 2.893 | train_wall 86 | gb_free 13.3 | wall 2487\n",
            "epoch 026:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:01:00 | INFO | fairseq.trainer | begin training epoch 26\n",
            "2023-03-05 19:01:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 026:  99% 167/168 [01:25<00:00,  2.48it/s, loss=2.687, ppl=6.44, wps=3701.7, ups=1.49, wpb=2492.5, bsz=96.6, num_updates=4300, lr=9.64486e-05, gnorm=2.275, train_wall=54, gb_free=13.2, wall=2541]2023-03-05 19:02:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 026 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 17.85it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 24.05it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 23.30it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 22.76it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.70it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.73it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.51it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.18it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.66it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.92it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.62it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.28it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.82it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.13it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.71it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.02it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.86it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.59it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.40it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.99it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.51it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.85it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.99it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.70it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.71it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:02:31 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 1.718 | ppl 3.29 | wps 13214.3 | wpb 1415.4 | bsz 53.1 | num_updates 4368 | best_loss 1.718\n",
            "2023-03-05 19:02:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 4368 updates\n",
            "2023-03-05 19:02:31 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint26.pt\n",
            "2023-03-05 19:02:34 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint26.pt\n",
            "2023-03-05 19:02:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint26.pt (epoch 26 @ 4368 updates, score 1.718) (writing took 8.929892338000172 seconds)\n",
            "2023-03-05 19:02:40 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
            "2023-03-05 19:02:40 | INFO | train | epoch 026 | loss 2.578 | ppl 5.97 | wps 4178.5 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 4368 | lr 9.56949e-05 | gnorm 2.16 | train_wall 86 | gb_free 13.2 | wall 2587\n",
            "epoch 027:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:02:40 | INFO | fairseq.trainer | begin training epoch 27\n",
            "2023-03-05 19:02:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 027:  99% 167/168 [01:26<00:00,  2.53it/s, loss=2.226, ppl=4.68, wps=4974.8, ups=1.97, wpb=2522.3, bsz=109.8, num_updates=4500, lr=9.42809e-05, gnorm=1.965, train_wall=50, gb_free=13.3, wall=2654]2023-03-05 19:04:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 027 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:   2% 1/46 [00:00<00:05,  8.32it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:   7% 3/46 [00:00<00:03, 13.89it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  11% 5/46 [00:00<00:02, 15.67it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  15% 7/46 [00:00<00:02, 16.57it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  22% 10/46 [00:00<00:01, 18.57it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  28% 13/46 [00:00<00:01, 19.44it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  35% 16/46 [00:00<00:01, 19.40it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  39% 18/46 [00:00<00:01, 19.39it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  43% 20/46 [00:01<00:01, 18.71it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 17.72it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.22it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.58it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.27it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.02it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  70% 32/46 [00:02<00:01, 10.70it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.03it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.65it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.98it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.82it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.58it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.42it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.03it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.54it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.86it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.98it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.70it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset: 100% 46/46 [00:05<00:00,  3.80it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:04:12 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 1.575 | ppl 2.98 | wps 12976.4 | wpb 1415.4 | bsz 53.1 | num_updates 4536 | best_loss 1.575\n",
            "2023-03-05 19:04:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 4536 updates\n",
            "2023-03-05 19:04:12 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint27.pt\n",
            "2023-03-05 19:04:15 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint27.pt\n",
            "2023-03-05 19:04:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint27.pt (epoch 27 @ 4536 updates, score 1.575) (writing took 8.336025966999841 seconds)\n",
            "2023-03-05 19:04:20 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
            "2023-03-05 19:04:20 | INFO | train | epoch 027 | loss 2.265 | ppl 4.81 | wps 4190.5 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 4536 | lr 9.3906e-05 | gnorm 2.141 | train_wall 86 | gb_free 13.4 | wall 2687\n",
            "epoch 028:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:04:20 | INFO | fairseq.trainer | begin training epoch 28\n",
            "2023-03-05 19:04:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 028:  99% 167/168 [01:25<00:00,  2.18it/s, loss=1.974, ppl=3.93, wps=4806.9, ups=1.93, wpb=2486.2, bsz=112.8, num_updates=4700, lr=9.22531e-05, gnorm=2.358, train_wall=51, gb_free=13.3, wall=2772]2023-03-05 19:05:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 028 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 17.97it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 22.46it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 23.04it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 22.84it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.47it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.67it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.41it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.10it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.57it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.88it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.64it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.42it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.94it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.21it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.76it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.06it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.90it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.63it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.45it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.04it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.55it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.89it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  4.02it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.73it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.82it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:05:52 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 1.414 | ppl 2.66 | wps 13297.4 | wpb 1415.4 | bsz 53.1 | num_updates 4704 | best_loss 1.414\n",
            "2023-03-05 19:05:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 4704 updates\n",
            "2023-03-05 19:05:52 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint28.pt\n",
            "2023-03-05 19:05:55 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint28.pt\n",
            "2023-03-05 19:06:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint28.pt (epoch 28 @ 4704 updates, score 1.414) (writing took 8.44631078600014 seconds)\n",
            "2023-03-05 19:06:00 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
            "2023-03-05 19:06:00 | INFO | train | epoch 028 | loss 2.044 | ppl 4.12 | wps 4194.9 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 4704 | lr 9.22139e-05 | gnorm 2.24 | train_wall 86 | gb_free 13.3 | wall 2787\n",
            "epoch 029:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:06:00 | INFO | fairseq.trainer | begin training epoch 29\n",
            "2023-03-05 19:06:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 029:  99% 167/168 [01:25<00:00,  1.23it/s, loss=1.822, ppl=3.53, wps=3789, ups=1.55, wpb=2452, bsz=102, num_updates=4800, lr=9.12871e-05, gnorm=1.82, train_wall=51, gb_free=13.2, wall=2836]2023-03-05 19:07:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 029 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:   2% 1/46 [00:00<00:05,  8.22it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:   9% 4/46 [00:00<00:02, 17.07it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  13% 6/46 [00:00<00:02, 17.15it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  20% 9/46 [00:00<00:01, 19.59it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 19.32it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  28% 13/46 [00:00<00:01, 19.26it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  35% 16/46 [00:00<00:01, 19.28it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  39% 18/46 [00:00<00:01, 19.28it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  43% 20/46 [00:01<00:01, 18.01it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 17.00it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 16.67it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 15.96it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 14.85it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 12.66it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  70% 32/46 [00:02<00:01, 10.52it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  8.97it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.59it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.94it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.81it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.57it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.42it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.06it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.56it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.90it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  4.02it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.72it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset: 100% 46/46 [00:05<00:00,  3.83it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:07:31 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 1.27 | ppl 2.41 | wps 13034.7 | wpb 1415.4 | bsz 53.1 | num_updates 4872 | best_loss 1.27\n",
            "2023-03-05 19:07:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 4872 updates\n",
            "2023-03-05 19:07:31 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint29.pt\n",
            "2023-03-05 19:07:34 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint29.pt\n",
            "2023-03-05 19:07:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint29.pt (epoch 29 @ 4872 updates, score 1.27) (writing took 8.21606793400042 seconds)\n",
            "2023-03-05 19:07:40 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
            "2023-03-05 19:07:40 | INFO | train | epoch 029 | loss 1.811 | ppl 3.51 | wps 4205.8 | ups 1.69 | wpb 2492.9 | bsz 106.7 | num_updates 4872 | lr 9.061e-05 | gnorm 1.832 | train_wall 86 | gb_free 13.3 | wall 2886\n",
            "epoch 030:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:07:40 | INFO | fairseq.trainer | begin training epoch 30\n",
            "2023-03-05 19:07:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 030:  99% 167/168 [01:25<00:00,  2.03it/s, loss=1.682, ppl=3.21, wps=4936.7, ups=1.97, wpb=2503.6, bsz=115.9, num_updates=5000, lr=8.94427e-05, gnorm=1.91, train_wall=50, gb_free=13.3, wall=2951]2023-03-05 19:09:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 030 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:   7% 3/46 [00:00<00:01, 21.90it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  13% 6/46 [00:00<00:01, 22.05it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  20% 9/46 [00:00<00:01, 22.79it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  26% 12/46 [00:00<00:01, 22.17it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 21.37it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  39% 18/46 [00:00<00:01, 20.45it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  46% 21/46 [00:01<00:01, 19.02it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 18.17it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 17.40it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 16.38it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 14.44it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  67% 31/46 [00:01<00:01, 12.12it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  72% 33/46 [00:02<00:01, 10.07it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.44it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.70it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.95it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.77it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.51it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.35it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.99it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.50it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.82it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.96it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.69it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.79it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:09:11 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 1.207 | ppl 2.31 | wps 13268.8 | wpb 1415.4 | bsz 53.1 | num_updates 5040 | best_loss 1.207\n",
            "2023-03-05 19:09:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 5040 updates\n",
            "2023-03-05 19:09:11 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint30.pt\n",
            "2023-03-05 19:09:14 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint30.pt\n",
            "2023-03-05 19:09:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint30.pt (epoch 30 @ 5040 updates, score 1.207) (writing took 8.230080893999911 seconds)\n",
            "2023-03-05 19:09:19 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
            "2023-03-05 19:09:19 | INFO | train | epoch 030 | loss 1.681 | ppl 3.21 | wps 4203.3 | ups 1.69 | wpb 2492.9 | bsz 106.7 | num_updates 5040 | lr 8.90871e-05 | gnorm 1.874 | train_wall 86 | gb_free 13.4 | wall 2986\n",
            "epoch 031:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:09:19 | INFO | fairseq.trainer | begin training epoch 31\n",
            "2023-03-05 19:09:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 031:  99% 167/168 [01:25<00:00,  2.07it/s, loss=1.507, ppl=2.84, wps=5058.8, ups=2.01, wpb=2515.1, bsz=110.9, num_updates=5200, lr=8.77058e-05, gnorm=2.086, train_wall=49, gb_free=13.3, wall=3068]2023-03-05 19:10:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 031 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:   7% 3/46 [00:00<00:01, 23.20it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  13% 6/46 [00:00<00:01, 22.99it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  20% 9/46 [00:00<00:01, 23.06it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  26% 12/46 [00:00<00:01, 21.44it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 20.22it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  39% 18/46 [00:00<00:01, 19.23it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  43% 20/46 [00:01<00:01, 18.23it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 17.12it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 16.45it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 15.74it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 14.50it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 12.52it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  70% 32/46 [00:02<00:01, 10.32it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  8.90it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.15it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.33it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.58it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.51it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.31it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.19it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.85it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.40it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.77it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.93it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.66it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset: 100% 46/46 [00:05<00:00,  3.78it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:10:51 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 1.11 | ppl 2.16 | wps 12993.6 | wpb 1415.4 | bsz 53.1 | num_updates 5208 | best_loss 1.11\n",
            "2023-03-05 19:10:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 5208 updates\n",
            "2023-03-05 19:10:51 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint31.pt\n",
            "2023-03-05 19:10:54 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint31.pt\n",
            "2023-03-05 19:10:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint31.pt (epoch 31 @ 5208 updates, score 1.11) (writing took 8.364962329999798 seconds)\n",
            "2023-03-05 19:10:59 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
            "2023-03-05 19:10:59 | INFO | train | epoch 031 | loss 1.577 | ppl 2.98 | wps 4195.9 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 5208 | lr 8.76384e-05 | gnorm 2.122 | train_wall 86 | gb_free 13.2 | wall 3086\n",
            "epoch 032:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:10:59 | INFO | fairseq.trainer | begin training epoch 32\n",
            "2023-03-05 19:10:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 032:  99% 167/168 [01:26<00:00,  2.00it/s, loss=1.442, ppl=2.72, wps=3572.3, ups=1.44, wpb=2475.8, bsz=97.9, num_updates=5300, lr=8.68744e-05, gnorm=1.683, train_wall=55, gb_free=13.3, wall=3138]2023-03-05 19:12:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 032 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 17.23it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 23.52it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 23.31it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 22.70it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.78it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.77it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.57it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.17it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.67it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.89it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.48it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.19it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.76it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.09it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.69it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.00it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.82it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.55it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.38it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.99it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.50it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.83it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.97it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.69it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.79it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:12:31 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 1.051 | ppl 2.07 | wps 13213.6 | wpb 1415.4 | bsz 53.1 | num_updates 5376 | best_loss 1.051\n",
            "2023-03-05 19:12:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 5376 updates\n",
            "2023-03-05 19:12:31 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint32.pt\n",
            "2023-03-05 19:12:34 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint32.pt\n",
            "2023-03-05 19:12:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint32.pt (epoch 32 @ 5376 updates, score 1.051) (writing took 8.588285230999645 seconds)\n",
            "2023-03-05 19:12:39 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
            "2023-03-05 19:12:39 | INFO | train | epoch 032 | loss 1.432 | ppl 2.7 | wps 4177.7 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 5376 | lr 8.62582e-05 | gnorm 1.522 | train_wall 86 | gb_free 13.4 | wall 3186\n",
            "epoch 033:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:12:39 | INFO | fairseq.trainer | begin training epoch 33\n",
            "2023-03-05 19:12:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 033:  99% 167/168 [01:25<00:00,  1.99it/s, loss=1.34, ppl=2.53, wps=5020.9, ups=2.01, wpb=2495.2, bsz=109.8, num_updates=5500, lr=8.52803e-05, gnorm=1.89, train_wall=49, gb_free=13.4, wall=3249]2023-03-05 19:14:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 033 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 19.07it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 23.37it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 23.79it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 23.08it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 22.06it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.70it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.61it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.26it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.61it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.52it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.31it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.06it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.64it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.04it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.60it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.93it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.79it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.53it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.36it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.97it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.48it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.81it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.97it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.69it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.79it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:14:11 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 0.999 | ppl 2 | wps 13172.9 | wpb 1415.4 | bsz 53.1 | num_updates 5544 | best_loss 0.999\n",
            "2023-03-05 19:14:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 5544 updates\n",
            "2023-03-05 19:14:11 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint33.pt\n",
            "2023-03-05 19:14:14 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint33.pt\n",
            "2023-03-05 19:14:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint33.pt (epoch 33 @ 5544 updates, score 0.999) (writing took 8.247255766999842 seconds)\n",
            "2023-03-05 19:14:19 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
            "2023-03-05 19:14:19 | INFO | train | epoch 033 | loss 1.371 | ppl 2.59 | wps 4187.5 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 5544 | lr 8.49412e-05 | gnorm 1.797 | train_wall 86 | gb_free 13.2 | wall 3286\n",
            "epoch 034:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:14:19 | INFO | fairseq.trainer | begin training epoch 34\n",
            "2023-03-05 19:14:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 034:  99% 167/168 [01:25<00:00,  1.59it/s, loss=1.278, ppl=2.42, wps=4919.1, ups=1.97, wpb=2497, bsz=110.2, num_updates=5700, lr=8.37708e-05, gnorm=1.517, train_wall=50, gb_free=13.2, wall=3365]2023-03-05 19:15:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 034 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:   7% 3/46 [00:00<00:01, 22.43it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  13% 6/46 [00:00<00:01, 23.78it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  20% 9/46 [00:00<00:01, 24.29it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  26% 12/46 [00:00<00:01, 23.02it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 21.91it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  39% 18/46 [00:00<00:01, 20.71it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  46% 21/46 [00:01<00:01, 18.64it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 18.00it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 17.25it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 16.31it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 14.39it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  67% 31/46 [00:01<00:01, 12.04it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  72% 33/46 [00:02<00:01,  9.91it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.34it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.63it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.87it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.74it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.46it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.30it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.92it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.45it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.81it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.95it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.67it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.78it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:15:51 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 0.966 | ppl 1.95 | wps 13246.4 | wpb 1415.4 | bsz 53.1 | num_updates 5712 | best_loss 0.966\n",
            "2023-03-05 19:15:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 5712 updates\n",
            "2023-03-05 19:15:51 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint34.pt\n",
            "2023-03-05 19:15:53 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint34.pt\n",
            "2023-03-05 19:15:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint34.pt (epoch 34 @ 5712 updates, score 0.966) (writing took 8.207711619000293 seconds)\n",
            "2023-03-05 19:15:59 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
            "2023-03-05 19:15:59 | INFO | train | epoch 034 | loss 1.268 | ppl 2.41 | wps 4210.9 | ups 1.69 | wpb 2492.9 | bsz 106.7 | num_updates 5712 | lr 8.36827e-05 | gnorm 1.434 | train_wall 86 | gb_free 13.2 | wall 3385\n",
            "epoch 035:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:15:59 | INFO | fairseq.trainer | begin training epoch 35\n",
            "2023-03-05 19:15:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 035:  99% 167/168 [01:26<00:00,  1.52it/s, loss=1.209, ppl=2.31, wps=3773.3, ups=1.49, wpb=2533.5, bsz=109.6, num_updates=5800, lr=8.30455e-05, gnorm=1.424, train_wall=54, gb_free=13.2, wall=3433]2023-03-05 19:17:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 035 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 17.49it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 22.51it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 23.00it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 22.87it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.82it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.89it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.67it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.27it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.77it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 17.03it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.60it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.27it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.87it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.20it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.77it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.08it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.89it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.61it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.43it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.04it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.53it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.88it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  4.01it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.72it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.71it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:17:31 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 0.934 | ppl 1.91 | wps 13248.5 | wpb 1415.4 | bsz 53.1 | num_updates 5880 | best_loss 0.934\n",
            "2023-03-05 19:17:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 5880 updates\n",
            "2023-03-05 19:17:31 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint35.pt\n",
            "2023-03-05 19:17:33 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint35.pt\n",
            "2023-03-05 19:17:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint35.pt (epoch 35 @ 5880 updates, score 0.934) (writing took 8.363004877000094 seconds)\n",
            "2023-03-05 19:17:39 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
            "2023-03-05 19:17:39 | INFO | train | epoch 035 | loss 1.2 | ppl 2.3 | wps 4183.7 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 5880 | lr 8.24786e-05 | gnorm 1.316 | train_wall 86 | gb_free 13.3 | wall 3486\n",
            "epoch 036:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:17:39 | INFO | fairseq.trainer | begin training epoch 36\n",
            "2023-03-05 19:17:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 036:  99% 167/168 [01:25<00:00,  2.18it/s, loss=1.073, ppl=2.1, wps=5075, ups=2.04, wpb=2486, bsz=112.1, num_updates=6000, lr=8.16497e-05, gnorm=1.115, train_wall=49, gb_free=13.3, wall=3548]2023-03-05 19:19:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 036 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:   2% 1/46 [00:00<00:06,  7.35it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:   9% 4/46 [00:00<00:02, 15.02it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  13% 6/46 [00:00<00:02, 16.45it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  20% 9/46 [00:00<00:01, 19.70it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  26% 12/46 [00:00<00:01, 20.30it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 20.27it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  39% 18/46 [00:00<00:01, 19.82it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  43% 20/46 [00:01<00:01, 18.75it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 17.79it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.41it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.74it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.40it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.14it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  70% 32/46 [00:02<00:01, 10.75it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.14it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.73it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.05it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.86it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.60it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.42it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.03it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.54it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.87it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  4.01it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.72it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset: 100% 46/46 [00:05<00:00,  3.83it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:19:10 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 0.892 | ppl 1.86 | wps 13130.8 | wpb 1415.4 | bsz 53.1 | num_updates 6048 | best_loss 0.892\n",
            "2023-03-05 19:19:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 6048 updates\n",
            "2023-03-05 19:19:10 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint36.pt\n",
            "2023-03-05 19:19:13 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint36.pt\n",
            "2023-03-05 19:19:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint36.pt (epoch 36 @ 6048 updates, score 0.892) (writing took 8.249038379000012 seconds)\n",
            "2023-03-05 19:19:19 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
            "2023-03-05 19:19:19 | INFO | train | epoch 036 | loss 1.135 | ppl 2.2 | wps 4203.1 | ups 1.69 | wpb 2492.9 | bsz 106.7 | num_updates 6048 | lr 8.1325e-05 | gnorm 1.214 | train_wall 86 | gb_free 13.4 | wall 3585\n",
            "epoch 037:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:19:19 | INFO | fairseq.trainer | begin training epoch 37\n",
            "2023-03-05 19:19:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 037:  99% 167/168 [01:25<00:00,  2.48it/s, loss=1.1, ppl=2.14, wps=4924.8, ups=1.97, wpb=2504.7, bsz=115.3, num_updates=6200, lr=8.03219e-05, gnorm=1.18, train_wall=50, gb_free=13.4, wall=3664]2023-03-05 19:20:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 037 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 16.60it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 22.29it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 22.90it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 22.62it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.68it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.71it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.60it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.27it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.76it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 17.01it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.53it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.21it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.83it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.16it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.73it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.07it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.89it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.60it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.43it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.04it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.55it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.88it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.99it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.71it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.73it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:20:50 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 0.844 | ppl 1.79 | wps 13220.9 | wpb 1415.4 | bsz 53.1 | num_updates 6216 | best_loss 0.844\n",
            "2023-03-05 19:20:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 6216 updates\n",
            "2023-03-05 19:20:50 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint37.pt\n",
            "2023-03-05 19:20:53 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint37.pt\n",
            "2023-03-05 19:20:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint37.pt (epoch 37 @ 6216 updates, score 0.844) (writing took 8.55646459700074 seconds)\n",
            "2023-03-05 19:20:59 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
            "2023-03-05 19:20:59 | INFO | train | epoch 037 | loss 1.084 | ppl 2.12 | wps 4171.8 | ups 1.67 | wpb 2492.9 | bsz 106.7 | num_updates 6216 | lr 8.02185e-05 | gnorm 1.238 | train_wall 86 | gb_free 13.2 | wall 3686\n",
            "epoch 038:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:20:59 | INFO | fairseq.trainer | begin training epoch 38\n",
            "2023-03-05 19:20:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 038:  99% 167/168 [01:25<00:00,  2.11it/s, loss=1.077, ppl=2.11, wps=3722.2, ups=1.5, wpb=2473.3, bsz=100.3, num_updates=6300, lr=7.96819e-05, gnorm=1.122, train_wall=52, gb_free=13.4, wall=3731]2023-03-05 19:22:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 038 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:   2% 1/46 [00:00<00:05,  8.88it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:   9% 4/46 [00:00<00:02, 16.09it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  13% 6/46 [00:00<00:02, 16.56it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  20% 9/46 [00:00<00:02, 18.11it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 18.54it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  28% 13/46 [00:00<00:01, 18.39it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 18.54it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 18.17it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  41% 19/46 [00:01<00:01, 17.65it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  46% 21/46 [00:01<00:01, 17.16it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 16.38it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 15.64it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 15.18it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 13.72it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  67% 31/46 [00:02<00:01, 11.55it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  72% 33/46 [00:02<00:01,  9.67it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.20it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.51it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  80% 37/46 [00:03<00:01,  6.79it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.63it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.39it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.28it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.92it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.44it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.78it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.93it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.67it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset: 100% 46/46 [00:05<00:00,  3.77it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:22:31 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 0.881 | ppl 1.84 | wps 12807.1 | wpb 1415.4 | bsz 53.1 | num_updates 6384 | best_loss 0.844\n",
            "2023-03-05 19:22:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 6384 updates\n",
            "2023-03-05 19:22:31 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint38.pt\n",
            "2023-03-05 19:22:33 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint38.pt\n",
            "2023-03-05 19:22:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint38.pt (epoch 38 @ 6384 updates, score 0.881) (writing took 5.5415946290004285 seconds)\n",
            "2023-03-05 19:22:36 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
            "2023-03-05 19:22:36 | INFO | train | epoch 038 | loss 1.045 | ppl 2.06 | wps 4313.5 | ups 1.73 | wpb 2492.9 | bsz 106.7 | num_updates 6384 | lr 7.91559e-05 | gnorm 1.325 | train_wall 86 | gb_free 13.3 | wall 3783\n",
            "epoch 039:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:22:36 | INFO | fairseq.trainer | begin training epoch 39\n",
            "2023-03-05 19:22:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 039:  99% 167/168 [01:25<00:00,  1.65it/s, loss=1.003, ppl=2, wps=4877.3, ups=1.95, wpb=2495.6, bsz=109, num_updates=6500, lr=7.84465e-05, gnorm=1.01, train_wall=51, gb_free=13.4, wall=3842]2023-03-05 19:24:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 039 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 15.81it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  11% 5/46 [00:00<00:02, 20.41it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 21.61it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 21.80it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 20.97it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.39it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.61it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.44it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.76it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.84it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.50it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.18it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  70% 32/46 [00:02<00:01, 10.85it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.14it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.70it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.01it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.86it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.60it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.43it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.05it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.56it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.87it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  4.00it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.71it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.82it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:24:07 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 0.828 | ppl 1.78 | wps 13220 | wpb 1415.4 | bsz 53.1 | num_updates 6552 | best_loss 0.828\n",
            "2023-03-05 19:24:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 6552 updates\n",
            "2023-03-05 19:24:07 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint39.pt\n",
            "2023-03-05 19:24:10 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint39.pt\n",
            "2023-03-05 19:24:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint39.pt (epoch 39 @ 6552 updates, score 0.828) (writing took 8.543828856999426 seconds)\n",
            "2023-03-05 19:24:16 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
            "2023-03-05 19:24:16 | INFO | train | epoch 039 | loss 0.996 | ppl 1.99 | wps 4193.3 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 6552 | lr 7.81345e-05 | gnorm 1.171 | train_wall 86 | gb_free 13.2 | wall 3883\n",
            "epoch 040:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:24:16 | INFO | fairseq.trainer | begin training epoch 40\n",
            "2023-03-05 19:24:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 040:  99% 167/168 [01:25<00:00,  1.97it/s, loss=0.938, ppl=1.92, wps=4836, ups=1.94, wpb=2497.8, bsz=113.6, num_updates=6700, lr=7.72667e-05, gnorm=1.075, train_wall=51, gb_free=13.2, wall=3959]2023-03-05 19:25:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 040 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 19.93it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 24.29it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 23.87it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 23.23it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.95it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.90it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.49it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.09it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.61it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.73it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.48it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 12.99it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.54it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.01it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.58it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.96it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.78it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.54it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.39it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.03it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.53it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.85it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.99it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.70it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.81it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:25:48 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 0.8 | ppl 1.74 | wps 13206.8 | wpb 1415.4 | bsz 53.1 | num_updates 6720 | best_loss 0.8\n",
            "2023-03-05 19:25:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 6720 updates\n",
            "2023-03-05 19:25:48 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint40.pt\n",
            "2023-03-05 19:25:50 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint40.pt\n",
            "2023-03-05 19:25:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint40.pt (epoch 40 @ 6720 updates, score 0.8) (writing took 8.266338935999556 seconds)\n",
            "2023-03-05 19:25:56 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
            "2023-03-05 19:25:56 | INFO | train | epoch 040 | loss 0.977 | ppl 1.97 | wps 4195.2 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 6720 | lr 7.71517e-05 | gnorm 1.184 | train_wall 86 | gb_free 13.5 | wall 3982\n",
            "epoch 041:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:25:56 | INFO | fairseq.trainer | begin training epoch 41\n",
            "2023-03-05 19:25:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 041:  99% 167/168 [01:26<00:00,  1.96it/s, loss=0.932, ppl=1.91, wps=4012, ups=1.59, wpb=2521.8, bsz=109.6, num_updates=6800, lr=7.66965e-05, gnorm=0.953, train_wall=49, gb_free=13.4, wall=4022]2023-03-05 19:27:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 041 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 19.03it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 22.39it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 23.65it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 23.09it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 22.06it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 21.06it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.73it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.31it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.73it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.90it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.62it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.32it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.84it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.16it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.72it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.03it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.85it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.59it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.40it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.00it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.51it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.87it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.99it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.71it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.81it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:27:28 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 0.759 | ppl 1.69 | wps 13288.1 | wpb 1415.4 | bsz 53.1 | num_updates 6888 | best_loss 0.759\n",
            "2023-03-05 19:27:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 6888 updates\n",
            "2023-03-05 19:27:28 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint41.pt\n",
            "2023-03-05 19:27:31 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint41.pt\n",
            "2023-03-05 19:27:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint41.pt (epoch 41 @ 6888 updates, score 0.759) (writing took 8.21974579200014 seconds)\n",
            "2023-03-05 19:27:36 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
            "2023-03-05 19:27:36 | INFO | train | epoch 041 | loss 0.933 | ppl 1.91 | wps 4181.7 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 6888 | lr 7.6205e-05 | gnorm 1.004 | train_wall 86 | gb_free 13.2 | wall 4083\n",
            "epoch 042:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:27:36 | INFO | fairseq.trainer | begin training epoch 42\n",
            "2023-03-05 19:27:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 042:  99% 167/168 [01:26<00:00,  2.36it/s, loss=0.912, ppl=1.88, wps=4732.3, ups=1.9, wpb=2484.6, bsz=103.6, num_updates=7000, lr=7.55929e-05, gnorm=1.008, train_wall=52, gb_free=13.2, wall=4141]2023-03-05 19:29:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 042 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 15.78it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 21.30it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 22.05it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 21.85it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.26it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.33it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.19it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 17.96it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.10it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.32it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 14.90it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 12.83it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  70% 32/46 [00:02<00:01, 10.42it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  8.92it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.15it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.40it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.63it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.51it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.29it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.17it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.82it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.37it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.70it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.87it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.62it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset: 100% 46/46 [00:05<00:00,  3.75it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:29:08 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 0.787 | ppl 1.73 | wps 13025.9 | wpb 1415.4 | bsz 53.1 | num_updates 7056 | best_loss 0.759\n",
            "2023-03-05 19:29:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 7056 updates\n",
            "2023-03-05 19:29:08 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint42.pt\n",
            "2023-03-05 19:29:11 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint42.pt\n",
            "2023-03-05 19:29:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint42.pt (epoch 42 @ 7056 updates, score 0.787) (writing took 5.53451070400024 seconds)\n",
            "2023-03-05 19:29:13 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
            "2023-03-05 19:29:13 | INFO | train | epoch 042 | loss 0.901 | ppl 1.87 | wps 4297.1 | ups 1.72 | wpb 2492.9 | bsz 106.7 | num_updates 7056 | lr 7.52923e-05 | gnorm 0.99 | train_wall 86 | gb_free 13.2 | wall 4180\n",
            "epoch 043:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:29:13 | INFO | fairseq.trainer | begin training epoch 43\n",
            "2023-03-05 19:29:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 043:  99% 167/168 [01:26<00:00,  1.46it/s, loss=0.862, ppl=1.82, wps=5235.1, ups=2.07, wpb=2529.2, bsz=111.1, num_updates=7200, lr=7.45356e-05, gnorm=0.872, train_wall=48, gb_free=13.2, wall=4253]2023-03-05 19:30:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 043 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:   2% 1/46 [00:00<00:05,  8.20it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:   9% 4/46 [00:00<00:02, 15.87it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  13% 6/46 [00:00<00:02, 16.76it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  20% 9/46 [00:00<00:01, 19.04it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 19.11it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  28% 13/46 [00:00<00:01, 18.81it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 18.99it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 18.71it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  41% 19/46 [00:01<00:01, 18.22it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  46% 21/46 [00:01<00:01, 18.03it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 17.31it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 16.67it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 15.71it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 14.01it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  67% 31/46 [00:01<00:01, 11.74it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  72% 33/46 [00:02<00:01,  9.81it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.23it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.50it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  6.79it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.66it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.41it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.27it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.90it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.44it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.77it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.92it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.66it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset: 100% 46/46 [00:05<00:00,  3.76it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:30:45 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 0.718 | ppl 1.64 | wps 12953.4 | wpb 1415.4 | bsz 53.1 | num_updates 7224 | best_loss 0.718\n",
            "2023-03-05 19:30:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 7224 updates\n",
            "2023-03-05 19:30:45 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint43.pt\n",
            "2023-03-05 19:30:48 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint43.pt\n",
            "2023-03-05 19:30:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint43.pt (epoch 43 @ 7224 updates, score 0.718) (writing took 8.185851391000142 seconds)\n",
            "2023-03-05 19:30:54 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
            "2023-03-05 19:30:54 | INFO | train | epoch 043 | loss 0.871 | ppl 1.83 | wps 4181.6 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 7224 | lr 7.44117e-05 | gnorm 0.968 | train_wall 86 | gb_free 13.4 | wall 4280\n",
            "epoch 044:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:30:54 | INFO | fairseq.trainer | begin training epoch 44\n",
            "2023-03-05 19:30:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 044:  99% 167/168 [01:25<00:00,  1.99it/s, loss=0.89, ppl=1.85, wps=3598.8, ups=1.46, wpb=2459.3, bsz=103.5, num_updates=7300, lr=7.40233e-05, gnorm=1.014, train_wall=55, gb_free=13.2, wall=4321]2023-03-05 19:32:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 044 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 19.33it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 23.60it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 22.91it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 22.55it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.64it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.61it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.38it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.04it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.55it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.78it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.54it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.27it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.74it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.10it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.68it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.03it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.85it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.56it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.40it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.02it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.52it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.84it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.98it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.70it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.75it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:32:25 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 0.727 | ppl 1.66 | wps 13177 | wpb 1415.4 | bsz 53.1 | num_updates 7392 | best_loss 0.718\n",
            "2023-03-05 19:32:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 7392 updates\n",
            "2023-03-05 19:32:25 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint44.pt\n",
            "2023-03-05 19:32:28 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint44.pt\n",
            "2023-03-05 19:32:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint44.pt (epoch 44 @ 7392 updates, score 0.727) (writing took 6.096687408999969 seconds)\n",
            "2023-03-05 19:32:32 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
            "2023-03-05 19:32:32 | INFO | train | epoch 044 | loss 0.845 | ppl 1.8 | wps 4276.1 | ups 1.72 | wpb 2492.9 | bsz 106.7 | num_updates 7392 | lr 7.35612e-05 | gnorm 0.959 | train_wall 86 | gb_free 13.3 | wall 4378\n",
            "epoch 045:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:32:32 | INFO | fairseq.trainer | begin training epoch 45\n",
            "2023-03-05 19:32:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 045:  99% 167/168 [01:26<00:00,  1.95it/s, loss=0.855, ppl=1.81, wps=4572.5, ups=1.85, wpb=2469, bsz=104.3, num_updates=7500, lr=7.30297e-05, gnorm=1.044, train_wall=54, gb_free=13.3, wall=4436]2023-03-05 19:33:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 045 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 17.58it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 23.29it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 23.42it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 22.67it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.53it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.62it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.39it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.21it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.63it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.86it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.48it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.22it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.68it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.10it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.67it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.00it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.79it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.53it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.35it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.99it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.50it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.84it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.96it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.68it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset: 100% 46/46 [00:05<00:00,  3.67it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:34:03 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 0.707 | ppl 1.63 | wps 13116.3 | wpb 1415.4 | bsz 53.1 | num_updates 7560 | best_loss 0.707\n",
            "2023-03-05 19:34:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 7560 updates\n",
            "2023-03-05 19:34:03 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint45.pt\n",
            "2023-03-05 19:34:06 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint45.pt\n",
            "2023-03-05 19:34:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint45.pt (epoch 45 @ 7560 updates, score 0.707) (writing took 8.459442020000097 seconds)\n",
            "2023-03-05 19:34:12 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
            "2023-03-05 19:34:12 | INFO | train | epoch 045 | loss 0.825 | ppl 1.77 | wps 4172.1 | ups 1.67 | wpb 2492.9 | bsz 106.7 | num_updates 7560 | lr 7.27393e-05 | gnorm 0.98 | train_wall 86 | gb_free 13.2 | wall 4479\n",
            "epoch 046:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:34:12 | INFO | fairseq.trainer | begin training epoch 46\n",
            "2023-03-05 19:34:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 046:  99% 167/168 [01:26<00:00,  2.09it/s, loss=0.789, ppl=1.73, wps=4692, ups=1.87, wpb=2509.3, bsz=104.9, num_updates=7700, lr=7.2075e-05, gnorm=0.913, train_wall=53, gb_free=13.2, wall=4551]2023-03-05 19:35:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 046 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:   2% 1/46 [00:00<00:05,  7.56it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:   9% 4/46 [00:00<00:02, 16.71it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  15% 7/46 [00:00<00:02, 19.12it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  22% 10/46 [00:00<00:01, 19.58it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  26% 12/46 [00:00<00:01, 19.22it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 19.15it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  35% 16/46 [00:00<00:01, 18.58it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  39% 18/46 [00:00<00:01, 18.36it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  43% 20/46 [00:01<00:01, 17.56it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 16.53it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 16.46it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 15.97it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 14.81it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 12.75it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  70% 32/46 [00:02<00:01, 10.37it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  8.91it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.55it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  80% 37/46 [00:03<00:01,  6.90it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.73it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.50it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.35it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.98it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.49it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.80it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.96it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.69it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset: 100% 46/46 [00:05<00:00,  3.79it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:35:44 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 0.697 | ppl 1.62 | wps 12938.5 | wpb 1415.4 | bsz 53.1 | num_updates 7728 | best_loss 0.697\n",
            "2023-03-05 19:35:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 7728 updates\n",
            "2023-03-05 19:35:44 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint46.pt\n",
            "2023-03-05 19:35:46 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint46.pt\n",
            "2023-03-05 19:35:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint46.pt (epoch 46 @ 7728 updates, score 0.697) (writing took 8.294437867999477 seconds)\n",
            "2023-03-05 19:35:52 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
            "2023-03-05 19:35:52 | INFO | train | epoch 046 | loss 0.797 | ppl 1.74 | wps 4184.2 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 7728 | lr 7.19443e-05 | gnorm 0.898 | train_wall 86 | gb_free 13.3 | wall 4579\n",
            "epoch 047:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:35:52 | INFO | fairseq.trainer | begin training epoch 47\n",
            "2023-03-05 19:35:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 047:  99% 167/168 [01:26<00:00,  2.29it/s, loss=0.807, ppl=1.75, wps=3643.7, ups=1.48, wpb=2468.9, bsz=102.4, num_updates=7800, lr=7.16115e-05, gnorm=0.911, train_wall=54, gb_free=13.6, wall=4619]2023-03-05 19:37:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 047 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 18.16it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 23.49it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 23.65it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 23.19it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.77it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.54it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.41it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.06it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.61it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.86it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.53it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.19it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.80it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.12it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.72it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.05it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.84it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.57it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.42it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.05it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.55it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.86it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.99it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.71it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.74it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:37:24 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 0.67 | ppl 1.59 | wps 13235.9 | wpb 1415.4 | bsz 53.1 | num_updates 7896 | best_loss 0.67\n",
            "2023-03-05 19:37:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 7896 updates\n",
            "2023-03-05 19:37:24 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint47.pt\n",
            "2023-03-05 19:37:27 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint47.pt\n",
            "2023-03-05 19:37:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint47.pt (epoch 47 @ 7896 updates, score 0.67) (writing took 8.533273460999226 seconds)\n",
            "2023-03-05 19:37:32 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
            "2023-03-05 19:37:32 | INFO | train | epoch 047 | loss 0.775 | ppl 1.71 | wps 4170.8 | ups 1.67 | wpb 2492.9 | bsz 106.7 | num_updates 7896 | lr 7.11748e-05 | gnorm 0.83 | train_wall 86 | gb_free 13.4 | wall 4679\n",
            "epoch 048:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:37:32 | INFO | fairseq.trainer | begin training epoch 48\n",
            "2023-03-05 19:37:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 048:  99% 167/168 [01:26<00:00,  1.59it/s, loss=0.73, ppl=1.66, wps=5008.4, ups=1.98, wpb=2524.6, bsz=110.6, num_updates=8000, lr=7.07107e-05, gnorm=0.807, train_wall=50, gb_free=13.3, wall=4733]2023-03-05 19:38:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 048 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:   2% 1/46 [00:00<00:05,  7.84it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:   7% 3/46 [00:00<00:03, 14.09it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  11% 5/46 [00:00<00:02, 15.97it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  15% 7/46 [00:00<00:02, 16.70it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  22% 10/46 [00:00<00:01, 18.17it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  26% 12/46 [00:00<00:01, 18.52it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 18.42it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  35% 16/46 [00:00<00:01, 17.81it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  39% 18/46 [00:01<00:01, 17.75it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  43% 20/46 [00:01<00:01, 17.01it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 16.40it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 15.98it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 15.29it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 14.12it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 12.18it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  70% 32/46 [00:02<00:01, 10.17it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  8.78it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.05it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.28it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  80% 37/46 [00:03<00:01,  6.57it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.47it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.29it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.20it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.86it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  91% 42/46 [00:04<00:00,  5.40it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.74it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.91it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.65it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset: 100% 46/46 [00:05<00:00,  3.75it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:39:04 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 0.697 | ppl 1.62 | wps 12690.7 | wpb 1415.4 | bsz 53.1 | num_updates 8064 | best_loss 0.67\n",
            "2023-03-05 19:39:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 8064 updates\n",
            "2023-03-05 19:39:04 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint48.pt\n",
            "2023-03-05 19:39:07 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint48.pt\n",
            "2023-03-05 19:39:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint48.pt (epoch 48 @ 8064 updates, score 0.697) (writing took 5.980722023999988 seconds)\n",
            "2023-03-05 19:39:10 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
            "2023-03-05 19:39:10 | INFO | train | epoch 048 | loss 0.756 | ppl 1.69 | wps 4278.8 | ups 1.72 | wpb 2492.9 | bsz 106.7 | num_updates 8064 | lr 7.04295e-05 | gnorm 1.77 | train_wall 86 | gb_free 13.3 | wall 4777\n",
            "epoch 049:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:39:10 | INFO | fairseq.trainer | begin training epoch 49\n",
            "2023-03-05 19:39:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 049:  99% 167/168 [01:26<00:00,  1.96it/s, loss=0.765, ppl=1.7, wps=4932.4, ups=1.98, wpb=2488.5, bsz=111.5, num_updates=8200, lr=6.9843e-05, gnorm=0.879, train_wall=50, gb_free=13.3, wall=4848]2023-03-05 19:40:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 049 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 19.60it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 23.27it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 22.59it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 22.45it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.27it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.40it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.30it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.25it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.65it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.87it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.53it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.25it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.77it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.12it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.68it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.00it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.83it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.58it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.38it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.00it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.52it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.85it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.98it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.69it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.78it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:40:42 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 0.662 | ppl 1.58 | wps 13184 | wpb 1415.4 | bsz 53.1 | num_updates 8232 | best_loss 0.662\n",
            "2023-03-05 19:40:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 8232 updates\n",
            "2023-03-05 19:40:42 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint49.pt\n",
            "2023-03-05 19:40:45 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint49.pt\n",
            "2023-03-05 19:40:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint49.pt (epoch 49 @ 8232 updates, score 0.662) (writing took 8.315851139000188 seconds)\n",
            "2023-03-05 19:40:50 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
            "2023-03-05 19:40:50 | INFO | train | epoch 049 | loss 0.75 | ppl 1.68 | wps 4186.7 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 8232 | lr 6.97071e-05 | gnorm 0.913 | train_wall 86 | gb_free 13.3 | wall 4877\n",
            "epoch 050:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:40:50 | INFO | fairseq.trainer | begin training epoch 50\n",
            "2023-03-05 19:40:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 050:  99% 167/168 [01:26<00:00,  1.97it/s, loss=0.694, ppl=1.62, wps=3923, ups=1.56, wpb=2518.7, bsz=102.7, num_updates=8300, lr=6.9421e-05, gnorm=0.761, train_wall=50, gb_free=13.4, wall=4912]2023-03-05 19:42:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 050 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 19.34it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 24.12it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 23.98it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 22.71it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.30it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.54it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.37it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.16it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.65it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.94it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.52it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.20it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.77it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.11it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.72it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.03it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.85it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.60it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.42it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.05it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.55it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.86it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.98it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.70it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.80it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:42:22 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 0.63 | ppl 1.55 | wps 13251.8 | wpb 1415.4 | bsz 53.1 | num_updates 8400 | best_loss 0.63\n",
            "2023-03-05 19:42:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 8400 updates\n",
            "2023-03-05 19:42:22 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint50.pt\n",
            "2023-03-05 19:42:25 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint50.pt\n",
            "2023-03-05 19:42:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint50.pt (epoch 50 @ 8400 updates, score 0.63) (writing took 8.52849152699946 seconds)\n",
            "2023-03-05 19:42:30 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
            "2023-03-05 19:42:30 | INFO | train | epoch 050 | loss 0.725 | ppl 1.65 | wps 4182.2 | ups 1.68 | wpb 2492.9 | bsz 106.7 | num_updates 8400 | lr 6.90066e-05 | gnorm 0.81 | train_wall 86 | gb_free 13.2 | wall 4977\n",
            "epoch 051:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:42:31 | INFO | fairseq.trainer | begin training epoch 51\n",
            "2023-03-05 19:42:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 051:  99% 167/168 [01:25<00:00,  1.69it/s, loss=0.711, ppl=1.64, wps=3919.4, ups=1.57, wpb=2496.1, bsz=108, num_updates=8500, lr=6.85994e-05, gnorm=0.749, train_wall=50, gb_free=13.3, wall=5027]2023-03-05 19:43:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 051 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:   2% 1/46 [00:00<00:05,  8.23it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:   7% 3/46 [00:00<00:03, 12.18it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  11% 5/46 [00:00<00:02, 15.01it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  17% 8/46 [00:00<00:02, 17.81it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  22% 10/46 [00:00<00:01, 18.12it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  28% 13/46 [00:00<00:01, 18.43it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  33% 15/46 [00:00<00:01, 18.84it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 18.29it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  41% 19/46 [00:01<00:01, 17.98it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  46% 21/46 [00:01<00:01, 17.56it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  50% 23/46 [00:01<00:01, 16.69it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  54% 25/46 [00:01<00:01, 15.98it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  59% 27/46 [00:01<00:01, 15.03it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  63% 29/46 [00:01<00:01, 13.43it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  67% 31/46 [00:02<00:01, 11.32it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  72% 33/46 [00:02<00:01,  9.57it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  76% 35/46 [00:02<00:01,  8.15it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.46it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  80% 37/46 [00:03<00:01,  6.77it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.63it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.42it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.29it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  5.93it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.45it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.81it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.96it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.69it/s]\u001b[A\n",
            "epoch 051 | valid on 'valid' subset: 100% 46/46 [00:05<00:00,  3.80it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:44:02 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 0.64 | ppl 1.56 | wps 12788.5 | wpb 1415.4 | bsz 53.1 | num_updates 8568 | best_loss 0.63\n",
            "2023-03-05 19:44:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 8568 updates\n",
            "2023-03-05 19:44:02 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint51.pt\n",
            "2023-03-05 19:44:05 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint51.pt\n",
            "2023-03-05 19:44:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint51.pt (epoch 51 @ 8568 updates, score 0.64) (writing took 5.539475968999795 seconds)\n",
            "2023-03-05 19:44:08 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)\n",
            "2023-03-05 19:44:08 | INFO | train | epoch 051 | loss 0.714 | ppl 1.64 | wps 4299.2 | ups 1.72 | wpb 2492.9 | bsz 106.7 | num_updates 8568 | lr 6.83267e-05 | gnorm 0.852 | train_wall 86 | gb_free 13.4 | wall 5074\n",
            "epoch 052:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:44:08 | INFO | fairseq.trainer | begin training epoch 52\n",
            "2023-03-05 19:44:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 052:  99% 167/168 [01:26<00:00,  2.01it/s, loss=0.745, ppl=1.68, wps=4631.8, ups=1.9, wpb=2434.4, bsz=99.1, num_updates=8700, lr=6.78064e-05, gnorm=0.962, train_wall=52, gb_free=13.3, wall=5145]2023-03-05 19:45:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 052 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 17.15it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 23.29it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 23.17it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 22.82it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.88it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.90it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.65it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.22it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.71it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 16.91it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.62it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.33it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.77it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.11it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.70it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.03it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  83% 38/46 [00:03<00:01,  6.86it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.58it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.41it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.03it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.53it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.86it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.99it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.71it/s]\u001b[A\n",
            "epoch 052 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.81it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:45:39 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 0.631 | ppl 1.55 | wps 13267.1 | wpb 1415.4 | bsz 53.1 | num_updates 8736 | best_loss 0.63\n",
            "2023-03-05 19:45:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 8736 updates\n",
            "2023-03-05 19:45:39 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint52.pt\n",
            "2023-03-05 19:45:42 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint52.pt\n",
            "2023-03-05 19:45:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint52.pt (epoch 52 @ 8736 updates, score 0.631) (writing took 5.534404678000101 seconds)\n",
            "2023-03-05 19:45:45 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)\n",
            "2023-03-05 19:45:45 | INFO | train | epoch 052 | loss 0.703 | ppl 1.63 | wps 4313.5 | ups 1.73 | wpb 2492.9 | bsz 106.7 | num_updates 8736 | lr 6.76665e-05 | gnorm 0.868 | train_wall 86 | gb_free 13.2 | wall 5172\n",
            "epoch 053:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:45:45 | INFO | fairseq.trainer | begin training epoch 53\n",
            "2023-03-05 19:45:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 053:  99% 167/168 [01:25<00:00,  2.30it/s, loss=0.702, ppl=1.63, wps=4883.1, ups=1.96, wpb=2488.3, bsz=110.9, num_updates=8900, lr=6.70402e-05, gnorm=0.787, train_wall=51, gb_free=13.2, wall=5256]2023-03-05 19:47:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 053 | valid on 'valid' subset:   0% 0/46 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:   4% 2/46 [00:00<00:02, 17.20it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  11% 5/46 [00:00<00:01, 23.00it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  17% 8/46 [00:00<00:01, 23.26it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  24% 11/46 [00:00<00:01, 22.99it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  30% 14/46 [00:00<00:01, 21.79it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  37% 17/46 [00:00<00:01, 20.89it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  43% 20/46 [00:00<00:01, 19.66it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  48% 22/46 [00:01<00:01, 18.41it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  52% 24/46 [00:01<00:01, 17.86it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  57% 26/46 [00:01<00:01, 17.08it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  61% 28/46 [00:01<00:01, 15.62it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  65% 30/46 [00:01<00:01, 13.31it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  70% 32/46 [00:01<00:01, 10.82it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  74% 34/46 [00:02<00:01,  9.12it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  78% 36/46 [00:02<00:01,  7.70it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  80% 37/46 [00:02<00:01,  7.03it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  83% 38/46 [00:02<00:01,  6.86it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  85% 39/46 [00:03<00:01,  6.58it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  87% 40/46 [00:03<00:00,  6.40it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  89% 41/46 [00:03<00:00,  6.02it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  91% 42/46 [00:03<00:00,  5.53it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  93% 43/46 [00:04<00:00,  4.88it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  96% 44/46 [00:04<00:00,  3.99it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset:  98% 45/46 [00:04<00:00,  3.71it/s]\u001b[A\n",
            "epoch 053 | valid on 'valid' subset: 100% 46/46 [00:04<00:00,  3.81it/s]\u001b[A\n",
            "                                                                        \u001b[A2023-03-05 19:47:16 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 0.643 | ppl 1.56 | wps 13289.6 | wpb 1415.4 | bsz 53.1 | num_updates 8904 | best_loss 0.63\n",
            "2023-03-05 19:47:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 8904 updates\n",
            "2023-03-05 19:47:16 | INFO | fairseq.trainer | Saving checkpoint to models/new_norm_lstm/checkpoint53.pt\n",
            "2023-03-05 19:47:19 | INFO | fairseq.trainer | Finished saving checkpoint to models/new_norm_lstm/checkpoint53.pt\n",
            "2023-03-05 19:47:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint models/new_norm_lstm/checkpoint53.pt (epoch 53 @ 8904 updates, score 0.643) (writing took 5.747444945999632 seconds)\n",
            "2023-03-05 19:47:22 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)\n",
            "2023-03-05 19:47:22 | INFO | train | epoch 053 | loss 0.684 | ppl 1.61 | wps 4311.2 | ups 1.73 | wpb 2492.9 | bsz 106.7 | num_updates 8904 | lr 6.70251e-05 | gnorm 0.788 | train_wall 86 | gb_free 13.3 | wall 5269\n",
            "epoch 054:   0% 0/168 [00:00<?, ?it/s]2023-03-05 19:47:22 | INFO | fairseq.trainer | begin training epoch 54\n",
            "2023-03-05 19:47:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 054:  74% 125/168 [01:04<00:21,  1.98it/s, loss=0.638, ppl=1.56, wps=4024.1, ups=1.6, wpb=2519.3, bsz=112.9, num_updates=9000, lr=6.66667e-05, gnorm=0.715, train_wall=51, gb_free=13.3, wall=5319]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il est possible de tester le modèle que nous venons de créer:"
      ],
      "metadata": {
        "id": "mf6pqL0zw7ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 10 data/dev.sp2000.src | \\\n",
        "    fairseq-interactive data/data_norm_bin_2000 \\\n",
        "        --source-lang src \\\n",
        "        --target-lang trg \\\n",
        "        --path models/new_norm_lstm/checkpoint_best.pt \\\n",
        "    > data/dev.sp2000.norm.output 2> /tmp/dev"
      ],
      "metadata": {
        "id": "W80APqv-wsxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ceux qui voudraient créer un modèle à base de transformeurs peuvent utiliser le code suivant:"
      ],
      "metadata": {
        "id": "3bWWYZ2BzqKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an empty model folder to store the model in\n",
        "!mkdir models/new_norm_transformer\n",
        "\n",
        "# call fairseq-train\n",
        "!fairseq-train \\\n",
        "        data/data_norm_bin_2000 \\\n",
        "        --save-dir models/new_norm_transformer \\\n",
        "        --save-interval 1 --patience 25 \\\n",
        "        --arch transformer \\\n",
        "        --encoder-layers 2 --decoder-layers 4 --encoder-attention-heads 4 \\\n",
        "        --encoder-embed-dim 256 --encoder-ffn-embed-dim 1024 --dropout 0.3 \\\n",
        "        --criterion cross_entropy --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "        --lr 0.001 --lr-scheduler inverse_sqrt \\\n",
        "        --warmup-updates 4000 \\\n",
        "        --max-tokens 3000 --max-tokens 3000 \\\n",
        "        --share-all-embeddings --batch-size-valid 64"
      ],
      "metadata": {
        "id": "GjZJGldGzpcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjIuPT0xCFlH"
      },
      "source": [
        "# 6. Générer des données artificielles\n",
        "Si l'on peut normaliser des données, il est possible de faire le processus inverse: les dé-normaliser, c'est-à-dire de créer des \"fausses\" phrases écrites comme on l'aurait fait dans le passé. Cela peut être utile pour créer des données d'entraînement par exemple."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ogbsBsXCFlH"
      },
      "source": [
        "1. D'abord on prépare les données normalisées pour la dénormalisation avec la même fonction `spm.encode` qui transforme la phrase en sous-mots/BPE.\n",
        "2. Ensuite, notre modèle de dénormalisation prévoyant cette option, on spécifie le système graphique de quelle décennie nous visons (`162` pour les années 20 du XVIIe s.)\n",
        "3. On sauvegarde le résultat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUH29A6pCFlH"
      },
      "outputs": [],
      "source": [
        "dev_trg_sp = spm.encode(dev_trg, out_type=str)\n",
        "decade_token = '▁<decade=162> '\n",
        "write_file([' '.join([decade_token] + phrase) for phrase in dev_trg_sp], 'data/dev.sp.trg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD29l1VBCFlH"
      },
      "source": [
        "### Dénormaliser le texte\n",
        "\n",
        "(10 premières phrases seulement. Vous pouvez faire plus de phrases en modifiant le 10. Vous pouvez tout normaliser en changeant `head -n 10` en `cat`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xiMwTVdCFlH"
      },
      "outputs": [],
      "source": [
        "!head -n 10 data/dev.sp.trg | fairseq-interactive models/denorm --source-lang trg --target-lang src --path models/denorm/lstm_denorm.pt > data/dev.sp.denorm.src.10.output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7IMoL0zCFlH"
      },
      "source": [
        "### Post-traiter la sortie du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jpb_0jcOCFlH"
      },
      "outputs": [],
      "source": [
        "dev_denorm_10 = extract_hypothesis('data/dev.sp.denorm.src.10.output')\n",
        "dev_denorm_10_postproc = decode_sp(dev_denorm_10)\n",
        "write_file(dev_denorm_10_postproc, 'data/dev.sp.denorm.10.src')\n",
        "dev_denorm_10_postproc[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aMYMFG8CFlH"
      },
      "source": [
        "Il y a pas mal d'étapes, donc pour faciliter le traitement, voici une fonction qui prend en entrée une liste de phrases et qui fait tout :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESB7k41oCFlH"
      },
      "outputs": [],
      "source": [
        "def denormalise(sents, decade):\n",
        "    # ntre modèle de dénormalisation ne fonctionne que pour le XVIIe s., on contrôle donc la décennie demandée\n",
        "    assert int(decade) >=1600 and int(decade) < 1700, 'Your decade must be between 1600 and 1690'\n",
        "    # on génère un fichier temporaire pour stocker les résultats\n",
        "    filetmp = 'data/tmp_denorm.sp.trg.tmp'\n",
        "    # On transforme en BPE\n",
        "    input_sp = spm.encode(sents, out_type=str)\n",
        "    # On ajoute le token de décennie à chaque phrase\n",
        "    decade_token = '▁<decade=' + str(decade)[:3] + '>'\n",
        "    input_sp_sents = [' '.join([decade_token] + sent) for sent in input_sp]\n",
        "    write_file(input_sp_sents, filetmp)\n",
        "    #print(\"preprocessed = \", input_sp_sents)\n",
        "    # On déormalise\n",
        "    !cat data/tmp_denorm.sp.trg.tmp | fairseq-interactive models/denorm --source-lang trg --target-lang src --path models/denorm/lstm_denorm.pt > data/tmp_denorm.sp.trg.output 2> /tmp/dev\n",
        "    # On passe au post-processing: extraction de la prédiction/hypothèse\n",
        "    outputs = extract_hypothesis('data/tmp_denorm.sp.trg.output')\n",
        "    #On transforme les sous-mots/BPE en mots\n",
        "    outputs_postproc = decode_sp(outputs)\n",
        "    return outputs_postproc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cXisFJSCFlH"
      },
      "source": [
        "On peut désormais utiliser notre fonction de la manière suivante:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV4a94OVCFlH"
      },
      "outputs": [],
      "source": [
        "print(denormalise([\"Je ne savais pas qu'il ferait si beau.\",\n",
        "                  \"Parti plus tôt que ses rivaux du parti Les Républicains et longtemps considéré comme favori, le président des Hauts-de-France a été balayé au terme d'une campagne interne marquée par le thème de l'immigration.\"],\n",
        "                  1640))\n",
        "print(denormalise([\"Je ne savais pas qu'il ferait si beau.\",\n",
        "                  \"Parti plus tôt que ses rivaux du parti Les Républicains et longtemps considéré comme favori, le président des Hauts-de-France a été balayé au terme d'une campagne interne marquée par le thème de l'immigration.\"], \n",
        "                  1690))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vpNoPYdCFlH"
      },
      "source": [
        "Et voici une fonction similaire pour la normalisation :"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tutoriel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}