{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS-4b9q7CFk7"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/gabays/32M7131/blob/main/Cours_03/Cours03.ipynb)\n",
        "\n",
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by/4.0/\"><img alt=\"Licence Creative Commons\" style=\"border-width:0;float:right;\\\" src=\"https://i.creativecommons.org/l/by/4.0/88x31.png\" /></a>\n",
        "\n",
        "Distant Reading 2: linguistique computationnelle\n",
        "\n",
        "# **Entre normalisation et traduction**\n",
        "\n",
        "Simon Gabay\n",
        "\n",
        "## Introduction\n",
        "\n",
        "L'idée de ce cours est de découvrir la traduction automatique de manière détournée, en s'intéressant à un problème de philologie: celui de la normalisation du texte. En effet, la sortie d'un OCR prend une forme proche de celle-ci:\n",
        "\n",
        ">QVe cette propoſtion, qu'vn eſpace eſt vuidé\n",
        "\n",
        "Il est évident que personne ne va éditer un texte comme cela, d'autant que les spécialistes du XVIIe s. ont pris l'habitude, dans leurs éditions, d'aligner le système graphique ancien sur le système graphique contemporain. Il nous faudrait donc un résultat du type\n",
        "\n",
        ">QVe cette propostion, qu'un espace est vidé\n",
        "\n",
        "Les philologues pourront déplorer que ce résultat masque un certain nombre de faits linguistiques, ce qui est vrai, mais cela a quelques vertus pour de possibles traitement informatiques, comme de \"lisser\" la langue et d'en retirer des aspérités qui pourraient gêner des algorithmes, comme en stylométrie.\n",
        "\n",
        "\n",
        "## 1. Préparer l'expérience\n",
        "Il faut d'abord installer les paquets nécessaires\n",
        "* _faireseq_ pour la gestion de l'entraînement\n",
        "* _sentencepiece_ pour créer des sous-mots\n",
        "* _sacrebleu_ pour l'évaluation du résultat avec un score BLEU\n",
        "* etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "T6bZAqsJCFlA"
      },
      "outputs": [],
      "source": [
        "!pip install fairseq@git+https://github.com/pytorch/fairseq.git@5a75b079bf8911a327940c28794608e003a9fa52 \n",
        "!pip install sentencepiece sacrebleu hydra-core omegaconf==2.0.5 gdown==4.2.0 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYs3zdFuCFlB"
      },
      "source": [
        "Télécharger les données et les modèles depuis le repo GitHub du cours et les structurer dans les dossiers `data/`, `models/`. Une partie du travail de préparation est fait via un script `structure_files.sh`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "tPDSiF6wCFlB"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/gabays/32M7131/releases/download/Norm/Normalisation-models.zip \n",
        "!unzip Normalisation-models.zip\n",
        "!mv -f French-normalisation-data-models data-models\n",
        "!mv data-models/structure_files.sh ./; bash structure_files.sh\n",
        "!rm Normalisation-models.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBYRG4b_CFlB"
      },
      "source": [
        "## 2. Préparation des données à normaliser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuMbPoVYCFlB"
      },
      "source": [
        "Fonctions pour\n",
        "1. lire le contenu d'un fichier ligne par ligne\n",
        "2. les lire depuis un fichier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKcu9lKJCFlC"
      },
      "outputs": [],
      "source": [
        "# lire un fichier ligne par ligne\n",
        "def read_file(filename):\n",
        "  list_sents = []\n",
        "  with open(filename) as fp:\n",
        "    for line in fp:\n",
        "      list_sents.append(line.strip())\n",
        "  return list_sents\n",
        "\n",
        "# écrire une liste de phrases dans un fichier\n",
        "def write_file(list_sents, filename):\n",
        "    with open(filename, 'w') as fp:\n",
        "        for sent in list_sents:\n",
        "            fp.write(sent + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3SJRUksCFlC"
      },
      "source": [
        "On va pouvoir charger deux textes:\n",
        "1. `dev.src` qui contient un texte _source_, c'est-à-dire à normaliser\n",
        "2. `dev.trg` qui contient un texte _cible_ (en anglais _target_) normalisé à la main."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTLASC6dCFlC"
      },
      "outputs": [],
      "source": [
        "dev_src = read_file('data/dev.src')\n",
        "dev_trg = read_file('data/dev.trg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD8yv0ZLCFlD"
      },
      "source": [
        "On peu regarder à quoi ressemblent ces deux fichiers que nous venons de charger:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC0JodX2CFlD"
      },
      "outputs": [],
      "source": [
        "for i in range(4):\n",
        "    print('src = ', dev_src[i])\n",
        "    print('trg = ', dev_trg[i])\n",
        "    print('--')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zi37Kbq0CFlD"
      },
      "source": [
        "On va désormais charger le modèle de segmentation en sous-mots (`bpe_joint_1000.model`). On parle de _Byte Pair Encoding_ ou \"codage par paires d’octets\" ([Sennrich 2016](https://aclanthology.org/P16-1162/)): à partir d'une analyse de tous les caractères, l'algorithme effectue des opérations de fusion pour les paires les plus courantes (comme `ment`, que l'on retrouve souvent dans les adverbes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB1pgYRzCFlD"
      },
      "outputs": [],
      "source": [
        "import sentencepiece\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file='data/bpe_joint_1000.model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxB6zszBCFlD"
      },
      "source": [
        "On applique ce modèle de segmentation sur les données à normaliser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIV2alvlCFlD"
      },
      "outputs": [],
      "source": [
        "dev_src_sp = spm.encode(dev_src, out_type=str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEs7dmxzCFlE"
      },
      "source": [
        "On sauvegarde le résultat dans un nouveau fichier `dev.sp.src`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MV3G-IfCFlE"
      },
      "outputs": [],
      "source": [
        "write_file([' '.join(phrase) for phrase in dev_src_sp], 'data/dev.sp.src')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JVS8LetCFlE"
      },
      "source": [
        "Voilà un extrait du fichier que nous venons de créer, avec les fameux BPE ou sous-mots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "o9HvML5NCFlE"
      },
      "outputs": [],
      "source": [
        "dev_src_sp[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BrvIw3yCFlE"
      },
      "source": [
        "On définitune fonction pour détokeniser une liste de phrases, c'est à dire de \"recoller\" les BPE pour retrouver nos phrases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnc6csGECFlE"
      },
      "outputs": [],
      "source": [
        "def decode_sp(list_sents):\n",
        "    return [''.join(sent).replace(' ', '').replace('▁', ' ').strip() for sent in list_sents]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLlMrNPOCFlE"
      },
      "source": [
        "Visualiser à quoi ressemble le texte détokenisé, qui doit ressembler au texte de départ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blOxyvXPCFlF"
      },
      "outputs": [],
      "source": [
        "decode_sp(dev_src_sp[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM6KeAufCFlF"
      },
      "source": [
        "## 3. Appliquer le modèle de normalisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPMoT2m3CFlF"
      },
      "source": [
        "### 3.1 Etape par étape\n",
        "Appliquer le modèle de normalisation sur le début des données pre-traitées (ça prend moins de temps pour tester que normaliser tout le texte)\n",
        "\n",
        "Il y aura un message \"UserWarning\", mais vous pouvez l'ignorer - ce n'est pas grave.\n",
        "\n",
        "Explications:\n",
        "- `head -n 10` affiche les 10 premières phrases\n",
        "- ces 10 premières lignes sont donné à _fairseq-interactive_ qui effectue la traductin (dans notre cas la normlaisation)\n",
        "- le résultat va dans `data/dev.sp.norm.trg.10.output`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "OSsTyPKZCFlF"
      },
      "outputs": [],
      "source": [
        "!head -n 10 data/dev.sp.src | fairseq-interactive models/norm/ --source-lang src --target-lang trg --path models/norm/lstm_norm.pt > data/dev.sp.norm.trg.10.output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regardons à quoi ressemble le résultat. Pour lire ce fichier, avec pour un exemple `i`:\n",
        "\n",
        "- `S-i`: le texte source\n",
        "- `H-i`: le score de l'hypothèse et l'hypothèse du modèle (c'est-à-dire la prédiction)\n",
        "- `P-i`: les scores de chaque sous-token produit par le modèle"
      ],
      "metadata": {
        "id": "LX32GRWDH7YQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 26 data/dev.sp.norm.trg.10.output | tail -n 20"
      ],
      "metadata": {
        "id": "eMbCUL-YH0V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTRxz9K-CFlF"
      },
      "source": [
        "On définit une fonction pour extraire l'hypothèse (la ligne commençant par `H` donc) de ce fichier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjmXAhX9CFlF"
      },
      "outputs": [],
      "source": [
        "def extract_hypothesis(filename):\n",
        "    outputs = []\n",
        "    with open(filename) as fp:\n",
        "        for line in fp:\n",
        "            # seulement les lignes qui commencet par H- (pour Hypothèse)\n",
        "            if 'H-' in line:\n",
        "                # prendre la 3ème colonne (c'est-à-dire l'indice 2)\n",
        "                outputs.append(line.strip().split('\\t')[2])\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37VZKIK8CFlF"
      },
      "source": [
        "On peut désormais extraire les hypothèses du fichier produit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0eoF2DDCFlF"
      },
      "outputs": [],
      "source": [
        "dev_norm_10 = extract_hypothesis('data/dev.sp.norm.trg.10.output')\n",
        "dev_norm_10[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqSje0hdCFlG"
      },
      "source": [
        "On peut désormais détokeniser le résultat avec la fonction `decode_sp` que nous avons défini plus haut:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikDPDxYdCFlG"
      },
      "outputs": [],
      "source": [
        "dev_norm_10_postproc = decode_sp(dev_norm_10)\n",
        "dev_norm_10_postproc[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYJ6ewy3CFlG"
      },
      "source": [
        "Il ne reste plus qu'à sauvegarder le résultat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKVwreqyCFlG"
      },
      "outputs": [],
      "source": [
        "write_file(dev_norm_10_postproc, 'data/dev.norm.10.trg')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Tout d'un coup\n",
        "Comme on ne va pas tout refaire étape par étape à chaque fois, maintenant que nous avons compris le fonctionnement, on crée une fonction qui permet de tout faire d'un coup."
      ],
      "metadata": {
        "id": "GTUiGaWdU6pI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalise(sents):\n",
        "    # generate temporary file\n",
        "    filetmp = 'data/tmp_norm.sp.src.tmp'\n",
        "    # preprocessing\n",
        "    input_sp = spm.encode(sents, out_type=str)\n",
        "    # add decade token to each sentence\n",
        "    input_sp_sents = [' '.join(sent) for sent in input_sp]\n",
        "    write_file(input_sp_sents, filetmp)\n",
        "    #print(\"preprocessed = \", input_sp_sents)\n",
        "    # denormalisation\n",
        "    !cat data/tmp_norm.sp.src.tmp | fairseq-interactive models/norm --source-lang src --target-lang trg --path models/norm/lstm_norm.pt > data/tmp_norm.sp.src.output 2> /tmp/dev\n",
        "    # postprocessing\n",
        "    outputs = extract_hypothesis('data/tmp_norm.sp.src.output')\n",
        "    outputs_postproc = decode_sp(outputs)\n",
        "    return outputs_postproc"
      ],
      "metadata": {
        "id": "YJdVB2FsUxLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La fonction s'utilise comme suit:"
      ],
      "metadata": {
        "id": "HV56r_8LU_Pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalise([\"1. QVe cette propostion, qu'vn espace est vuidé, repugne au sens commun.\",\n",
        "          \"Affectoit un mépris qui marquoit ſon eſtime,\"])"
      ],
      "metadata": {
        "id": "4CRm-yLGVD4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si je veux traiter tout le fichier (attention, cela prend un peu de temps, même avec un GPU):"
      ],
      "metadata": {
        "id": "-LO2Jp3RVNTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_norm = normalise(dev_src)\n",
        "#Je sauvegarde\n",
        "write_file(dev_norm, 'data/dev.norm.trg')"
      ],
      "metadata": {
        "id": "fa95MmCXVMkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDfClaIjCFlG"
      },
      "source": [
        "## 4. Contrôle qualité\n",
        "Nous pouvons faire deux choses pour contrôler la qualité de notre travail:\n",
        "1. Mesurer l'efficacité de notre modèle\n",
        "2. Comparer ces résultats avec une autre méthode\n",
        "\n",
        "### 4.1 Quelques métriques\n",
        "On se rappelle que pour ce premier cas de normalisation nous avons une version _gold_, qui nous permet de comparer les prédictions avec une version \"parfaite\". Afin de mesurer la distance entre le résultat obtenu et le résultat attendu, nous avons plusieurs métriques à disposition:\n",
        "- BLEU: le métrique d'évaluation le plus fréquemment utilisé en traduction automatique\n",
        "- ChrF: CharacterF score (comme le score BLEU mais basé sur des n-grams de caractères)\n",
        "- TER: _translation edit rate_\n",
        "\n",
        "Attention : puisque nous avons seulement normalisé 10 phrases, il faut seulement comparer contre les 10 première phrases de référence. Pour un résultat plus fiable, il faudrait calculer ces scores sur un plus grand nombre de phrases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnrKcz1VCFlG"
      },
      "outputs": [],
      "source": [
        "from sacrebleu.metrics import BLEU, CHRF, TER\n",
        "bleu = BLEU()\n",
        "bleu.corpus_score(dev_norm_10_postproc, [dev_trg[:10]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SieYL2K0CFlG"
      },
      "outputs": [],
      "source": [
        "chrf = CHRF()\n",
        "chrf.corpus_score(dev_norm_10_postproc, [dev_trg[:10]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFUmRYFaCFlG"
      },
      "outputs": [],
      "source": [
        "ter = TER()\n",
        "ter.corpus_score(dev_norm_10_postproc, [dev_trg[:10]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmjbIv1iCFlG"
      },
      "source": [
        "Une évaluation plus adaptée : la précision au niveau de chaque mot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqzlkL_7CFlG"
      },
      "outputs": [],
      "source": [
        "import align"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Td3NGxLLCFlG"
      },
      "outputs": [],
      "source": [
        "# d'abord créer un fichier qui ne contient que les 10 première phrases du document cible\n",
        "!head -n 10 data/dev.trg > data/dev.10.trg\n",
        "align_dev_norm_10 = align.align('data/dev.10.trg', 'data/dev.norm.10.trg')\n",
        "\n",
        "print(align_dev_norm_10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCub5rjyCFlG"
      },
      "source": [
        "Le résultat de l'alignement est une liste de phrases, où chaque mot de la phrase est comme suit:\n",
        "\n",
        "- le mot tout seul s'il est pareil dans les deux textes (ex : `QUe`)\n",
        "- le mot du premier document et le mot du deuxième document, séparé par `>` s'ils sont différents (ex : `proposition>propostion`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSe6kVkuCFlH"
      },
      "outputs": [],
      "source": [
        "num_diff = 0\n",
        "total = 0\n",
        "for sentence in align_dev_norm_10:\n",
        "    for word in sentence:\n",
        "        if '>' in word:\n",
        "            num_diff += 1\n",
        "        total += 1\n",
        "print('Accuracy = ' + str((total - num_diff)/total))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Utiliser ces métriques avec le test\n",
        "Si vous avez bien travaillé, vous avez prévu un jeu de test, qui n'a pas été vu pendant l'entraînement. Vous pouvez donc normaliser ce jeu de données, et appliquer les métriques que nous venons de voir pour évaluer proprement l'efficacité de notre modèle\n"
      ],
      "metadata": {
        "id": "A4PN-7_7YlUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#METTRE ICI VOTRE CODE"
      ],
      "metadata": {
        "id": "iDTGMwEjY7Vd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Construire une _baseline_\n",
        "La _baseline_ va être le score obtenu avec une méthode plus rudimentaire, pour bien contrôler que nous n'ontiendrions pas des résulats aussi bon sans tout le travail que nous venons de faire. Il est par exemple possible d'utiliser des expressions régulières."
      ],
      "metadata": {
        "id": "NQECKuKAV3By"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On crée une fonction qui va normaliser une phrase avec une fonction qui normalise un mot\n",
        "import utils\n",
        "from importlib import reload\n",
        "reload(utils)\n",
        "def normalise_sent(sent, normalise_word_function):\n",
        "    norm_sent = []\n",
        "    # On tokenise la phrase (de manière pas très très propre…) et on applique la normalisation choisie à chaque token\n",
        "    for word in utils.basic_tokenise(sent).split():\n",
        "        norm_sent.append(normalise_word_function(word))\n",
        "    return utils.detokenise(' '.join(norm_sent))"
      ],
      "metadata": {
        "id": "DXIhfYGcWhXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parmi les différentes options qui s'offre à nous, nous pouvons… ne rien faire"
      ],
      "metadata": {
        "id": "Urr8eSQDXET5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function that returns the word itself\n",
        "def return_word(word):\n",
        "    return word"
      ],
      "metadata": {
        "id": "Dw_Y4DqPXOxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut désormais normaliser la phrase avec notre fonction `normalise_sent`, qui applique la fonction `return_word` pour chaque token"
      ],
      "metadata": {
        "id": "_454bjVsXTQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalise_word_function = return_word\n",
        "normalise_sent(\"QVe cette propoſtion, qu'vn eſpace eſt vuidé\", normalise_word_function)"
      ],
      "metadata": {
        "id": "lG5yGKN2XgPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C'est super, mais ça sert à rien… Essayons de faire mieux avec une regex"
      ],
      "metadata": {
        "id": "yk8fpasZXhjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_long_s(word):\n",
        "    word = word.replace('ſ', 's')\n",
        "    return word"
      ],
      "metadata": {
        "id": "k2waj4BQXvcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut tester cette nouvelle fonction:"
      ],
      "metadata": {
        "id": "2oJPYdlcX3Zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalise_word_function = replace_long_s\n",
        "normalise_sent(\"QVe cette propoſtion, qu'vn eſpace eſt vuidé\", normalise_word_function)"
      ],
      "metadata": {
        "id": "TnSBlJW5X6Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "On peut maintenant tester avec plusieurs regex"
      ],
      "metadata": {
        "id": "JLuNsZTZYBSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def replace_regex(word):\n",
        "    word = word.replace('ſ', 's')\n",
        "    word = re.sub(\"([Qq])v\", r'\\1u', word)\n",
        "    word = re.sub(\"([Qq])V\", r'\\1U', word)\n",
        "    word = re.sub(\"('?)vn(e?)\", r'\\1un\\2', word)\n",
        "    return word\n",
        "\n",
        "normalise_word_function = replace_regex\n",
        "normalise_sent(\"QVe cette propoſtion, qu'vn eſpace eſt vuidé\", normalise_word_function)"
      ],
      "metadata": {
        "id": "iL1V23KFYK-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Je n'ai pèlus qu'à traiter tout le fichier… et évaluer le résultat avec les métriques précédentes!"
      ],
      "metadata": {
        "id": "lOgJmR92YSpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#METTRE ICI VOTRE CODE"
      ],
      "metadata": {
        "id": "vDFJwap6YcBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 Entraîner un modèle\n",
        "\n",
        "Nous allons entraîner un modèle de segmentation en sous-mots en avec le _toolkit_ [SentencePiece](https://github.com/google/sentencepiece/blob/master/README.md).\n",
        "\n",
        "Ce sera un modèle \"joint\", c'est-à-dire entraîné pour segmenter la langue source et la langue cible. On peut ainsi faire des sous-mots qui peuvent être partagés par les deux langues. Ce type de modèle est particulièrement utile pour deux langues proches lexicalement, comme c'est le cas entre le français du XVIIe s. et le français contemporain, mais pas pour les langues trop distantes (du français au chinois).\n",
        "\n",
        "La taille du vocabulaire ici est de 2000, mais ceci peut être changé. La taille du vocabulaire détermine combien de sous-tokens sont utilisés. Plus le vocabulaire est petit, plus le texte sera découpé, plus le vocabulaire est grand, moins le texte sera découpé (ça ressemblera plus à un découpage sur les espaces). La taille du vocabulaire dépend évidemment des cas (on dit qu'il est _task dependent_) et doit être testée pour être définie optimalement."
      ],
      "metadata": {
        "id": "MCe1K3Nmar9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On mélange la source et la cible pour faire un seul fichier\n",
        "!cat data/train.src data/train.trg > data/all_train.src-trg\n",
        "#On crée le vocabulaire\n",
        "sentencepiece.SentencePieceTrainer.train(input='data/all_train.src-trg', \n",
        "                               model_prefix='data/bpe_joint_2000', \n",
        "                               vocab_size=2000)"
      ],
      "metadata": {
        "id": "wXNe_CDsehG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nous allons désormais préparer nos trois jeux de données (`train`, `dev` et `test`)\n"
      ],
      "metadata": {
        "id": "xHsuxN2ZiQoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#On charge les jeux de données\n",
        "train_src = read_file('data/train.src')\n",
        "train_trg = read_file('data/train.trg')\n",
        "dev_src = read_file('data/dev.src')\n",
        "dev_trg = read_file('data/dev.trg')\n",
        "test_src = read_file('data/test.src')\n",
        "test_trg = read_file('data/test.trg')\n",
        "\n",
        "# On charge le modèle que nous venons de fabriquer avec SentencePiece\n",
        "spm = sentencepiece.SentencePieceProcessor(model_file='data/bpe_joint_2000.model')\n",
        "\n",
        "# On applique le modèle aux jeux de données\n",
        "train_src_sp = spm.encode(train_src, out_type=str)\n",
        "train_trg_sp = spm.encode(train_trg, out_type=str)\n",
        "dev_src_sp = spm.encode(dev_src, out_type=str)\n",
        "dev_trg_sp = spm.encode(dev_trg, out_type=str)\n",
        "test_src_sp = spm.encode(test_src, out_type=str)\n",
        "test_trg_sp = spm.encode(test_trg, out_type=str)\n",
        "\n",
        "# On contrôle le résultat (src et trg doivent avoir la même longueur pour chaque type de jeu)\n",
        "print(len(train_src_sp), len(train_trg_sp))\n",
        "print(len(dev_src_sp), len(dev_trg_sp))\n",
        "print(len(test_src_sp), len(test_trg_sp))\n",
        "\n",
        "# On crée les fichiers\n",
        "write_file([' '.join(sent) for sent in train_src_sp], 'data/train.sp2000.src')\n",
        "write_file([' '.join(sent) for sent in train_trg_sp], 'data/train.sp2000.trg')\n",
        "write_file([' '.join(sent) for sent in dev_src_sp], 'data/dev.sp2000.src')\n",
        "write_file([' '.join(sent) for sent in dev_trg_sp], 'data/dev.sp2000.trg')\n",
        "write_file([' '.join(sent) for sent in test_src_sp], 'data/test.sp2000.src')\n",
        "write_file([' '.join(sent) for sent in test_trg_sp], 'data/test.sp2000.trg')"
      ],
      "metadata": {
        "id": "PHQQbipOigd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour entrainer un modèle, il va d'abord falloir binariser les données. En gros tout devient des chiffres, ce qui permet à la machine d'aller plus vite. Ces chiffres correspondent aux entrées d'un dictionnaire où sont stockés les \"vrais\" mots."
      ],
      "metadata": {
        "id": "Jw-5slmBdsuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-preprocess --destdir data/data_norm_bin_2000/ \\\n",
        "                    -s trg -t src \\\n",
        "                    --trainpref data/train.sp2000 \\\n",
        "                    --validpref data/dev.sp2000 \\\n",
        "                    --testpref data/test.sp2000 \\\n",
        "                    --joined-dictionary"
      ],
      "metadata": {
        "id": "wSk6hw-tjFAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maintenant on peut appeler `fairseq-train`. Plusieurs paramètres sont à disposition:\n",
        "\n",
        "- `--save-dir` permet de dire où le modèle sera traité. Le meilleur modèle, sauvegardé dans le dossier indiqué, prendra le nom de `checkpoint_best.pt`.\n",
        "- `--save-interval` permet de définir la fréquence de sauvegarde (_checkpoint_), la valeur fournie déterminant le nombre _n_ d'_epochs_ entre deux _checkpoints_. La dernière sauvegarde est disponible avec le nom `checkpoint_last.pt`.\n",
        "- `--arch lstm` permet de préciser que nous voulons une archicture LSTM et non _transformer_ (il faudrait alors utiliser `--arch transformer`).\n",
        "- Une série de paramètres va permettre de déterminer le nombre de couches pour l'encodeur (`--encoder-layers`) et le décodeur (`--decoder-layers`). Une couche convient aux problèmes très simples: plus on augmente le nombre de couches (2 ou 3 par exemple), meilleurs seront (théoriquement) les résultats mais plus dur sera l'entraînement.\n",
        "- On va ensuite définir la taille des _embeddings_ (vecteurs) qui représentent les tokens pour l'encodeur (`--encoder-embed-dim`), le décodeur (`--decoder-embed-dim`) et la sortie (`--decoder-out-embed-dim`) et la sortie. La valeur se situe entre 100 et 1000 (et tourne généralement autour de 300). Plus on augmente ce chiffre plus on ajoute d'information (mais la quantité d'information ajoutée va decrescendo), plus on réduit la valeur, moins on retient d'information (et on perd donc en sémantisme).\n",
        "- Il faut aussi définir la taille de la couche/représentation cachée pour l'encodeur (`--encoder-hidden-size`) et le décodeur (`--decoder-hidden-size`), soit le nombre de _features_ retenu.\n",
        "- `--encoder-bidirectional` précise que l'encodeur est bidirectionnel (Bi-LSTM) et non unidirectionnel (LSTM): on utilise le contexte gauche _et droit_ du token. La représentation finale des mots de l'entrée est la concaténation des représentations gauche-droite et droite-gauche.\n",
        "- `--dropout` permet de déterminer la proportion de neurones désactivés lors de l'entraînement pour éviter l'_overfitting_.\n",
        "- `--optimizer` permet de déterminer l'optimiseur pour la descente de gradient.\n",
        "- `--lr` définit le taux d'apprentissage (_learning rate_).\n",
        "- `--lr-scheduler` va déterminer l'ajustement du _lrearning rate_.\n",
        "- `--warmup-updates` va permettre d'utiliser un taux d'apprentissage très faible au début du processus d'entraînement, avant de passer au _lr_ normal. On va ainsi limiter l'effet des premières données vues par le modèle.\n",
        "- `--share-all-embeddings` précise que l'on choisit de partager les _embeddings_ entre l'encodeur et le décodeur pour profiter de la similarité lexicale entre la langue source et langue cible (et pour accélerer l'entraînement).\n",
        "- `--max-tokens` définit la _batch size_, soit le nombre d'exemples envoyés en une seule fois au modèle lors de l'apprentissage."
      ],
      "metadata": {
        "id": "sFw1iuTujHwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an empty model folder to store the model in\n",
        "!mkdir models/new_norm_lstm\n",
        "\n",
        "# call fairseq-train\n",
        "!fairseq-train \\\n",
        "        data/data_norm_bin_2000 \\\n",
        "        --save-dir models/new_norm_lstm \\\n",
        "        --save-interval 1 --patience 12 \\\n",
        "        --arch lstm \\\n",
        "        --encoder-layers 3 --decoder-layers 3 \\\n",
        "        --encoder-embed-dim 384 --decoder-embed-dim 384 --decoder-out-embed-dim 384 \\\n",
        "        --encoder-hidden-size 768 --encoder-bidirectional --decoder-hidden-size 768 \\\n",
        "        --dropout 0.3 \\\n",
        "        --criterion cross_entropy --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "        --lr 0.0001 --lr-scheduler inverse_sqrt \\\n",
        "        --warmup-updates 4000 \\\n",
        "        --share-all-embeddings \\\n",
        "        --max-tokens 3000 \\\n",
        "        --batch-size-valid 64"
      ],
      "metadata": {
        "id": "KuseEFZ6wlbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Il est possible de tester le modèle que nous venons de créer:"
      ],
      "metadata": {
        "id": "mf6pqL0zw7ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 10 data/dev.sp2000.src | \\\n",
        "    fairseq-interactive data/data_norm_bin_2000 \\\n",
        "        --source-lang src \\\n",
        "        --target-lang trg \\\n",
        "        --path models/new_norm_lstm/checkpoint_best.pt \\\n",
        "    > data/dev.sp2000.norm.output 2> /tmp/dev"
      ],
      "metadata": {
        "id": "W80APqv-wsxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ceux qui voudraient créer un modèle à base de transformeurs peuvent utiliser le code suivant:"
      ],
      "metadata": {
        "id": "3bWWYZ2BzqKl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create an empty model folder to store the model in\n",
        "!mkdir models/new_norm_transformer\n",
        "\n",
        "# call fairseq-train\n",
        "!fairseq-train \\\n",
        "        data/data_norm_bin_2000 \\\n",
        "        --save-dir models/new_norm_transformer \\\n",
        "        --save-interval 1 --patience 25 \\\n",
        "        --arch transformer \\\n",
        "        --encoder-layers 2 --decoder-layers 4 --encoder-attention-heads 4 \\\n",
        "        --encoder-embed-dim 256 --encoder-ffn-embed-dim 1024 --dropout 0.3 \\\n",
        "        --criterion cross_entropy --optimizer adam --adam-betas '(0.9, 0.98)' \\\n",
        "        --lr 0.001 --lr-scheduler inverse_sqrt \\\n",
        "        --warmup-updates 4000 \\\n",
        "        --max-tokens 3000 --max-tokens 3000 \\\n",
        "        --share-all-embeddings --batch-size-valid 64"
      ],
      "metadata": {
        "id": "GjZJGldGzpcm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjIuPT0xCFlH"
      },
      "source": [
        "# 6. Générer des données artificielles\n",
        "Si l'on peut normaliser des données, il est possible de faire le processus inverse: les dé-normaliser, c'est-à-dire de créer des \"fausses\" phrases écrites comme on l'aurait fait dans le passé. Cela peut être utile pour créer des données d'entraînement par exemple."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ogbsBsXCFlH"
      },
      "source": [
        "1. D'abord on prépare les données normalisées pour la dénormalisation avec la même fonction `spm.encode` qui transforme la phrase en sous-mots/BPE.\n",
        "2. Ensuite, notre modèle de dénormalisation prévoyant cette option, on spécifie le système graphique de quelle décennie nous visons (`162` pour les années 20 du XVIIe s.)\n",
        "3. On sauvegarde le résultat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUH29A6pCFlH"
      },
      "outputs": [],
      "source": [
        "dev_trg_sp = spm.encode(dev_trg, out_type=str)\n",
        "decade_token = '▁<decade=162> '\n",
        "write_file([' '.join([decade_token] + phrase) for phrase in dev_trg_sp], 'data/dev.sp.trg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD29l1VBCFlH"
      },
      "source": [
        "### Dénormaliser le texte\n",
        "\n",
        "(10 premières phrases seulement. Vous pouvez faire plus de phrases en modifiant le 10. Vous pouvez tout normaliser en changeant `head -n 10` en `cat`.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xiMwTVdCFlH"
      },
      "outputs": [],
      "source": [
        "!head -n 10 data/dev.sp.trg | fairseq-interactive models/denorm --source-lang trg --target-lang src --path models/denorm/lstm_denorm.pt > data/dev.sp.denorm.src.10.output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7IMoL0zCFlH"
      },
      "source": [
        "### Post-traiter la sortie du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jpb_0jcOCFlH"
      },
      "outputs": [],
      "source": [
        "dev_denorm_10 = extract_hypothesis('data/dev.sp.denorm.src.10.output')\n",
        "dev_denorm_10_postproc = decode_sp(dev_denorm_10)\n",
        "write_file(dev_denorm_10_postproc, 'data/dev.sp.denorm.10.src')\n",
        "dev_denorm_10_postproc[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aMYMFG8CFlH"
      },
      "source": [
        "Il y a pas mal d'étapes, donc pour faciliter le traitement, voici une fonction qui prend en entrée une liste de phrases et qui fait tout :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESB7k41oCFlH"
      },
      "outputs": [],
      "source": [
        "def denormalise(sents, decade):\n",
        "    # ntre modèle de dénormalisation ne fonctionne que pour le XVIIe s., on contrôle donc la décennie demandée\n",
        "    assert int(decade) >=1600 and int(decade) < 1700, 'Your decade must be between 1600 and 1690'\n",
        "    # on génère un fichier temporaire pour stocker les résultats\n",
        "    filetmp = 'data/tmp_denorm.sp.trg.tmp'\n",
        "    # On transforme en BPE\n",
        "    input_sp = spm.encode(sents, out_type=str)\n",
        "    # On ajoute le token de décennie à chaque phrase\n",
        "    decade_token = '▁<decade=' + str(decade)[:3] + '>'\n",
        "    input_sp_sents = [' '.join([decade_token] + sent) for sent in input_sp]\n",
        "    write_file(input_sp_sents, filetmp)\n",
        "    #print(\"preprocessed = \", input_sp_sents)\n",
        "    # On déormalise\n",
        "    !cat data/tmp_denorm.sp.trg.tmp | fairseq-interactive models/denorm --source-lang trg --target-lang src --path models/denorm/lstm_denorm.pt > data/tmp_denorm.sp.trg.output 2> /tmp/dev\n",
        "    # On passe au post-processing: extraction de la prédiction/hypothèse\n",
        "    outputs = extract_hypothesis('data/tmp_denorm.sp.trg.output')\n",
        "    #On transforme les sous-mots/BPE en mots\n",
        "    outputs_postproc = decode_sp(outputs)\n",
        "    return outputs_postproc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cXisFJSCFlH"
      },
      "source": [
        "On peut désormais utiliser notre fonction de la manière suivante:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV4a94OVCFlH"
      },
      "outputs": [],
      "source": [
        "print(denormalise([\"Je ne savais pas qu'il ferait si beau.\",\n",
        "                  \"Parti plus tôt que ses rivaux du parti Les Républicains et longtemps considéré comme favori, le président des Hauts-de-France a été balayé au terme d'une campagne interne marquée par le thème de l'immigration.\"],\n",
        "                  1640))\n",
        "print(denormalise([\"Je ne savais pas qu'il ferait si beau.\",\n",
        "                  \"Parti plus tôt que ses rivaux du parti Les Républicains et longtemps considéré comme favori, le président des Hauts-de-France a été balayé au terme d'une campagne interne marquée par le thème de l'immigration.\"], \n",
        "                  1690))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vpNoPYdCFlH"
      },
      "source": [
        "Et voici une fonction similaire pour la normalisation :"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tutoriel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}